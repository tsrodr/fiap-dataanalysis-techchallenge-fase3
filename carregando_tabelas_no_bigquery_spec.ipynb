{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 16:33:39 WARN Utils: Your hostname, spark-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/09/30 16:33:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/spark/.ivy2/cache\n",
      "The jars for the packages stored in: /home/spark/.ivy2/jars\n",
      "com.google.cloud.spark#spark-bigquery-with-dependencies_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1e29bff5-fca5-4da6-aeb1-3b00208f82d7;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.23.2 in central\n",
      ":: resolution report :: resolve 179ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.23.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1e29bff5-fca5-4da6-aeb1-3b00208f82d7\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/4ms)\n",
      "24/09/30 16:33:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Criação da sessão Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark BigQuery Connection\") \\\n",
    "    .config('spark.jars.packages', 'com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2') \\\n",
    "    .config(\"spark.jars\", \"/usr/local/lib/spark-connectors/bigquery-connector-hadoop2-latest.jar\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true -Dio.netty.noUnsafe=true\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true -Dio.netty.noUnsafe=true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"viewsEnabled\", True)\n",
    "spark.conf.set(\"materializationDataset\", \"SOR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n",
    "sc._jsc.hadoopConfiguration().set('fs.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem')\n",
    "sc._jsc.hadoopConfiguration().set('fs.gs.auth.service.account.json.keyfile', '/usr/local/lib/gcp/credentials/my-project-1508437523553-e9bafe7e3368.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para salvar DataFrame em formato Parquet\n",
    "def save_to_bigquery(df, dataset, table_name):\n",
    "    # Salva o DataFrame em formato Parquet\n",
    "    df.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"table\", f\"{dataset.upper()}.{table_name}\") \\\n",
    "    .option(\"temporaryGcsBucket\", \"meu-bucket-temporario-spark\") \\\n",
    "    .option(\"credentialsFile\", \"/usr/local/lib/gcp/credentials/my-project-1508437523553-e9bafe7e3368.json\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para ler dados do BigQuery\n",
    "def read_from_bigquery(dataset, table_name):  \n",
    "    df = spark.read \\\n",
    "        .format('bigquery') \\\n",
    "        .option('table', f\"{dataset.upper()}.{table_name}\") \\\n",
    "        .option(\"credentialsFile\", \"/usr/local/lib/gcp/credentials/my-project-1508437523553-e9bafe7e3368.json\") \\\n",
    "        .load()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dimensao = read_from_bigquery('SOR', 'tbx002_dimensao_geral')\n",
    "df_dimensao.createOrReplaceTempView(\"tbx002_dimensao_geral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_from_bigquery('SOR', 'tbx001_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário de mapeamento de renomeação de colunas\n",
    "col_rename_map = {\n",
    "    \"UF\": \"uf\",\n",
    "    \"Ano\": \"ano\",\n",
    "    \"V1013\": \"mes\",\n",
    "    \"V1012\": \"semana\",\n",
    "    \"A001B3\": \"ano_nascimento\",\n",
    "    \"A003\": \"sexo\",\n",
    "    \"A004\": \"cor_raca\",\n",
    "    \"V1023\": \"tipo_area\",\n",
    "    \"A005\": \"escolaridade\",\n",
    "    \"B002\": \"foi_posto_saude\",\n",
    "    \"B0031\": \"ficou_em_casa\",\n",
    "    \"B005\": \"ficou_internado\",\n",
    "    \"B009B\": \"resultado_covid\",\n",
    "    \"B009D\" : \"resultado_covid_2\",\n",
    "    \"B009F\": \"resultado_covid_3\",\n",
    "    \"B007\": \"tem_plano_saude\",\n",
    "    \"D0071\": \"faixa_rendimento\",\n",
    "    \"F001\": \"situacao_domicilio\",\n",
    "    \"B0011\": \"teve_febre\",\n",
    "    \"B0012\": \"teve_tosse\",\n",
    "    \"B0013\": \"teve_dor_garganta\",\n",
    "    \"B0014\": \"teve_dificuldade_respirar\",\n",
    "    \"B0015\": \"teve_dor_cabeca\",\n",
    "    \"B0016\": \"teve_dor_peito\",\n",
    "    \"B0017\": \"teve_nausea\",\n",
    "    \"B0018\": \"teve_nariz_entupido_escorrendo\",\n",
    "    \"B0019\": \"teve_fadiga\",\n",
    "    \"B00110\": \"teve_dor_olhos\",\n",
    "    \"B00111\": \"teve_perda_olfato_sabor\",\n",
    "    \"B00112\": \"teve_dor_muscular\",\n",
    "    \"B00113\": \"teve_diarreia\"\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Aplicar a renomeação das colunas e selecionar apenas as colunas renomeadas\n",
    "df = df.select([F.col(old_name).alias(new_name) for old_name, new_name in col_rename_map.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uf: string (nullable = true)\n",
      " |-- ano: string (nullable = true)\n",
      " |-- mes: string (nullable = true)\n",
      " |-- semana: string (nullable = true)\n",
      " |-- ano_nascimento: string (nullable = true)\n",
      " |-- sexo: string (nullable = true)\n",
      " |-- cor_raca: string (nullable = true)\n",
      " |-- tipo_area: string (nullable = true)\n",
      " |-- escolaridade: string (nullable = true)\n",
      " |-- foi_posto_saude: string (nullable = true)\n",
      " |-- ficou_em_casa: string (nullable = true)\n",
      " |-- ficou_internado: string (nullable = true)\n",
      " |-- resultado_covid: string (nullable = true)\n",
      " |-- resultado_covid_2: string (nullable = true)\n",
      " |-- resultado_covid_3: string (nullable = true)\n",
      " |-- tem_plano_saude: string (nullable = true)\n",
      " |-- faixa_rendimento: string (nullable = true)\n",
      " |-- situacao_domicilio: string (nullable = true)\n",
      " |-- teve_febre: string (nullable = true)\n",
      " |-- teve_tosse: string (nullable = true)\n",
      " |-- teve_dor_garganta: string (nullable = true)\n",
      " |-- teve_dificuldade_respirar: string (nullable = true)\n",
      " |-- teve_dor_cabeca: string (nullable = true)\n",
      " |-- teve_dor_peito: string (nullable = true)\n",
      " |-- teve_nausea: string (nullable = true)\n",
      " |-- teve_nariz_entupido_escorrendo: string (nullable = true)\n",
      " |-- teve_fadiga: string (nullable = true)\n",
      " |-- teve_dor_olhos: string (nullable = true)\n",
      " |-- teve_perda_olfato_sabor: string (nullable = true)\n",
      " |-- teve_dor_muscular: string (nullable = true)\n",
      " |-- teve_diarreia: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"tbx001_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# realizar o join entre os dataframes, dimensionando a tabela de fatos\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    TRIM(T2.categoria_descricao) AS estado,\n",
    "    CAST(TRIM(T1.ano) AS INT) AS ano,\n",
    "    CAST(TRIM(T1.mes) AS INT) AS mes,\n",
    "    CAST(TRIM(T1.semana) AS INT) AS semana,\n",
    "    CAST(TRIM(T1.ano_nascimento) AS INT) AS ano_nascimento,\n",
    "    -- Definição da faixa etária com base na idade\n",
    "    CASE\n",
    "        WHEN (CAST(TRIM(T1.ano) AS INT) - CAST(TRIM(T1.ano_nascimento) AS INT)) BETWEEN 0 AND 17 THEN '0-17'\n",
    "        WHEN (CAST(TRIM(T1.ano) AS INT) - CAST(TRIM(T1.ano_nascimento) AS INT)) BETWEEN 18 AND 29 THEN '18-29'\n",
    "        WHEN (CAST(TRIM(T1.ano) AS INT) - CAST(TRIM(T1.ano_nascimento) AS INT)) BETWEEN 30 AND 39 THEN '30-39'\n",
    "        WHEN (CAST(TRIM(T1.ano) AS INT) - CAST(TRIM(T1.ano_nascimento) AS INT)) BETWEEN 40 AND 49 THEN '40-49'\n",
    "        WHEN (CAST(TRIM(T1.ano) AS INT) - CAST(TRIM(T1.ano_nascimento) AS INT)) BETWEEN 50 AND 59 THEN '50-59'\n",
    "        WHEN (CAST(TRIM(T1.ano) AS INT) - CAST(TRIM(T1.ano_nascimento) AS INT)) BETWEEN 60 AND 69 THEN '60-69'\n",
    "        WHEN (CAST(TRIM(T1.ano) AS INT) - CAST(TRIM(T1.ano_nascimento) AS INT)) BETWEEN 70 AND 79 THEN '70-79'\n",
    "        WHEN (CAST(TRIM(T1.ano) AS INT) - CAST(TRIM(T1.ano_nascimento) AS INT)) BETWEEN 80 AND 89 THEN '80-89'\n",
    "        WHEN (CAST(TRIM(T1.ano) AS INT) - CAST(TRIM(T1.ano_nascimento) AS INT)) BETWEEN 90 AND 99 THEN '90-99'\n",
    "        ELSE '100+'\n",
    "    END AS faixa_etaria,\n",
    "    TRIM(T3.categoria_descricao) AS sexo,\n",
    "    TRIM(T4.categoria_descricao) AS cor_raca,\n",
    "    TRIM(T5.categoria_descricao) AS tipo_area,\n",
    "    TRIM(T6.categoria_descricao) AS escolaridade,\n",
    "    CASE\n",
    "        WHEN T1.teve_febre = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_tosse = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_dor_garganta = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_dificuldade_respirar = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_dor_cabeca = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_dor_peito = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_nausea = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_nariz_entupido_escorrendo = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_fadiga = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_dor_olhos = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_perda_olfato_sabor = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_dor_muscular = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_diarreia = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_febre IS NULL\n",
    "             AND T1.teve_tosse IS NULL\n",
    "             AND T1.teve_dor_garganta IS NULL\n",
    "             AND T1.teve_dificuldade_respirar IS NULL\n",
    "             AND T1.teve_dor_cabeca IS NULL\n",
    "             AND T1.teve_dor_peito IS NULL\n",
    "             AND T1.teve_nausea IS NULL\n",
    "             AND T1.teve_nariz_entupido_escorrendo IS NULL\n",
    "             AND T1.teve_fadiga IS NULL\n",
    "             AND T1.teve_dor_olhos IS NULL\n",
    "             AND T1.teve_perda_olfato_sabor IS NULL\n",
    "             AND T1.teve_dor_muscular IS NULL\n",
    "             AND T1.teve_diarreia IS NULL \n",
    "        THEN NULL\n",
    "        ELSE 'Não'\n",
    "    END AS teve_sintomas_covid,\n",
    "    TRIM(T7.categoria_descricao) AS foi_posto_saude,\n",
    "    TRIM(T8.categoria_descricao) AS ficou_em_casa,\n",
    "    TRIM(T9.categoria_descricao) AS ficou_internado,    \n",
    "    CASE \n",
    "        when TRIM(T9.categoria_descricao) = 'Sim' or TRIM(T7.categoria_descricao) = 'Sim' then 'Sim'\n",
    "        when TRIM(T7.categoria_descricao) is null and TRIM(T9.categoria_descricao) is null then null\n",
    "        when TRIM(T9.categoria_descricao) is not null then TRIM(T9.categoria_descricao)\n",
    "        when TRIM(T7.categoria_descricao) is not null then TRIM(T7.categoria_descricao)\n",
    "        end as foi_ao_posto_ou_internado,   \n",
    "    CASE\n",
    "        WHEN T1.resultado_covid = 1 THEN 'Sim'\n",
    "        WHEN T1.resultado_covid_2 = 1 THEN 'Sim'\n",
    "        WHEN T1.resultado_covid_3 = 1 THEN 'Sim'\n",
    "        WHEN T1.resultado_covid IS NULL \n",
    "             AND T1.resultado_covid_2 IS NULL \n",
    "             AND T1.resultado_covid_3 IS NULL THEN NULL      \n",
    "        ELSE 'Não'\n",
    "    END AS teve_covid,\n",
    "    CASE\n",
    "        WHEN T10.categoria_descricao IS NOT NULL THEN TRIM(T10.categoria_descricao)\n",
    "        WHEN T11.categoria_descricao IS NOT NULL THEN TRIM(T11.categoria_descricao)\n",
    "        WHEN T12.categoria_descricao IS NOT NULL THEN TRIM(T12.categoria_descricao)\n",
    "    END AS resultado_covid,\n",
    "    TRIM(T13.categoria_descricao) AS tem_plano_saude,\n",
    "    TRIM(T14.categoria_descricao) AS faixa_rendimento,\n",
    "    TRIM(T15.categoria_descricao) AS situacao_domicilio\n",
    "FROM tbx001_data T1\n",
    "LEFT JOIN tbx002_dimensao_geral T2 ON T2.codigo_variavel = 'UF' AND TRIM(T1.uf) = TRIM(T2.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T3 ON T3.codigo_variavel = 'A003' AND TRIM(T1.sexo) = TRIM(T3.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T4 ON T4.codigo_variavel = 'A004' AND TRIM(T1.cor_raca) = TRIM(T4.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T5 ON T5.codigo_variavel = 'V1023' AND TRIM(T1.tipo_area) = TRIM(T5.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T6 ON T6.codigo_variavel = 'A005' AND TRIM(T1.escolaridade) = TRIM(T6.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T7 ON T7.codigo_variavel = 'B002' AND TRIM(T1.foi_posto_saude) = TRIM(T7.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T8 ON T8.codigo_variavel = 'B0031' AND TRIM(T1.ficou_em_casa) = TRIM(T8.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T9 ON T9.codigo_variavel = 'B005' AND TRIM(T1.ficou_internado) = TRIM(T9.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T10 ON T10.codigo_variavel = 'B009B' AND TRIM(T1.resultado_covid) = TRIM(T10.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T11 ON T11.codigo_variavel = 'B009D' AND TRIM(T1.resultado_covid_2) = TRIM(T11.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T12 ON T12.codigo_variavel = 'B009F' AND TRIM(T1.resultado_covid_3) = TRIM(T12.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T13 ON T13.codigo_variavel = 'B007' AND TRIM(T1.tem_plano_saude) = TRIM(T13.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T14 ON T14.codigo_variavel = 'D0071' AND TRIM(T1.faixa_rendimento) = TRIM(T14.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T15 ON T15.codigo_variavel = 'F001' AND TRIM(T1.situacao_domicilio) = TRIM(T15.categoria_tipo)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar a consulta SQL\n",
    "df_joined = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(df_joined.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- estado: string (nullable = true)\n",
      " |-- ano: integer (nullable = true)\n",
      " |-- mes: integer (nullable = true)\n",
      " |-- semana: integer (nullable = true)\n",
      " |-- ano_nascimento: integer (nullable = true)\n",
      " |-- faixa_etaria: string (nullable = false)\n",
      " |-- sexo: string (nullable = true)\n",
      " |-- cor_raca: string (nullable = true)\n",
      " |-- tipo_area: string (nullable = true)\n",
      " |-- escolaridade: string (nullable = true)\n",
      " |-- teve_sintomas_covid: string (nullable = true)\n",
      " |-- foi_posto_saude: string (nullable = true)\n",
      " |-- ficou_em_casa: string (nullable = true)\n",
      " |-- ficou_internado: string (nullable = true)\n",
      " |-- foi_ao_posto_ou_internado: string (nullable = true)\n",
      " |-- teve_covid: string (nullable = true)\n",
      " |-- resultado_covid: string (nullable = true)\n",
      " |-- tem_plano_saude: string (nullable = true)\n",
      " |-- faixa_rendimento: string (nullable = true)\n",
      " |-- situacao_domicilio: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 17:08:39 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx001_data, parameters sent from Spark:|requiredColumns=[UF,Ano,V1013,V1012,A001B3,A003,A004,V1023,A005,B002,B0031,B005,B009B,B009D,B009F,B007,D0071,F001,B0011,B0012,B0013,B0014,B0015,B0016,B0017,B0018,B0019,B00110,B00111,B00112,B00113],|filters=[]\n",
      "24/09/30 17:08:40 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEF1bzN1S1ExVVNvMxoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:08:39.935Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:08:40.592Z\",\"readSessionPrepDuration\":226,\"readSessionCreationDuration\":431,\"readSessionDuration\":657}\n",
      "24/09/30 17:08:40 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 4 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEF1bzN1S1ExVVNvMxoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:08:40 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx001_data': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEF1bzN1S1ExVVNvMxoCdngaAnVo\n",
      "24/09/30 17:08:40 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,UF)]\n",
      "24/09/30 17:08:41 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDAxeXlLZWdlR0MxcRoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:08:40.602Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:08:41.260Z\",\"readSessionPrepDuration\":227,\"readSessionCreationDuration\":431,\"readSessionDuration\":658}\n",
      "24/09/30 17:08:41 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDAxeXlLZWdlR0MxcRoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:08:41 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDAxeXlLZWdlR0MxcRoCdngaAnVo\n",
      "24/09/30 17:08:41 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,A003)]\n",
      "24/09/30 17:08:41 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDExjY2cyUmRxejVXZBoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:08:41.266Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:08:41.761Z\",\"readSessionPrepDuration\":185,\"readSessionCreationDuration\":310,\"readSessionDuration\":495}\n",
      "24/09/30 17:08:41 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDExjY2cyUmRxejVXZBoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:08:41 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDExjY2cyUmRxejVXZBoCdngaAnVo\n",
      "24/09/30 17:08:41 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,A004)]\n",
      "24/09/30 17:08:42 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEhiNWtmRThLcnRVXxoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:08:41.763Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:08:42.270Z\",\"readSessionPrepDuration\":220,\"readSessionCreationDuration\":287,\"readSessionDuration\":507}\n",
      "24/09/30 17:08:42 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEhiNWtmRThLcnRVXxoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:08:42 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEhiNWtmRThLcnRVXxoCdngaAnVo\n",
      "24/09/30 17:08:42 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,V1023)]\n",
      "24/09/30 17:08:42 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDl2SGI4RXBwYkFFNhoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:08:42.274Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:08:42.793Z\",\"readSessionPrepDuration\":225,\"readSessionCreationDuration\":294,\"readSessionDuration\":519}\n",
      "24/09/30 17:08:42 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDl2SGI4RXBwYkFFNhoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:08:42 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDl2SGI4RXBwYkFFNhoCdngaAnVo\n",
      "24/09/30 17:08:42 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,A005)]\n",
      "24/09/30 17:08:43 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHk3UDBKZURFbGk3NxoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:08:42.797Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:08:43.306Z\",\"readSessionPrepDuration\":216,\"readSessionCreationDuration\":293,\"readSessionDuration\":509}\n",
      "24/09/30 17:08:43 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHk3UDBKZURFbGk3NxoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:08:43 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHk3UDBKZURFbGk3NxoCdngaAnVo\n",
      "24/09/30 17:08:43 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B002)]\n",
      "24/09/30 17:08:43 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFdjRmdRbGRhRjdWcRoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:08:43.309Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:08:43.849Z\",\"readSessionPrepDuration\":233,\"readSessionCreationDuration\":307,\"readSessionDuration\":540}\n",
      "24/09/30 17:08:43 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFdjRmdRbGRhRjdWcRoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:08:43 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFdjRmdRbGRhRjdWcRoCdngaAnVo\n",
      "24/09/30 17:08:43 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B0031)]\n",
      "24/09/30 17:08:44 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHRTdnBxVXJMcUF5WRoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:08:43.851Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:08:44.357Z\",\"readSessionPrepDuration\":220,\"readSessionCreationDuration\":286,\"readSessionDuration\":506}\n",
      "24/09/30 17:08:44 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHRTdnBxVXJMcUF5WRoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:08:44 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHRTdnBxVXJMcUF5WRoCdngaAnVo\n",
      "24/09/30 17:08:44 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B005)]\n",
      "24/09/30 17:08:44 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFZBZ3JUSFp6TXEwehoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:08:44.359Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:08:44.909Z\",\"readSessionPrepDuration\":214,\"readSessionCreationDuration\":336,\"readSessionDuration\":550}\n",
      "24/09/30 17:08:44 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFZBZ3JUSFp6TXEwehoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:08:44 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFZBZ3JUSFp6TXEwehoCdngaAnVo\n",
      "24/09/30 17:08:44 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B009B)]\n",
      "24/09/30 17:08:45 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEgwMEc2d0VYMkhYRxoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:08:44.912Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:08:45.418Z\",\"readSessionPrepDuration\":221,\"readSessionCreationDuration\":285,\"readSessionDuration\":506}\n",
      "24/09/30 17:08:45 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEgwMEc2d0VYMkhYRxoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:08:45 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEgwMEc2d0VYMkhYRxoCdngaAnVo\n",
      "24/09/30 17:08:45 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B009D)]\n",
      "24/09/30 17:08:45 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFNFTzA3ejZTSjhRdxoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:08:45.421Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:08:45.938Z\",\"readSessionPrepDuration\":214,\"readSessionCreationDuration\":303,\"readSessionDuration\":517}\n",
      "24/09/30 17:08:45 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFNFTzA3ejZTSjhRdxoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:08:45 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFNFTzA3ejZTSjhRdxoCdngaAnVo\n",
      "24/09/30 17:08:45 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B009F)]\n",
      "24/09/30 17:08:46 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFV3QVlpR1ltUGRISRoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:08:45.940Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:08:46.432Z\",\"readSessionPrepDuration\":198,\"readSessionCreationDuration\":294,\"readSessionDuration\":492}\n",
      "24/09/30 17:08:46 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFV3QVlpR1ltUGRISRoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:08:46 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFV3QVlpR1ltUGRISRoCdngaAnVo\n",
      "24/09/30 17:08:46 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B007)]\n",
      "24/09/30 17:08:46 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDG1OQjd5RmphcHpGNRoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:08:46.434Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:08:46.912Z\",\"readSessionPrepDuration\":193,\"readSessionCreationDuration\":285,\"readSessionDuration\":478}\n",
      "24/09/30 17:08:46 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDG1OQjd5RmphcHpGNRoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:08:46 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDG1OQjd5RmphcHpGNRoCdngaAnVo\n",
      "24/09/30 17:08:46 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,D0071)]\n",
      "24/09/30 17:08:47 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHZLVF9QZDcwQ1FJcRoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:08:46.914Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:08:47.399Z\",\"readSessionPrepDuration\":190,\"readSessionCreationDuration\":295,\"readSessionDuration\":485}\n",
      "24/09/30 17:08:47 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHZLVF9QZDcwQ1FJcRoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:08:47 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHZLVF9QZDcwQ1FJcRoCdngaAnVo\n",
      "24/09/30 17:08:47 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,F001)]\n",
      "24/09/30 17:08:47 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFBYc0Rkd0lqLW02ZxoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:08:47.403Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:08:47.889Z\",\"readSessionPrepDuration\":191,\"readSessionCreationDuration\":295,\"readSessionDuration\":486}\n",
      "24/09/30 17:08:47 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFBYc0Rkd0lqLW02ZxoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:08:47 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFBYc0Rkd0lqLW02ZxoCdngaAnVo\n",
      "24/09/30 17:08:47 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:47 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:47 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:47 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Got job 45 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Final stage: ResultStage 45 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[162] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:08:47 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:47 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:47 INFO MemoryStore: Block broadcast_87 stored as values in memory (estimated size 20.5 KiB, free 309.9 MiB)\n",
      "24/09/30 17:08:47 INFO MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 309.9 MiB)\n",
      "24/09/30 17:08:47 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:47 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:47 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on 10.0.2.15:33255 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:08:47 INFO SparkContext: Created broadcast 87 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[162] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:08:47 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:08:47 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:47 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 51) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:08:47 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Got job 46 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Final stage: ResultStage 46 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Submitting ResultStage 46 (MapPartitionsRDD[165] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:08:47 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:47 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:47 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:47 INFO MemoryStore: Block broadcast_88 stored as values in memory (estimated size 20.5 KiB, free 309.8 MiB)\n",
      "24/09/30 17:08:47 INFO Executor: Running task 0.0 in stage 45.0 (TID 51)\n",
      "24/09/30 17:08:47 INFO MemoryStore: Block broadcast_88_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 309.8 MiB)\n",
      "24/09/30 17:08:47 INFO BlockManagerInfo: Added broadcast_88_piece0 in memory on 10.0.2.15:33255 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:08:47 INFO SparkContext: Created broadcast 88 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:08:47 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (MapPartitionsRDD[165] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:08:47 INFO TaskSchedulerImpl: Adding task set 46.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Got job 47 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Final stage: ResultStage 47 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Submitting ResultStage 47 (MapPartitionsRDD[176] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:08:47 INFO MemoryStore: Block broadcast_89 stored as values in memory (estimated size 20.5 KiB, free 309.8 MiB)\n",
      "24/09/30 17:08:47 INFO TaskSetManager: Starting task 0.0 in stage 46.0 (TID 52) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:08:47 INFO MemoryStore: Block broadcast_89_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 309.8 MiB)\n",
      "24/09/30 17:08:47 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on 10.0.2.15:33255 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:08:47 INFO Executor: Running task 0.0 in stage 46.0 (TID 52)\n",
      "24/09/30 17:08:47 INFO SparkContext: Created broadcast 89 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 47 (MapPartitionsRDD[176] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:08:47 INFO TaskSchedulerImpl: Adding task set 47.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Got job 49 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Final stage: ResultStage 48 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Submitting ResultStage 48 (MapPartitionsRDD[161] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:08:47 INFO MemoryStore: Block broadcast_90 stored as values in memory (estimated size 20.5 KiB, free 309.8 MiB)\n",
      "24/09/30 17:08:47 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 53) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:08:47 INFO MemoryStore: Block broadcast_90_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 309.8 MiB)\n",
      "24/09/30 17:08:47 INFO Executor: Running task 0.0 in stage 47.0 (TID 53)\n",
      "24/09/30 17:08:47 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on 10.0.2.15:33255 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:08:47 INFO SparkContext: Created broadcast 90 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 48 (MapPartitionsRDD[161] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:08:47 INFO TaskSchedulerImpl: Adding task set 48.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Got job 50 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Final stage: ResultStage 49 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:08:47 INFO DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[170] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:08:47 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 54) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:08:47 INFO MemoryStore: Block broadcast_91 stored as values in memory (estimated size 20.5 KiB, free 309.8 MiB)\n",
      "24/09/30 17:08:47 INFO MemoryStore: Block broadcast_91_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 309.7 MiB)\n",
      "24/09/30 17:08:47 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on 10.0.2.15:33255 (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/30 17:08:47 INFO Executor: Running task 0.0 in stage 48.0 (TID 54)\n",
      "24/09/30 17:08:48 INFO SparkContext: Created broadcast 91 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[170] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Adding task set 49.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Got job 48 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Final stage: ResultStage 50 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[174] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_92 stored as values in memory (estimated size 20.5 KiB, free 309.7 MiB)\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_92_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 309.7 MiB)\n",
      "24/09/30 17:08:48 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on 10.0.2.15:33255 (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/30 17:08:48 INFO SparkContext: Created broadcast 92 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 50 (MapPartitionsRDD[174] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Adding task set 50.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Got job 51 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Final stage: ResultStage 51 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Submitting ResultStage 51 (MapPartitionsRDD[168] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_93 stored as values in memory (estimated size 20.5 KiB, free 309.7 MiB)\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_93_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 309.7 MiB)\n",
      "24/09/30 17:08:48 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on 10.0.2.15:33255 (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/30 17:08:48 INFO SparkContext: Created broadcast 93 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 51 (MapPartitionsRDD[168] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Adding task set 51.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Got job 52 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Final stage: ResultStage 52 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Submitting ResultStage 52 (MapPartitionsRDD[166] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_94 stored as values in memory (estimated size 20.5 KiB, free 309.7 MiB)\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_94_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 309.7 MiB)\n",
      "24/09/30 17:08:48 INFO BlockManagerInfo: Added broadcast_94_piece0 in memory on 10.0.2.15:33255 (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/30 17:08:48 INFO SparkContext: Created broadcast 94 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[166] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Adding task set 52.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Got job 53 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Final stage: ResultStage 53 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[172] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_95 stored as values in memory (estimated size 20.5 KiB, free 309.6 MiB)\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_95_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 309.6 MiB)\n",
      "24/09/30 17:08:48 INFO BlockManagerInfo: Added broadcast_95_piece0 in memory on 10.0.2.15:33255 (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/30 17:08:48 INFO SparkContext: Created broadcast 95 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 53 (MapPartitionsRDD[172] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Adding task set 53.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Got job 54 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Final stage: ResultStage 54 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Submitting ResultStage 54 (MapPartitionsRDD[180] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_96 stored as values in memory (estimated size 20.5 KiB, free 309.6 MiB)\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_96_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 309.6 MiB)\n",
      "24/09/30 17:08:48 INFO BlockManagerInfo: Added broadcast_96_piece0 in memory on 10.0.2.15:33255 (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/30 17:08:48 INFO SparkContext: Created broadcast 96 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 54 (MapPartitionsRDD[180] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Adding task set 54.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Got job 55 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Final stage: ResultStage 55 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Submitting ResultStage 55 (MapPartitionsRDD[182] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_97 stored as values in memory (estimated size 20.5 KiB, free 309.6 MiB)\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_97_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 309.6 MiB)\n",
      "24/09/30 17:08:48 INFO BlockManagerInfo: Added broadcast_97_piece0 in memory on 10.0.2.15:33255 (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/30 17:08:48 INFO SparkContext: Created broadcast 97 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 55 (MapPartitionsRDD[182] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Adding task set 55.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Got job 56 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Final stage: ResultStage 56 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Submitting ResultStage 56 (MapPartitionsRDD[184] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_98 stored as values in memory (estimated size 20.5 KiB, free 309.5 MiB)\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_98_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 309.5 MiB)\n",
      "24/09/30 17:08:48 INFO BlockManagerInfo: Added broadcast_98_piece0 in memory on 10.0.2.15:33255 (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/30 17:08:48 INFO SparkContext: Created broadcast 98 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 56 (MapPartitionsRDD[184] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Adding task set 56.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Got job 57 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Final stage: ResultStage 57 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Submitting ResultStage 57 (MapPartitionsRDD[186] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_99 stored as values in memory (estimated size 20.5 KiB, free 309.5 MiB)\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_99_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 309.5 MiB)\n",
      "24/09/30 17:08:48 INFO BlockManagerInfo: Added broadcast_99_piece0 in memory on 10.0.2.15:33255 (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/30 17:08:48 INFO SparkContext: Created broadcast 99 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 57 (MapPartitionsRDD[186] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Adding task set 57.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Got job 58 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Final stage: ResultStage 58 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Submitting ResultStage 58 (MapPartitionsRDD[179] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_100 stored as values in memory (estimated size 20.5 KiB, free 309.5 MiB)\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_100_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 309.5 MiB)\n",
      "24/09/30 17:08:48 INFO BlockManagerInfo: Added broadcast_100_piece0 in memory on 10.0.2.15:33255 (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/30 17:08:48 INFO SparkContext: Created broadcast 100 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 58 (MapPartitionsRDD[179] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Adding task set 58.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:08:48 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDAxeXlLZWdlR0MxcRoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:08:47.980Z\",\"Ended\":\"2024-09-30T20:08:48.397Z\",\"Parse Timings\":\"Average: PT0.000303442S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.395797058S Samples: 1\",\"Bytes/s\":1964,\"Rows/s\":0,\"Bytes\":776,\"Rows\":27,\"I/O time\":395}\n",
      "24/09/30 17:08:48 INFO Executor: Finished task 0.0 in stage 45.0 (TID 51). 2060 bytes result sent to driver\n",
      "24/09/30 17:08:48 INFO TaskSetManager: Starting task 0.0 in stage 50.0 (TID 55) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:08:48 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 51) in 436 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:08:48 INFO Executor: Running task 0.0 in stage 50.0 (TID 55)\n",
      "24/09/30 17:08:48 INFO DAGScheduler: ResultStage 45 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0,447 s\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Job 45 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 45: Stage finished\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Job 45 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 0,458165 s\n",
      "24/09/30 17:08:48 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEhiNWtmRThLcnRVXxoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:08:47.990Z\",\"Ended\":\"2024-09-30T20:08:48.408Z\",\"Parse Timings\":\"Average: PT0.00013038S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.387775921S Samples: 1\",\"Bytes/s\":868,\"Rows/s\":0,\"Bytes\":336,\"Rows\":6,\"I/O time\":387}\n",
      "24/09/30 17:08:48 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDl2SGI4RXBwYkFFNhoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:08:48.006Z\",\"Ended\":\"2024-09-30T20:08:48.408Z\",\"Parse Timings\":\"Average: PT0.000152086S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.372030234S Samples: 1\",\"Bytes/s\":1376,\"Rows/s\":0,\"Bytes\":512,\"Rows\":4,\"I/O time\":372}\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_101 stored as values in memory (estimated size 4.0 MiB, free 305.5 MiB)\n",
      "24/09/30 17:08:48 INFO Executor: Finished task 0.0 in stage 48.0 (TID 54). 1702 bytes result sent to driver\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_101_piece0 stored as bytes in memory (estimated size 1038.0 B, free 305.5 MiB)\n",
      "24/09/30 17:08:48 INFO BlockManagerInfo: Added broadcast_101_piece0 in memory on 10.0.2.15:33255 (size: 1038.0 B, free: 366.1 MiB)\n",
      "24/09/30 17:08:48 INFO Executor: Finished task 0.0 in stage 46.0 (TID 52). 1602 bytes result sent to driver\n",
      "24/09/30 17:08:48 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 56) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:08:48 INFO SparkContext: Created broadcast 101 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:48 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 57) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:08:48 INFO Executor: Running task 0.0 in stage 51.0 (TID 57)\n",
      "24/09/30 17:08:48 INFO Executor: Running task 0.0 in stage 49.0 (TID 56)\n",
      "24/09/30 17:08:48 INFO TaskSetManager: Finished task 0.0 in stage 46.0 (TID 52) in 432 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Removed TaskSet 46.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:08:48 INFO DAGScheduler: ResultStage 46 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0,445 s\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Job 46 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 46: Stage finished\n",
      "24/09/30 17:08:48 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 54) in 419 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:08:48 INFO DAGScheduler: ResultStage 48 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0,427 s\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Job 49 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 48: Stage finished\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Job 49 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 0,462213 s\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Job 46 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 0,463872 s\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_102 stored as values in memory (estimated size 4.0 MiB, free 301.5 MiB)\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_102_piece0 stored as bytes in memory (estimated size 446.0 B, free 301.5 MiB)\n",
      "24/09/30 17:08:48 INFO BlockManagerInfo: Added broadcast_102_piece0 in memory on 10.0.2.15:33255 (size: 446.0 B, free: 366.1 MiB)\n",
      "24/09/30 17:08:48 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDExjY2cyUmRxejVXZBoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:08:47.995Z\",\"Ended\":\"2024-09-30T20:08:48.426Z\",\"Parse Timings\":\"Average: PT0.000079529S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.412246408S Samples: 1\",\"Bytes/s\":679,\"Rows/s\":0,\"Bytes\":280,\"Rows\":2,\"I/O time\":412}\n",
      "24/09/30 17:08:48 INFO SparkContext: Created broadcast 102 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:48 INFO Executor: Finished task 0.0 in stage 47.0 (TID 53). 1503 bytes result sent to driver\n",
      "24/09/30 17:08:48 INFO TaskSetManager: Starting task 0.0 in stage 52.0 (TID 58) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:08:48 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 53) in 461 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_103 stored as values in memory (estimated size 4.0 MiB, free 297.5 MiB)\n",
      "24/09/30 17:08:48 INFO DAGScheduler: ResultStage 47 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0,472 s\n",
      "24/09/30 17:08:48 INFO Executor: Running task 0.0 in stage 52.0 (TID 58)\n",
      "24/09/30 17:08:48 INFO BlockManagerInfo: Removed broadcast_87_piece0 on 10.0.2.15:33255 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_103_piece0 stored as bytes in memory (estimated size 359.0 B, free 297.5 MiB)\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Job 47 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 47: Stage finished\n",
      "24/09/30 17:08:48 INFO BlockManagerInfo: Added broadcast_103_piece0 in memory on 10.0.2.15:33255 (size: 359.0 B, free: 366.1 MiB)\n",
      "24/09/30 17:08:48 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx001_data, parameters sent from Spark:|requiredColumns=[UF,Ano,V1013,V1012,A001B3,A003,A004,V1023,A005,B002,B0031,B005,B009B,B009D,B009F,B007,D0071,F001,B0011,B0012,B0013,B0014,B0015,B0016,B0017,B0018,B0019,B00110,B00111,B00112,B00113],|filters=[]\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Job 47 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 0,513377 s\n",
      "24/09/30 17:08:48 INFO SparkContext: Created broadcast 103 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_104 stored as values in memory (estimated size 4.0 MiB, free 293.5 MiB)\n",
      "24/09/30 17:08:48 INFO BlockManagerInfo: Removed broadcast_90_piece0 on 10.0.2.15:33255 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_104_piece0 stored as bytes in memory (estimated size 215.0 B, free 293.5 MiB)\n",
      "24/09/30 17:08:48 INFO BlockManagerInfo: Added broadcast_104_piece0 in memory on 10.0.2.15:33255 (size: 215.0 B, free: 366.1 MiB)\n",
      "24/09/30 17:08:48 INFO SparkContext: Created broadcast 104 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:48 INFO BlockManagerInfo: Removed broadcast_88_piece0 on 10.0.2.15:33255 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/30 17:08:48 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHRTdnBxVXJMcUF5WRoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:08:48.415Z\",\"Ended\":\"2024-09-30T20:08:48.763Z\",\"Parse Timings\":\"Average: PT0.000090523S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.319073437S Samples: 1\",\"Bytes/s\":877,\"Rows/s\":0,\"Bytes\":280,\"Rows\":3,\"I/O time\":319}\n",
      "24/09/30 17:08:48 INFO Executor: Finished task 0.0 in stage 51.0 (TID 57). 1564 bytes result sent to driver\n",
      "24/09/30 17:08:48 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 59) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:08:48 INFO Executor: Running task 0.0 in stage 53.0 (TID 59)\n",
      "24/09/30 17:08:48 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 57) in 353 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:08:48 INFO DAGScheduler: ResultStage 51 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0,756 s\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Job 51 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 51: Stage finished\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Job 51 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 0,812254 s\n",
      "24/09/30 17:08:48 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFdjRmdRbGRhRjdWcRoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:08:48.407Z\",\"Ended\":\"2024-09-30T20:08:48.774Z\",\"Parse Timings\":\"Average: PT0.00006494S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.329931767S Samples: 1\",\"Bytes/s\":851,\"Rows/s\":0,\"Bytes\":280,\"Rows\":3,\"I/O time\":329}\n",
      "24/09/30 17:08:48 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHk3UDBKZURFbGk3NxoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:08:48.415Z\",\"Ended\":\"2024-09-30T20:08:48.777Z\",\"Parse Timings\":\"Average: PT0.000027043S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.324292365S Samples: 1\",\"Bytes/s\":1481,\"Rows/s\":0,\"Bytes\":480,\"Rows\":8,\"I/O time\":324}\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_105 stored as values in memory (estimated size 4.0 MiB, free 289.6 MiB)\n",
      "24/09/30 17:08:48 INFO Executor: Finished task 0.0 in stage 49.0 (TID 56). 1748 bytes result sent to driver\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_105_piece0 stored as bytes in memory (estimated size 244.0 B, free 289.6 MiB)\n",
      "24/09/30 17:08:48 INFO Executor: Finished task 0.0 in stage 50.0 (TID 55). 1564 bytes result sent to driver\n",
      "24/09/30 17:08:48 INFO BlockManagerInfo: Added broadcast_105_piece0 in memory on 10.0.2.15:33255 (size: 244.0 B, free: 366.1 MiB)\n",
      "24/09/30 17:08:48 INFO SparkContext: Created broadcast 105 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:48 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 60) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:08:48 INFO TaskSetManager: Starting task 0.0 in stage 55.0 (TID 61) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:08:48 INFO Executor: Running task 0.0 in stage 54.0 (TID 60)\n",
      "24/09/30 17:08:48 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 56) in 370 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:08:48 INFO TaskSetManager: Finished task 0.0 in stage 50.0 (TID 55) in 382 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:08:48 INFO DAGScheduler: ResultStage 49 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0,788 s\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Job 50 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 49: Stage finished\n",
      "24/09/30 17:08:48 INFO DAGScheduler: ResultStage 50 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0,779 s\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Job 48 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 50: Stage finished\n",
      "24/09/30 17:08:48 INFO Executor: Running task 0.0 in stage 55.0 (TID 61)\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Job 48 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 0,831479 s\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Job 50 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 0,832058 s\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_106 stored as values in memory (estimated size 4.0 MiB, free 285.6 MiB)\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_107 stored as values in memory (estimated size 4.0 MiB, free 281.6 MiB)\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_106_piece0 stored as bytes in memory (estimated size 244.0 B, free 281.6 MiB)\n",
      "24/09/30 17:08:48 INFO BlockManagerInfo: Added broadcast_106_piece0 in memory on 10.0.2.15:33255 (size: 244.0 B, free: 366.1 MiB)\n",
      "24/09/30 17:08:48 INFO SparkContext: Created broadcast 106 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:48 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFZBZ3JUSFp6TXEwehoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:08:48.461Z\",\"Ended\":\"2024-09-30T20:08:48.807Z\",\"Parse Timings\":\"Average: PT0.000201145S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.312536131S Samples: 1\",\"Bytes/s\":1025,\"Rows/s\":0,\"Bytes\":320,\"Rows\":4,\"I/O time\":312}\n",
      "24/09/30 17:08:48 INFO Executor: Finished task 0.0 in stage 52.0 (TID 58). 1562 bytes result sent to driver\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_107_piece0 stored as bytes in memory (estimated size 492.0 B, free 281.6 MiB)\n",
      "24/09/30 17:08:48 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 62) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:08:48 INFO Executor: Running task 0.0 in stage 56.0 (TID 62)\n",
      "24/09/30 17:08:48 INFO TaskSetManager: Finished task 0.0 in stage 52.0 (TID 58) in 371 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Removed TaskSet 52.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:08:48 INFO BlockManagerInfo: Added broadcast_107_piece0 in memory on 10.0.2.15:33255 (size: 492.0 B, free: 366.1 MiB)\n",
      "24/09/30 17:08:48 INFO SparkContext: Created broadcast 107 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:48 INFO DAGScheduler: ResultStage 52 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0,808 s\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Job 52 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:08:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 52: Stage finished\n",
      "24/09/30 17:08:48 INFO DAGScheduler: Job 52 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 0,867956 s\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_108 stored as values in memory (estimated size 4.0 MiB, free 277.6 MiB)\n",
      "24/09/30 17:08:48 INFO MemoryStore: Block broadcast_108_piece0 stored as bytes in memory (estimated size 295.0 B, free 277.6 MiB)\n",
      "24/09/30 17:08:48 INFO BlockManagerInfo: Added broadcast_108_piece0 in memory on 10.0.2.15:33255 (size: 295.0 B, free: 366.1 MiB)\n",
      "24/09/30 17:08:48 INFO SparkContext: Created broadcast 108 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:49 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEpMNjg5X1lKOHFqRxoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:08:48.461Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:08:49.052Z\",\"readSessionPrepDuration\":210,\"readSessionCreationDuration\":381,\"readSessionDuration\":591}\n",
      "24/09/30 17:08:49 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 4 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEpMNjg5X1lKOHFqRxoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:08:49 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx001_data': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEpMNjg5X1lKOHFqRxoCdngaAnVo\n",
      "24/09/30 17:08:49 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx001_data, parameters sent from Spark:|requiredColumns=[UF,Ano,V1013,V1012,A001B3,A003,A004,V1023,A005,B002,B0031,B005,B009B,B009D,B009F,B007,D0071,F001,B0011,B0012,B0013,B0014,B0015,B0016,B0017,B0018,B0019,B00110,B00111,B00112,B00113],|filters=[]\n",
      "24/09/30 17:08:49 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEgwMEc2d0VYMkhYRxoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:08:48.768Z\",\"Ended\":\"2024-09-30T20:08:49.098Z\",\"Parse Timings\":\"Average: PT0.00004201S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.302850777S Samples: 1\",\"Bytes/s\":1165,\"Rows/s\":0,\"Bytes\":352,\"Rows\":5,\"I/O time\":302}\n",
      "24/09/30 17:08:49 INFO Executor: Finished task 0.0 in stage 53.0 (TID 59). 1596 bytes result sent to driver\n",
      "24/09/30 17:08:49 INFO TaskSetManager: Starting task 0.0 in stage 57.0 (TID 63) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:08:49 INFO Executor: Running task 0.0 in stage 57.0 (TID 63)\n",
      "24/09/30 17:08:49 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 59) in 336 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:08:49 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:08:49 INFO DAGScheduler: ResultStage 53 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1,082 s\n",
      "24/09/30 17:08:49 INFO DAGScheduler: Job 53 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:08:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 53: Stage finished\n",
      "24/09/30 17:08:49 INFO DAGScheduler: Job 53 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 1,143521 s\n",
      "24/09/30 17:08:49 INFO MemoryStore: Block broadcast_109 stored as values in memory (estimated size 4.0 MiB, free 273.6 MiB)\n",
      "24/09/30 17:08:49 INFO MemoryStore: Block broadcast_109_piece0 stored as bytes in memory (estimated size 348.0 B, free 273.6 MiB)\n",
      "24/09/30 17:08:49 INFO BlockManagerInfo: Added broadcast_109_piece0 in memory on 10.0.2.15:33255 (size: 348.0 B, free: 366.1 MiB)\n",
      "24/09/30 17:08:49 INFO SparkContext: Created broadcast 109 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:49 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFV3QVlpR1ltUGRISRoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:08:48.785Z\",\"Ended\":\"2024-09-30T20:08:49.127Z\",\"Parse Timings\":\"Average: PT0.000036748S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.297873369S Samples: 1\",\"Bytes/s\":1185,\"Rows/s\":0,\"Bytes\":352,\"Rows\":5,\"I/O time\":297}\n",
      "24/09/30 17:08:49 INFO Executor: Finished task 0.0 in stage 55.0 (TID 61). 1596 bytes result sent to driver\n",
      "24/09/30 17:08:49 INFO TaskSetManager: Starting task 0.0 in stage 58.0 (TID 64) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:08:49 INFO Executor: Running task 0.0 in stage 58.0 (TID 64)\n",
      "24/09/30 17:08:49 INFO TaskSetManager: Finished task 0.0 in stage 55.0 (TID 61) in 348 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:08:49 INFO TaskSchedulerImpl: Removed TaskSet 55.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:08:49 INFO DAGScheduler: ResultStage 55 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1,102 s\n",
      "24/09/30 17:08:49 INFO DAGScheduler: Job 55 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:08:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 55: Stage finished\n",
      "24/09/30 17:08:49 INFO DAGScheduler: Job 55 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 1,161142 s\n",
      "24/09/30 17:08:49 INFO MemoryStore: Block broadcast_110 stored as values in memory (estimated size 4.0 MiB, free 269.6 MiB)\n",
      "24/09/30 17:08:49 INFO MemoryStore: Block broadcast_110_piece0 stored as bytes in memory (estimated size 348.0 B, free 269.6 MiB)\n",
      "24/09/30 17:08:49 INFO BlockManagerInfo: Added broadcast_110_piece0 in memory on 10.0.2.15:33255 (size: 348.0 B, free: 366.1 MiB)\n",
      "24/09/30 17:08:49 INFO SparkContext: Created broadcast 110 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:49 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDG1OQjd5RmphcHpGNRoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:08:48.784Z\",\"Ended\":\"2024-09-30T20:08:49.140Z\",\"Parse Timings\":\"Average: PT0.000029298S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.317297512S Samples: 1\",\"Bytes/s\":883,\"Rows/s\":0,\"Bytes\":280,\"Rows\":3,\"I/O time\":317}\n",
      "24/09/30 17:08:49 INFO Executor: Finished task 0.0 in stage 54.0 (TID 60). 1521 bytes result sent to driver\n",
      "24/09/30 17:08:49 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 60) in 362 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:08:49 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:08:49 INFO DAGScheduler: ResultStage 54 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1,118 s\n",
      "24/09/30 17:08:49 INFO DAGScheduler: Job 54 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:08:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 54: Stage finished\n",
      "24/09/30 17:08:49 INFO DAGScheduler: Job 54 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 1,173946 s\n",
      "24/09/30 17:08:49 INFO MemoryStore: Block broadcast_111 stored as values in memory (estimated size 4.0 MiB, free 265.6 MiB)\n",
      "24/09/30 17:08:49 INFO MemoryStore: Block broadcast_111_piece0 stored as bytes in memory (estimated size 244.0 B, free 265.6 MiB)\n",
      "24/09/30 17:08:49 INFO BlockManagerInfo: Added broadcast_111_piece0 in memory on 10.0.2.15:33255 (size: 244.0 B, free: 366.1 MiB)\n",
      "24/09/30 17:08:49 INFO SparkContext: Created broadcast 111 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:49 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFNFTzA3ejZTSjhRdxoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:08:48.823Z\",\"Ended\":\"2024-09-30T20:08:49.149Z\",\"Parse Timings\":\"Average: PT0.000041289S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.286517192S Samples: 1\",\"Bytes/s\":1230,\"Rows/s\":0,\"Bytes\":352,\"Rows\":5,\"I/O time\":286}\n",
      "24/09/30 17:08:49 INFO Executor: Finished task 0.0 in stage 56.0 (TID 62). 1596 bytes result sent to driver\n",
      "24/09/30 17:08:49 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 62) in 335 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:08:49 INFO TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:08:49 INFO DAGScheduler: ResultStage 56 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1,120 s\n",
      "24/09/30 17:08:49 INFO DAGScheduler: Job 56 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:08:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 56: Stage finished\n",
      "24/09/30 17:08:49 INFO DAGScheduler: Job 56 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 1,177210 s\n",
      "24/09/30 17:08:49 INFO MemoryStore: Block broadcast_112 stored as values in memory (estimated size 4.0 MiB, free 261.5 MiB)\n",
      "24/09/30 17:08:49 INFO MemoryStore: Block broadcast_112_piece0 stored as bytes in memory (estimated size 348.0 B, free 261.5 MiB)\n",
      "24/09/30 17:08:49 INFO BlockManagerInfo: Added broadcast_112_piece0 in memory on 10.0.2.15:33255 (size: 348.0 B, free: 366.1 MiB)\n",
      "24/09/30 17:08:49 INFO SparkContext: Created broadcast 112 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:49 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFBYc0Rkd0lqLW02ZxoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:08:49.107Z\",\"Ended\":\"2024-09-30T20:08:49.434Z\",\"Parse Timings\":\"Average: PT0.000043446S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.300466253S Samples: 1\",\"Bytes/s\":1440,\"Rows/s\":0,\"Bytes\":432,\"Rows\":7,\"I/O time\":300}\n",
      "24/09/30 17:08:49 INFO Executor: Finished task 0.0 in stage 57.0 (TID 63). 1687 bytes result sent to driver\n",
      "24/09/30 17:08:49 INFO TaskSetManager: Finished task 0.0 in stage 57.0 (TID 63) in 338 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:08:49 INFO TaskSchedulerImpl: Removed TaskSet 57.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:08:49 INFO DAGScheduler: ResultStage 57 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1,404 s\n",
      "24/09/30 17:08:49 INFO DAGScheduler: Job 57 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:08:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 57: Stage finished\n",
      "24/09/30 17:08:49 INFO DAGScheduler: Job 57 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 1,461907 s\n",
      "24/09/30 17:08:49 INFO MemoryStore: Block broadcast_113 stored as values in memory (estimated size 4.0 MiB, free 257.5 MiB)\n",
      "24/09/30 17:08:49 INFO MemoryStore: Block broadcast_113_piece0 stored as bytes in memory (estimated size 467.0 B, free 257.5 MiB)\n",
      "24/09/30 17:08:49 INFO BlockManagerInfo: Added broadcast_113_piece0 in memory on 10.0.2.15:33255 (size: 467.0 B, free: 366.1 MiB)\n",
      "24/09/30 17:08:49 INFO SparkContext: Created broadcast 113 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:49 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHZLVF9QZDcwQ1FJcRoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:08:49.131Z\",\"Ended\":\"2024-09-30T20:08:49.456Z\",\"Parse Timings\":\"Average: PT0.000071566S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.291421132S Samples: 1\",\"Bytes/s\":934,\"Rows/s\":0,\"Bytes\":272,\"Rows\":2,\"I/O time\":291}\n",
      "24/09/30 17:08:49 INFO Executor: Finished task 0.0 in stage 58.0 (TID 64). 1500 bytes result sent to driver\n",
      "24/09/30 17:08:49 INFO TaskSetManager: Finished task 0.0 in stage 58.0 (TID 64) in 329 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:08:49 INFO TaskSchedulerImpl: Removed TaskSet 58.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:08:49 INFO DAGScheduler: ResultStage 58 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1,420 s\n",
      "24/09/30 17:08:49 INFO DAGScheduler: Job 58 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:08:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 58: Stage finished\n",
      "24/09/30 17:08:49 INFO DAGScheduler: Job 58 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 1,479884 s\n",
      "24/09/30 17:08:49 INFO MemoryStore: Block broadcast_114 stored as values in memory (estimated size 4.0 MiB, free 253.5 MiB)\n",
      "24/09/30 17:08:49 INFO MemoryStore: Block broadcast_114_piece0 stored as bytes in memory (estimated size 213.0 B, free 253.5 MiB)\n",
      "24/09/30 17:08:49 INFO BlockManagerInfo: Added broadcast_114_piece0 in memory on 10.0.2.15:33255 (size: 213.0 B, free: 366.1 MiB)\n",
      "24/09/30 17:08:49 INFO SparkContext: Created broadcast 114 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:08:49 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGZIM3BtdG1ITWFneRoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:08:49.070Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:08:49.624Z\",\"readSessionPrepDuration\":226,\"readSessionCreationDuration\":328,\"readSessionDuration\":554}\n",
      "24/09/30 17:08:49 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 4 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGZIM3BtdG1ITWFneRoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:08:49 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx001_data': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGZIM3BtdG1ITWFneRoCdngaAnVo\n",
      "24/09/30 17:08:49 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx001_data, parameters sent from Spark:|requiredColumns=[UF,Ano,V1013,V1012,A001B3,A003,A004,V1023,A005,B002,B0031,B005,B009B,B009D,B009F,B007,D0071,F001,B0011,B0012,B0013,B0014,B0015,B0016,B0017,B0018,B0019,B00110,B00111,B00112,B00113],|filters=[]\n",
      "24/09/30 17:08:50 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJWb0J4c3BzUVNfTxoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:08:49.637Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:08:50.129Z\",\"readSessionPrepDuration\":194,\"readSessionCreationDuration\":298,\"readSessionDuration\":492}\n",
      "24/09/30 17:08:50 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 4 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJWb0J4c3BzUVNfTxoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:08:50 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx001_data': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJWb0J4c3BzUVNfTxoCdngaAnVo\n",
      "24/09/30 17:08:50 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/30 17:08:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/30 17:08:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/30 17:08:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/30 17:08:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/30 17:08:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/30 17:08:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/30 17:08:51 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105\n",
      "24/09/30 17:08:51 INFO DAGScheduler: Got job 59 (save at BigQueryWriteHelper.java:105) with 4 output partitions\n",
      "24/09/30 17:08:51 INFO DAGScheduler: Final stage: ResultStage 59 (save at BigQueryWriteHelper.java:105)\n",
      "24/09/30 17:08:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:08:51 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:08:51 INFO DAGScheduler: Submitting ResultStage 59 (MapPartitionsRDD[191] at save at BigQueryWriteHelper.java:105), which has no missing parents\n",
      "24/09/30 17:08:51 INFO MemoryStore: Block broadcast_115 stored as values in memory (estimated size 295.3 KiB, free 253.3 MiB)\n",
      "24/09/30 17:08:51 INFO MemoryStore: Block broadcast_115_piece0 stored as bytes in memory (estimated size 99.6 KiB, free 253.2 MiB)\n",
      "24/09/30 17:08:51 INFO BlockManagerInfo: Added broadcast_115_piece0 in memory on 10.0.2.15:33255 (size: 99.6 KiB, free: 366.0 MiB)\n",
      "24/09/30 17:08:51 INFO SparkContext: Created broadcast 115 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:08:51 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 59 (MapPartitionsRDD[191] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "24/09/30 17:08:51 INFO TaskSchedulerImpl: Adding task set 59.0 with 4 tasks resource profile 0\n",
      "24/09/30 17:08:51 INFO TaskSetManager: Starting task 0.0 in stage 59.0 (TID 65) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:08:51 INFO TaskSetManager: Starting task 1.0 in stage 59.0 (TID 66) (10.0.2.15, executor driver, partition 1, PROCESS_LOCAL, 9270 bytes) \n",
      "24/09/30 17:08:51 INFO TaskSetManager: Starting task 2.0 in stage 59.0 (TID 67) (10.0.2.15, executor driver, partition 2, PROCESS_LOCAL, 9270 bytes) \n",
      "24/09/30 17:08:51 INFO TaskSetManager: Starting task 3.0 in stage 59.0 (TID 68) (10.0.2.15, executor driver, partition 3, PROCESS_LOCAL, 9270 bytes) \n",
      "24/09/30 17:08:51 INFO Executor: Running task 1.0 in stage 59.0 (TID 66)\n",
      "24/09/30 17:08:51 INFO Executor: Running task 0.0 in stage 59.0 (TID 65)\n",
      "24/09/30 17:08:51 INFO Executor: Running task 3.0 in stage 59.0 (TID 68)\n",
      "24/09/30 17:08:51 INFO Executor: Running task 2.0 in stage 59.0 (TID 67)\n",
      "24/09/30 17:08:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/30 17:08:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/30 17:08:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/30 17:08:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/30 17:08:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/30 17:08:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/30 17:08:51 INFO CodecConfig: Compression: SNAPPY\n",
      "24/09/30 17:08:51 INFO CodecConfig: Compression: SNAPPY\n",
      "24/09/30 17:08:51 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/09/30 17:08:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"estado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ano\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"mes\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"semana\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ano_nascimento\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"faixa_etaria\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"sexo\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"cor_raca\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tipo_area\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"escolaridade\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_sintomas_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"foi_posto_saude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ficou_em_casa\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ficou_internado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"foi_ao_posto_ou_internado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"resultado_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tem_plano_saude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"faixa_rendimento\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"situacao_domicilio\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary estado (STRING);\n",
      "  optional int32 ano;\n",
      "  optional int32 mes;\n",
      "  optional int32 semana;\n",
      "  optional int32 ano_nascimento;\n",
      "  required binary faixa_etaria (STRING);\n",
      "  optional binary sexo (STRING);\n",
      "  optional binary cor_raca (STRING);\n",
      "  optional binary tipo_area (STRING);\n",
      "  optional binary escolaridade (STRING);\n",
      "  optional binary teve_sintomas_covid (STRING);\n",
      "  optional binary foi_posto_saude (STRING);\n",
      "  optional binary ficou_em_casa (STRING);\n",
      "  optional binary ficou_internado (STRING);\n",
      "  optional binary foi_ao_posto_ou_internado (STRING);\n",
      "  optional binary teve_covid (STRING);\n",
      "  optional binary resultado_covid (STRING);\n",
      "  optional binary tem_plano_saude (STRING);\n",
      "  optional binary faixa_rendimento (STRING);\n",
      "  optional binary situacao_domicilio (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/09/30 17:08:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/30 17:08:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/30 17:08:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/30 17:08:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/30 17:08:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/30 17:08:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/30 17:08:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/30 17:08:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/30 17:08:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/30 17:08:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/30 17:08:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/30 17:08:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/30 17:08:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/30 17:08:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/30 17:08:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/30 17:08:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/30 17:08:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/30 17:08:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/30 17:08:51 INFO CodecConfig: Compression: SNAPPY\n",
      "24/09/30 17:08:51 INFO CodecConfig: Compression: SNAPPY\n",
      "24/09/30 17:08:51 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/09/30 17:08:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"estado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ano\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"mes\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"semana\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ano_nascimento\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"faixa_etaria\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"sexo\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"cor_raca\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tipo_area\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"escolaridade\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_sintomas_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"foi_posto_saude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ficou_em_casa\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ficou_internado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"foi_ao_posto_ou_internado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"resultado_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tem_plano_saude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"faixa_rendimento\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"situacao_domicilio\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary estado (STRING);\n",
      "  optional int32 ano;\n",
      "  optional int32 mes;\n",
      "  optional int32 semana;\n",
      "  optional int32 ano_nascimento;\n",
      "  required binary faixa_etaria (STRING);\n",
      "  optional binary sexo (STRING);\n",
      "  optional binary cor_raca (STRING);\n",
      "  optional binary tipo_area (STRING);\n",
      "  optional binary escolaridade (STRING);\n",
      "  optional binary teve_sintomas_covid (STRING);\n",
      "  optional binary foi_posto_saude (STRING);\n",
      "  optional binary ficou_em_casa (STRING);\n",
      "  optional binary ficou_internado (STRING);\n",
      "  optional binary foi_ao_posto_ou_internado (STRING);\n",
      "  optional binary teve_covid (STRING);\n",
      "  optional binary resultado_covid (STRING);\n",
      "  optional binary tem_plano_saude (STRING);\n",
      "  optional binary faixa_rendimento (STRING);\n",
      "  optional binary situacao_domicilio (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/09/30 17:08:51 INFO CodecConfig: Compression: SNAPPY\n",
      "24/09/30 17:08:51 INFO CodecConfig: Compression: SNAPPY\n",
      "24/09/30 17:08:51 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/09/30 17:08:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"estado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ano\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"mes\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"semana\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ano_nascimento\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"faixa_etaria\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"sexo\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"cor_raca\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tipo_area\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"escolaridade\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_sintomas_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"foi_posto_saude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ficou_em_casa\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ficou_internado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"foi_ao_posto_ou_internado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"resultado_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tem_plano_saude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"faixa_rendimento\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"situacao_domicilio\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary estado (STRING);\n",
      "  optional int32 ano;\n",
      "  optional int32 mes;\n",
      "  optional int32 semana;\n",
      "  optional int32 ano_nascimento;\n",
      "  required binary faixa_etaria (STRING);\n",
      "  optional binary sexo (STRING);\n",
      "  optional binary cor_raca (STRING);\n",
      "  optional binary tipo_area (STRING);\n",
      "  optional binary escolaridade (STRING);\n",
      "  optional binary teve_sintomas_covid (STRING);\n",
      "  optional binary foi_posto_saude (STRING);\n",
      "  optional binary ficou_em_casa (STRING);\n",
      "  optional binary ficou_internado (STRING);\n",
      "  optional binary foi_ao_posto_ou_internado (STRING);\n",
      "  optional binary teve_covid (STRING);\n",
      "  optional binary resultado_covid (STRING);\n",
      "  optional binary tem_plano_saude (STRING);\n",
      "  optional binary faixa_rendimento (STRING);\n",
      "  optional binary situacao_domicilio (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/09/30 17:08:51 INFO CodecConfig: Compression: SNAPPY\n",
      "24/09/30 17:08:51 INFO CodecConfig: Compression: SNAPPY\n",
      "24/09/30 17:08:51 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/09/30 17:08:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"estado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ano\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"mes\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"semana\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ano_nascimento\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"faixa_etaria\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"sexo\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"cor_raca\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tipo_area\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"escolaridade\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_sintomas_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"foi_posto_saude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ficou_em_casa\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ficou_internado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"foi_ao_posto_ou_internado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"resultado_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tem_plano_saude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"faixa_rendimento\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"situacao_domicilio\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary estado (STRING);\n",
      "  optional int32 ano;\n",
      "  optional int32 mes;\n",
      "  optional int32 semana;\n",
      "  optional int32 ano_nascimento;\n",
      "  required binary faixa_etaria (STRING);\n",
      "  optional binary sexo (STRING);\n",
      "  optional binary cor_raca (STRING);\n",
      "  optional binary tipo_area (STRING);\n",
      "  optional binary escolaridade (STRING);\n",
      "  optional binary teve_sintomas_covid (STRING);\n",
      "  optional binary foi_posto_saude (STRING);\n",
      "  optional binary ficou_em_casa (STRING);\n",
      "  optional binary ficou_internado (STRING);\n",
      "  optional binary foi_ao_posto_ou_internado (STRING);\n",
      "  optional binary teve_covid (STRING);\n",
      "  optional binary resultado_covid (STRING);\n",
      "  optional binary tem_plano_saude (STRING);\n",
      "  optional binary faixa_rendimento (STRING);\n",
      "  optional binary situacao_domicilio (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_94_piece0 on 10.0.2.15:33255 in memory (size: 10.2 KiB, free: 366.0 MiB)\n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_75_piece0 on 10.0.2.15:33255 in memory (size: 446.0 B, free: 366.0 MiB)\n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_85_piece0 on 10.0.2.15:33255 in memory (size: 492.0 B, free: 366.0 MiB)\n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_95_piece0 on 10.0.2.15:33255 in memory (size: 10.2 KiB, free: 366.0 MiB)\n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_72_piece0 on 10.0.2.15:33255 in memory (size: 295.0 B, free: 366.0 MiB)\n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_77_piece0 on 10.0.2.15:33255 in memory (size: 348.0 B, free: 366.0 MiB)\n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_76_piece0 on 10.0.2.15:33255 in memory (size: 467.0 B, free: 366.0 MiB)\n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_84_piece0 on 10.0.2.15:33255 in memory (size: 244.0 B, free: 366.0 MiB)\n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_78_piece0 on 10.0.2.15:33255 in memory (size: 348.0 B, free: 366.0 MiB)\n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_82_piece0 on 10.0.2.15:33255 in memory (size: 348.0 B, free: 366.0 MiB)\n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_93_piece0 on 10.0.2.15:33255 in memory (size: 10.2 KiB, free: 366.0 MiB)\n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_73_piece0 on 10.0.2.15:33255 in memory (size: 244.0 B, free: 366.0 MiB)\n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_96_piece0 on 10.0.2.15:33255 in memory (size: 10.2 KiB, free: 366.0 MiB)\n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_99_piece0 on 10.0.2.15:33255 in memory (size: 10.2 KiB, free: 366.0 MiB)\n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_89_piece0 on 10.0.2.15:33255 in memory (size: 10.2 KiB, free: 366.0 MiB)\n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_79_piece0 on 10.0.2.15:33255 in memory (size: 1038.0 B, free: 366.0 MiB)\n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_74_piece0 on 10.0.2.15:33255 in memory (size: 244.0 B, free: 366.0 MiB)\n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_83_piece0 on 10.0.2.15:33255 in memory (size: 482.0 B, free: 366.1 MiB)\n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_91_piece0 on 10.0.2.15:33255 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_97_piece0 on 10.0.2.15:33255 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_80_piece0 on 10.0.2.15:33255 in memory (size: 359.0 B, free: 366.1 MiB)\n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_92_piece0 on 10.0.2.15:33255 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_98_piece0 on 10.0.2.15:33255 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_81_piece0 on 10.0.2.15:33255 in memory (size: 215.0 B, free: 366.1 MiB)\n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_86_piece0 on 10.0.2.15:33255 in memory (size: 99.6 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:08:53 INFO BlockManagerInfo: Removed broadcast_100_piece0 on 10.0.2.15:33255 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:09:24 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJWb0J4c3BzUVNfTxoCdngaAnVo/streams/GgJ2eBoCdWgoAg,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJWb0J4c3BzUVNfTxoCdngaAnVo/streams/CAEaAnZ4GgJ1aCgC,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJWb0J4c3BzUVNfTxoCdngaAnVo/streams/CAIaAnZ4GgJ1aCgC,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJWb0J4c3BzUVNfTxoCdngaAnVo/streams/CAMaAnZ4GgJ1aCgC\",\"Started\":\"2024-09-30T20:08:51.302Z\",\"Ended\":\"2024-09-30T20:09:24.343Z\",\"Parse Timings\":\"Average: PT0.000132867S Samples: 691\",\"Time in Spark\":\"Average: PT0.04665908S\",\"Time waiting for service\":\"Average: PT0.03264583S Samples: 691\",\"Bytes/s\":4618862,\"Rows/s\":7281472,\"Bytes\":104192296,\"Rows\":662614,\"I/O time\":22558}\n",
      "24/09/30 17:09:27 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJWb0J4c3BzUVNfTxoCdngaAnVo/streams/GgJ2eBoCdWgoAg,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJWb0J4c3BzUVNfTxoCdngaAnVo/streams/CAEaAnZ4GgJ1aCgC,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJWb0J4c3BzUVNfTxoCdngaAnVo/streams/CAIaAnZ4GgJ1aCgC,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJWb0J4c3BzUVNfTxoCdngaAnVo/streams/CAMaAnZ4GgJ1aCgC\",\"Started\":\"2024-09-30T20:08:51.311Z\",\"Ended\":\"2024-09-30T20:09:27.112Z\",\"Parse Timings\":\"Average: PT0.000141613S Samples: 691\",\"Time in Spark\":\"Average: PT0.050848959S\",\"Time waiting for service\":\"Average: PT0.035024054S Samples: 691\",\"Bytes/s\":4347506,\"Rows/s\":6831082,\"Bytes\":105214000,\"Rows\":662615,\"I/O time\":24201}\n",
      "24/09/30 17:09:28 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJWb0J4c3BzUVNfTxoCdngaAnVo/streams/GgJ2eBoCdWgoAg,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJWb0J4c3BzUVNfTxoCdngaAnVo/streams/CAEaAnZ4GgJ1aCgC,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJWb0J4c3BzUVNfTxoCdngaAnVo/streams/CAIaAnZ4GgJ1aCgC,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJWb0J4c3BzUVNfTxoCdngaAnVo/streams/CAMaAnZ4GgJ1aCgC\",\"Started\":\"2024-09-30T20:08:51.308Z\",\"Ended\":\"2024-09-30T20:09:28.540Z\",\"Parse Timings\":\"Average: PT0.000183978S Samples: 691\",\"Time in Spark\":\"Average: PT0.053048895S\",\"Time waiting for service\":\"Average: PT0.034903094S Samples: 691\",\"Bytes/s\":4415817,\"Rows/s\":5217440,\"Bytes\":106500688,\"Rows\":662615,\"I/O time\":24118}\n",
      "24/09/30 17:09:29 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727724822043-d41e3f64-4c02-48be-bf7b-fc1e43762fbf/_temporary/0/_temporary/' directory.\n",
      "24/09/30 17:09:29 INFO GhfsStorageStatistics: Detected potential high latency for operation op_rename. latencyMs=1889; previousMaxLatencyMs=1859; operationCount=17; context=rename(gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727724822043-d41e3f64-4c02-48be-bf7b-fc1e43762fbf/_temporary/0/_temporary/attempt_202409301708512263391987324257286_0059_m_000000_65 -> gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727724822043-d41e3f64-4c02-48be-bf7b-fc1e43762fbf/_temporary/0/task_202409301708512263391987324257286_0059_m_000000)\n",
      "24/09/30 17:09:29 INFO FileOutputCommitter: Saved output of task 'attempt_202409301708512263391987324257286_0059_m_000000_65' to gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727724822043-d41e3f64-4c02-48be-bf7b-fc1e43762fbf/_temporary/0/task_202409301708512263391987324257286_0059_m_000000\n",
      "24/09/30 17:09:29 INFO SparkHadoopMapRedUtil: attempt_202409301708512263391987324257286_0059_m_000000_65: Committed. Elapsed time: 2633 ms.\n",
      "24/09/30 17:09:29 INFO Executor: Finished task 0.0 in stage 59.0 (TID 65). 3311 bytes result sent to driver\n",
      "24/09/30 17:09:29 INFO TaskSetManager: Finished task 0.0 in stage 59.0 (TID 65) in 38149 ms on 10.0.2.15 (executor driver) (1/4)\n",
      "24/09/30 17:09:30 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJWb0J4c3BzUVNfTxoCdngaAnVo/streams/GgJ2eBoCdWgoAg,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJWb0J4c3BzUVNfTxoCdngaAnVo/streams/CAEaAnZ4GgJ1aCgC,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJWb0J4c3BzUVNfTxoCdngaAnVo/streams/CAIaAnZ4GgJ1aCgC,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJWb0J4c3BzUVNfTxoCdngaAnVo/streams/CAMaAnZ4GgJ1aCgC\",\"Started\":\"2024-09-30T20:08:51.302Z\",\"Ended\":\"2024-09-30T20:09:30.235Z\",\"Parse Timings\":\"Average: PT0.000115315S Samples: 691\",\"Time in Spark\":\"Average: PT0.055537384S\",\"Time waiting for service\":\"Average: PT0.041820206S Samples: 691\",\"Bytes/s\":3663937,\"Rows/s\":8387531,\"Bytes\":105876800,\"Rows\":662615,\"I/O time\":28897}\n",
      "24/09/30 17:09:31 INFO FileOutputCommitter: Saved output of task 'attempt_202409301708512263391987324257286_0059_m_000001_66' to gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727724822043-d41e3f64-4c02-48be-bf7b-fc1e43762fbf/_temporary/0/task_202409301708512263391987324257286_0059_m_000001\n",
      "24/09/30 17:09:31 INFO SparkHadoopMapRedUtil: attempt_202409301708512263391987324257286_0059_m_000001_66: Committed. Elapsed time: 2237 ms.\n",
      "24/09/30 17:09:31 INFO Executor: Finished task 1.0 in stage 59.0 (TID 66). 3311 bytes result sent to driver\n",
      "24/09/30 17:09:31 INFO TaskSetManager: Finished task 1.0 in stage 59.0 (TID 66) in 40394 ms on 10.0.2.15 (executor driver) (2/4)\n",
      "24/09/30 17:09:33 INFO FileOutputCommitter: Saved output of task 'attempt_202409301708512263391987324257286_0059_m_000003_68' to gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727724822043-d41e3f64-4c02-48be-bf7b-fc1e43762fbf/_temporary/0/task_202409301708512263391987324257286_0059_m_000003\n",
      "24/09/30 17:09:33 INFO SparkHadoopMapRedUtil: attempt_202409301708512263391987324257286_0059_m_000003_68: Committed. Elapsed time: 2555 ms.\n",
      "24/09/30 17:09:33 INFO Executor: Finished task 3.0 in stage 59.0 (TID 68). 3311 bytes result sent to driver\n",
      "24/09/30 17:09:33 INFO TaskSetManager: Finished task 3.0 in stage 59.0 (TID 68) in 41981 ms on 10.0.2.15 (executor driver) (3/4)\n",
      "24/09/30 17:09:34 INFO FileOutputCommitter: Saved output of task 'attempt_202409301708512263391987324257286_0059_m_000002_67' to gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727724822043-d41e3f64-4c02-48be-bf7b-fc1e43762fbf/_temporary/0/task_202409301708512263391987324257286_0059_m_000002\n",
      "24/09/30 17:09:34 INFO SparkHadoopMapRedUtil: attempt_202409301708512263391987324257286_0059_m_000002_67: Committed. Elapsed time: 2214 ms.\n",
      "24/09/30 17:09:34 INFO Executor: Finished task 2.0 in stage 59.0 (TID 67). 3311 bytes result sent to driver\n",
      "24/09/30 17:09:34 INFO TaskSetManager: Finished task 2.0 in stage 59.0 (TID 67) in 43450 ms on 10.0.2.15 (executor driver) (4/4)\n",
      "24/09/30 17:09:34 INFO TaskSchedulerImpl: Removed TaskSet 59.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:09:34 INFO DAGScheduler: ResultStage 59 (save at BigQueryWriteHelper.java:105) finished in 43,494 s\n",
      "24/09/30 17:09:34 INFO DAGScheduler: Job 59 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:09:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 59: Stage finished\n",
      "24/09/30 17:09:34 INFO DAGScheduler: Job 59 finished: save at BigQueryWriteHelper.java:105, took 43,499132 s\n",
      "24/09/30 17:09:34 INFO FileFormatWriter: Start to commit write Job 20f813dc-6db6-491e-9f77-0c4bc0c3e066.\n",
      "24/09/30 17:09:37 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727724822043-d41e3f64-4c02-48be-bf7b-fc1e43762fbf/_temporary/0/task_202409301708512263391987324257286_0059_m_000000/' directory.\n",
      "24/09/30 17:09:40 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727724822043-d41e3f64-4c02-48be-bf7b-fc1e43762fbf/_temporary/0/task_202409301708512263391987324257286_0059_m_000001/' directory.\n",
      "24/09/30 17:09:42 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727724822043-d41e3f64-4c02-48be-bf7b-fc1e43762fbf/_temporary/0/task_202409301708512263391987324257286_0059_m_000002/' directory.\n",
      "24/09/30 17:09:45 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727724822043-d41e3f64-4c02-48be-bf7b-fc1e43762fbf/_temporary/0/task_202409301708512263391987324257286_0059_m_000003/' directory.\n",
      "24/09/30 17:09:46 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727724822043-d41e3f64-4c02-48be-bf7b-fc1e43762fbf/' directory.\n",
      "24/09/30 17:09:48 INFO FileFormatWriter: Write Job 20f813dc-6db6-491e-9f77-0c4bc0c3e066 committed. Elapsed time: 13408 ms.\n",
      "24/09/30 17:09:48 INFO FileFormatWriter: Finished processing stats for write job 20f813dc-6db6-491e-9f77-0c4bc0c3e066.\n",
      "24/09/30 17:09:48 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=SPEC, tableId=tbx001_data}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=estado, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ano, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=mes, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=semana, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ano_nascimento, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=faixa_etaria, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=sexo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=cor_raca, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=tipo_area, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=escolaridade, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teve_sintomas_covid, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=foi_posto_saude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ficou_em_casa, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ficou_internado, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=foi_ao_posto_ou_internado, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teve_covid, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=resultado_covid, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=tem_plano_saude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=faixa_rendimento, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=situacao_domicilio, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727724822043-d41e3f64-4c02-48be-bf7b-fc1e43762fbf/part-00000-f37b5f03-629e-489a-9e3c-7bf9839a7159-c000.snappy.parquet, gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727724822043-d41e3f64-4c02-48be-bf7b-fc1e43762fbf/part-00001-f37b5f03-629e-489a-9e3c-7bf9839a7159-c000.snappy.parquet, gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727724822043-d41e3f64-4c02-48be-bf7b-fc1e43762fbf/part-00003-f37b5f03-629e-489a-9e3c-7bf9839a7159-c000.snappy.parquet, gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727724822043-d41e3f64-4c02-48be-bf7b-fc1e43762fbf/part-00002-f37b5f03-629e-489a-9e3c-7bf9839a7159-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=my-project-1508437523553, job=158f054c-65c9-4a95-9a9c-c39696b34b61, location=us-east1}\n",
      "24/09/30 17:10:01 INFO BigQueryClient: Done loading to SPEC.tbx001_data. jobId: JobId{project=my-project-1508437523553, job=158f054c-65c9-4a95-9a9c-c39696b34b61, location=us-east1}\n"
     ]
    }
   ],
   "source": [
    "save_to_bigquery(df_joined, \"SPEC\", \"tbx001_data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
