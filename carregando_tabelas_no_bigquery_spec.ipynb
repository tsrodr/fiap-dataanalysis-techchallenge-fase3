{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 17:37:46 WARN Utils: Your hostname, spark-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/09/30 17:37:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/spark/.ivy2/cache\n",
      "The jars for the packages stored in: /home/spark/.ivy2/jars\n",
      "com.google.cloud.spark#spark-bigquery-with-dependencies_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-aefdc8d1-e549-4a1a-8ae9-499a55bddbc4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.23.2 in central\n",
      ":: resolution report :: resolve 164ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.23.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-aefdc8d1-e549-4a1a-8ae9-499a55bddbc4\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/3ms)\n",
      "24/09/30 17:37:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/30 17:37:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Criação da sessão Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark BigQuery Connection\") \\\n",
    "    .config('spark.jars.packages', 'com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2') \\\n",
    "    .config(\"spark.jars\", \"/usr/local/lib/spark-connectors/bigquery-connector-hadoop2-latest.jar\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true -Dio.netty.noUnsafe=true\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true -Dio.netty.noUnsafe=true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"viewsEnabled\", True)\n",
    "spark.conf.set(\"materializationDataset\", \"SOR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n",
    "sc._jsc.hadoopConfiguration().set('fs.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem')\n",
    "sc._jsc.hadoopConfiguration().set('fs.gs.auth.service.account.json.keyfile', '/usr/local/lib/gcp/credentials/my-project-1508437523553-e9bafe7e3368.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para salvar DataFrame em formato Parquet\n",
    "def save_to_bigquery(df, dataset, table_name):\n",
    "    # Salva o DataFrame em formato Parquet\n",
    "    df.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"table\", f\"{dataset.upper()}.{table_name}\") \\\n",
    "    .option(\"temporaryGcsBucket\", \"meu-bucket-temporario-spark\") \\\n",
    "    .option(\"credentialsFile\", \"/usr/local/lib/gcp/credentials/my-project-1508437523553-e9bafe7e3368.json\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para ler dados do BigQuery\n",
    "def read_from_bigquery(dataset, table_name):  \n",
    "    df = spark.read \\\n",
    "        .format('bigquery') \\\n",
    "        .option('table', f\"{dataset.upper()}.{table_name}\") \\\n",
    "        .option(\"credentialsFile\", \"/usr/local/lib/gcp/credentials/my-project-1508437523553-e9bafe7e3368.json\") \\\n",
    "        .load()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dimensao = read_from_bigquery('SOR', 'tbx002_dimensao_geral')\n",
    "df_dimensao.createOrReplaceTempView(\"tbx002_dimensao_geral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_from_bigquery('SOR', 'tbx001_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário de mapeamento de renomeação de colunas\n",
    "col_rename_map = {\n",
    "    \"UF\": \"uf\",\n",
    "    \"Ano\": \"ano\",\n",
    "    \"V1013\": \"mes\",\n",
    "    \"V1012\": \"semana\",\n",
    "    \"A001B3\": \"ano_nascimento\",\n",
    "    \"A003\": \"sexo\",\n",
    "    \"A004\": \"cor_raca\",\n",
    "    \"V1023\": \"tipo_area\",\n",
    "    \"A005\": \"escolaridade\",\n",
    "    \"B002\": \"foi_posto_saude\",\n",
    "    \"B0031\": \"ficou_em_casa\",\n",
    "    \"B005\": \"ficou_internado\",\n",
    "    \"B009B\": \"resultado_covid\",\n",
    "    \"B009D\" : \"resultado_covid_2\",\n",
    "    \"B009F\": \"resultado_covid_3\",\n",
    "    \"B007\": \"tem_plano_saude\",\n",
    "    \"C01011\": \"faixa_rendimento\",\n",
    "    \"F001\": \"situacao_domicilio\",\n",
    "    \"B0011\": \"teve_febre\",\n",
    "    \"B0012\": \"teve_tosse\",\n",
    "    \"B0013\": \"teve_dor_garganta\",\n",
    "    \"B0014\": \"teve_dificuldade_respirar\",\n",
    "    \"B0015\": \"teve_dor_cabeca\",\n",
    "    \"B0016\": \"teve_dor_peito\",\n",
    "    \"B0017\": \"teve_nausea\",\n",
    "    \"B0018\": \"teve_nariz_entupido_escorrendo\",\n",
    "    \"B0019\": \"teve_fadiga\",\n",
    "    \"B00110\": \"teve_dor_olhos\",\n",
    "    \"B00111\": \"teve_perda_olfato_sabor\",\n",
    "    \"B00112\": \"teve_dor_muscular\",\n",
    "    \"B00113\": \"teve_diarreia\"\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Aplicar a renomeação das colunas e selecionar apenas as colunas renomeadas\n",
    "df = df.select([F.col(old_name).alias(new_name) for old_name, new_name in col_rename_map.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uf: string (nullable = true)\n",
      " |-- ano: string (nullable = true)\n",
      " |-- mes: string (nullable = true)\n",
      " |-- semana: string (nullable = true)\n",
      " |-- ano_nascimento: string (nullable = true)\n",
      " |-- sexo: string (nullable = true)\n",
      " |-- cor_raca: string (nullable = true)\n",
      " |-- tipo_area: string (nullable = true)\n",
      " |-- escolaridade: string (nullable = true)\n",
      " |-- foi_posto_saude: string (nullable = true)\n",
      " |-- ficou_em_casa: string (nullable = true)\n",
      " |-- ficou_internado: string (nullable = true)\n",
      " |-- resultado_covid: string (nullable = true)\n",
      " |-- resultado_covid_2: string (nullable = true)\n",
      " |-- resultado_covid_3: string (nullable = true)\n",
      " |-- tem_plano_saude: string (nullable = true)\n",
      " |-- faixa_rendimento: string (nullable = true)\n",
      " |-- situacao_domicilio: string (nullable = true)\n",
      " |-- teve_febre: string (nullable = true)\n",
      " |-- teve_tosse: string (nullable = true)\n",
      " |-- teve_dor_garganta: string (nullable = true)\n",
      " |-- teve_dificuldade_respirar: string (nullable = true)\n",
      " |-- teve_dor_cabeca: string (nullable = true)\n",
      " |-- teve_dor_peito: string (nullable = true)\n",
      " |-- teve_nausea: string (nullable = true)\n",
      " |-- teve_nariz_entupido_escorrendo: string (nullable = true)\n",
      " |-- teve_fadiga: string (nullable = true)\n",
      " |-- teve_dor_olhos: string (nullable = true)\n",
      " |-- teve_perda_olfato_sabor: string (nullable = true)\n",
      " |-- teve_dor_muscular: string (nullable = true)\n",
      " |-- teve_diarreia: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 17:37:59 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"tbx001_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# realizar o join entre os dataframes, dimensionando a tabela de fatos\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    TRIM(T2.categoria_descricao) AS estado,\n",
    "    CAST(TRIM(T1.ano) AS INT) AS ano,\n",
    "    CAST(TRIM(T1.mes) AS INT) AS mes,\n",
    "    CAST(TRIM(T1.semana) AS INT) AS semana,\n",
    "    CAST(TRIM(T1.ano_nascimento) AS INT) AS ano_nascimento,\n",
    "    -- Definição da faixa etária com base na idade\n",
    "    CASE\n",
    "        WHEN (CAST(TRIM(T1.ano) AS INT) - CAST(TRIM(T1.ano_nascimento) AS INT)) BETWEEN 0 AND 17 THEN '0-17'\n",
    "        WHEN (CAST(TRIM(T1.ano) AS INT) - CAST(TRIM(T1.ano_nascimento) AS INT)) BETWEEN 18 AND 29 THEN '18-29'\n",
    "        WHEN (CAST(TRIM(T1.ano) AS INT) - CAST(TRIM(T1.ano_nascimento) AS INT)) BETWEEN 30 AND 39 THEN '30-39'\n",
    "        WHEN (CAST(TRIM(T1.ano) AS INT) - CAST(TRIM(T1.ano_nascimento) AS INT)) BETWEEN 40 AND 49 THEN '40-49'\n",
    "        WHEN (CAST(TRIM(T1.ano) AS INT) - CAST(TRIM(T1.ano_nascimento) AS INT)) BETWEEN 50 AND 59 THEN '50-59'\n",
    "        WHEN (CAST(TRIM(T1.ano) AS INT) - CAST(TRIM(T1.ano_nascimento) AS INT)) BETWEEN 60 AND 69 THEN '60-69'\n",
    "        WHEN (CAST(TRIM(T1.ano) AS INT) - CAST(TRIM(T1.ano_nascimento) AS INT)) BETWEEN 70 AND 79 THEN '70-79'\n",
    "        WHEN (CAST(TRIM(T1.ano) AS INT) - CAST(TRIM(T1.ano_nascimento) AS INT)) BETWEEN 80 AND 89 THEN '80-89'\n",
    "        WHEN (CAST(TRIM(T1.ano) AS INT) - CAST(TRIM(T1.ano_nascimento) AS INT)) BETWEEN 90 AND 99 THEN '90-99'\n",
    "        ELSE '100+'\n",
    "    END AS faixa_etaria,\n",
    "    TRIM(T3.categoria_descricao) AS sexo,\n",
    "    TRIM(T4.categoria_descricao) AS cor_raca,\n",
    "    TRIM(T5.categoria_descricao) AS tipo_area,\n",
    "    TRIM(T6.categoria_descricao) AS escolaridade,\n",
    "    CASE\n",
    "        WHEN T1.teve_febre = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_tosse = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_dor_garganta = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_dificuldade_respirar = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_dor_cabeca = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_dor_peito = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_nausea = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_nariz_entupido_escorrendo = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_fadiga = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_dor_olhos = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_perda_olfato_sabor = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_dor_muscular = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_diarreia = 1 THEN 'Sim'\n",
    "        WHEN T1.teve_febre IS NULL\n",
    "             AND T1.teve_tosse IS NULL\n",
    "             AND T1.teve_dor_garganta IS NULL\n",
    "             AND T1.teve_dificuldade_respirar IS NULL\n",
    "             AND T1.teve_dor_cabeca IS NULL\n",
    "             AND T1.teve_dor_peito IS NULL\n",
    "             AND T1.teve_nausea IS NULL\n",
    "             AND T1.teve_nariz_entupido_escorrendo IS NULL\n",
    "             AND T1.teve_fadiga IS NULL\n",
    "             AND T1.teve_dor_olhos IS NULL\n",
    "             AND T1.teve_perda_olfato_sabor IS NULL\n",
    "             AND T1.teve_dor_muscular IS NULL\n",
    "             AND T1.teve_diarreia IS NULL \n",
    "        THEN NULL\n",
    "        ELSE 'Não'\n",
    "    END AS teve_sintomas_covid,\n",
    "    TRIM(T7.categoria_descricao) AS foi_posto_saude,\n",
    "    TRIM(T8.categoria_descricao) AS ficou_em_casa,\n",
    "    TRIM(T9.categoria_descricao) AS ficou_internado,    \n",
    "    CASE \n",
    "        when TRIM(T9.categoria_descricao) = 'Sim' or TRIM(T7.categoria_descricao) = 'Sim' then 'Sim'\n",
    "        when TRIM(T7.categoria_descricao) is null and TRIM(T9.categoria_descricao) is null then null\n",
    "        when TRIM(T9.categoria_descricao) is not null then TRIM(T9.categoria_descricao)\n",
    "        when TRIM(T7.categoria_descricao) is not null then TRIM(T7.categoria_descricao)\n",
    "        end as foi_ao_posto_ou_internado,   \n",
    "    CASE\n",
    "        WHEN T1.resultado_covid = 1 THEN 'Sim'\n",
    "        WHEN T1.resultado_covid_2 = 1 THEN 'Sim'\n",
    "        WHEN T1.resultado_covid_3 = 1 THEN 'Sim'\n",
    "        WHEN T1.resultado_covid IS NULL \n",
    "             AND T1.resultado_covid_2 IS NULL \n",
    "             AND T1.resultado_covid_3 IS NULL THEN NULL      \n",
    "        ELSE 'Não'\n",
    "    END AS teve_covid,\n",
    "    CASE\n",
    "        WHEN T10.categoria_descricao IS NOT NULL THEN TRIM(T10.categoria_descricao)\n",
    "        WHEN T11.categoria_descricao IS NOT NULL THEN TRIM(T11.categoria_descricao)\n",
    "        WHEN T12.categoria_descricao IS NOT NULL THEN TRIM(T12.categoria_descricao)\n",
    "    END AS resultado_covid,\n",
    "    TRIM(T13.categoria_descricao) AS tem_plano_saude,\n",
    "    TRIM(T14.categoria_descricao) AS faixa_rendimento,\n",
    "    TRIM(T15.categoria_descricao) AS situacao_domicilio\n",
    "FROM tbx001_data T1\n",
    "LEFT JOIN tbx002_dimensao_geral T2 ON T2.codigo_variavel = 'UF' AND TRIM(T1.uf) = TRIM(T2.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T3 ON T3.codigo_variavel = 'A003' AND TRIM(T1.sexo) = TRIM(T3.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T4 ON T4.codigo_variavel = 'A004' AND TRIM(T1.cor_raca) = TRIM(T4.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T5 ON T5.codigo_variavel = 'V1023' AND TRIM(T1.tipo_area) = TRIM(T5.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T6 ON T6.codigo_variavel = 'A005' AND TRIM(T1.escolaridade) = TRIM(T6.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T7 ON T7.codigo_variavel = 'B002' AND TRIM(T1.foi_posto_saude) = TRIM(T7.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T8 ON T8.codigo_variavel = 'B0031' AND TRIM(T1.ficou_em_casa) = TRIM(T8.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T9 ON T9.codigo_variavel = 'B005' AND TRIM(T1.ficou_internado) = TRIM(T9.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T10 ON T10.codigo_variavel = 'B009B' AND TRIM(T1.resultado_covid) = TRIM(T10.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T11 ON T11.codigo_variavel = 'B009D' AND TRIM(T1.resultado_covid_2) = TRIM(T11.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T12 ON T12.codigo_variavel = 'B009F' AND TRIM(T1.resultado_covid_3) = TRIM(T12.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T13 ON T13.codigo_variavel = 'B007' AND TRIM(T1.tem_plano_saude) = TRIM(T13.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T14 ON T14.codigo_variavel = 'C01011' AND TRIM(T1.faixa_rendimento) = TRIM(T14.categoria_tipo)\n",
    "LEFT JOIN tbx002_dimensao_geral T15 ON T15.codigo_variavel = 'F001' AND TRIM(T1.situacao_domicilio) = TRIM(T15.categoria_tipo)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar a consulta SQL\n",
    "df_joined = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(df_joined.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- estado: string (nullable = true)\n",
      " |-- ano: integer (nullable = true)\n",
      " |-- mes: integer (nullable = true)\n",
      " |-- semana: integer (nullable = true)\n",
      " |-- ano_nascimento: integer (nullable = true)\n",
      " |-- faixa_etaria: string (nullable = false)\n",
      " |-- sexo: string (nullable = true)\n",
      " |-- cor_raca: string (nullable = true)\n",
      " |-- tipo_area: string (nullable = true)\n",
      " |-- escolaridade: string (nullable = true)\n",
      " |-- teve_sintomas_covid: string (nullable = true)\n",
      " |-- foi_posto_saude: string (nullable = true)\n",
      " |-- ficou_em_casa: string (nullable = true)\n",
      " |-- ficou_internado: string (nullable = true)\n",
      " |-- foi_ao_posto_ou_internado: string (nullable = true)\n",
      " |-- teve_covid: string (nullable = true)\n",
      " |-- resultado_covid: string (nullable = true)\n",
      " |-- tem_plano_saude: string (nullable = true)\n",
      " |-- faixa_rendimento: string (nullable = true)\n",
      " |-- situacao_domicilio: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 17:38:01 INFO GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=662; previousMaxLatencyMs=0; operationCount=1; context=gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2\n",
      "24/09/30 17:38:01 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx001_data, parameters sent from Spark:|requiredColumns=[UF,Ano,V1013,V1012,A001B3,A003,A004,V1023,A005,B002,B0031,B005,B009B,B009D,B009F,B007,C01011,F001,B0011,B0012,B0013,B0014,B0015,B0016,B0017,B0018,B0019,B00110,B00111,B00112,B00113],|filters=[]\n",
      "24/09/30 17:38:03 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEVFcnRXVnpIVjMyYxoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:38:01.334Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:38:03.640Z\",\"readSessionPrepDuration\":1206,\"readSessionCreationDuration\":1100,\"readSessionDuration\":2306}\n",
      "24/09/30 17:38:03 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 4 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEVFcnRXVnpIVjMyYxoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:38:03 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx001_data': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEVFcnRXVnpIVjMyYxoCdngaAnVo\n",
      "24/09/30 17:38:03 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,UF)]\n",
      "24/09/30 17:38:04 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDgzT09fYjE2bURnMBoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:38:03.727Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:38:04.445Z\",\"readSessionPrepDuration\":226,\"readSessionCreationDuration\":492,\"readSessionDuration\":718}\n",
      "24/09/30 17:38:04 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDgzT09fYjE2bURnMBoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:38:04 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDgzT09fYjE2bURnMBoCdngaAnVo\n",
      "24/09/30 17:38:04 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,A003)]\n",
      "24/09/30 17:38:05 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlLZGZDUENTVXdhShoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:38:04.452Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:38:05.062Z\",\"readSessionPrepDuration\":231,\"readSessionCreationDuration\":379,\"readSessionDuration\":610}\n",
      "24/09/30 17:38:05 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlLZGZDUENTVXdhShoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:38:05 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlLZGZDUENTVXdhShoCdngaAnVo\n",
      "24/09/30 17:38:05 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,A004)]\n",
      "24/09/30 17:38:05 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDBRVS12T3Brc1F3eRoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:38:05.066Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:38:05.580Z\",\"readSessionPrepDuration\":232,\"readSessionCreationDuration\":282,\"readSessionDuration\":514}\n",
      "24/09/30 17:38:05 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDBRVS12T3Brc1F3eRoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:38:05 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDBRVS12T3Brc1F3eRoCdngaAnVo\n",
      "24/09/30 17:38:05 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,V1023)]\n",
      "24/09/30 17:38:06 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEZIbG5USE9haHVmTRoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:38:05.583Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:38:06.105Z\",\"readSessionPrepDuration\":230,\"readSessionCreationDuration\":292,\"readSessionDuration\":522}\n",
      "24/09/30 17:38:06 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEZIbG5USE9haHVmTRoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:38:06 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEZIbG5USE9haHVmTRoCdngaAnVo\n",
      "24/09/30 17:38:06 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,A005)]\n",
      "24/09/30 17:38:06 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFR2MlhESklmT3RmTBoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:38:06.108Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:38:06.651Z\",\"readSessionPrepDuration\":222,\"readSessionCreationDuration\":321,\"readSessionDuration\":543}\n",
      "24/09/30 17:38:06 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFR2MlhESklmT3RmTBoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:38:06 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFR2MlhESklmT3RmTBoCdngaAnVo\n",
      "24/09/30 17:38:06 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B002)]\n",
      "24/09/30 17:38:07 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDG9tVl9SM215YjdxchoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:38:06.654Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:38:07.141Z\",\"readSessionPrepDuration\":188,\"readSessionCreationDuration\":299,\"readSessionDuration\":487}\n",
      "24/09/30 17:38:07 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDG9tVl9SM215YjdxchoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:38:07 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDG9tVl9SM215YjdxchoCdngaAnVo\n",
      "24/09/30 17:38:07 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B0031)]\n",
      "24/09/30 17:38:07 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGRQUldCdjUyOUtPbhoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:38:07.144Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:38:07.766Z\",\"readSessionPrepDuration\":228,\"readSessionCreationDuration\":394,\"readSessionDuration\":622}\n",
      "24/09/30 17:38:07 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGRQUldCdjUyOUtPbhoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:38:07 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGRQUldCdjUyOUtPbhoCdngaAnVo\n",
      "24/09/30 17:38:07 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B005)]\n",
      "24/09/30 17:38:08 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHdaalZSUUZ3Q25faBoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:38:07.768Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:38:08.244Z\",\"readSessionPrepDuration\":196,\"readSessionCreationDuration\":280,\"readSessionDuration\":476}\n",
      "24/09/30 17:38:08 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHdaalZSUUZ3Q25faBoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:38:08 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHdaalZSUUZ3Q25faBoCdngaAnVo\n",
      "24/09/30 17:38:08 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B009B)]\n",
      "24/09/30 17:38:08 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDJLNl9QaFppYldLdRoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:38:08.248Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:38:08.748Z\",\"readSessionPrepDuration\":224,\"readSessionCreationDuration\":276,\"readSessionDuration\":500}\n",
      "24/09/30 17:38:08 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDJLNl9QaFppYldLdRoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:38:08 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDJLNl9QaFppYldLdRoCdngaAnVo\n",
      "24/09/30 17:38:08 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B009D)]\n",
      "24/09/30 17:38:09 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDENMUGdoYy10M1UwcxoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:38:08.751Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:38:09.235Z\",\"readSessionPrepDuration\":192,\"readSessionCreationDuration\":292,\"readSessionDuration\":484}\n",
      "24/09/30 17:38:09 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDENMUGdoYy10M1UwcxoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:38:09 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDENMUGdoYy10M1UwcxoCdngaAnVo\n",
      "24/09/30 17:38:09 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B009F)]\n",
      "24/09/30 17:38:09 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEg3ZElkUzY5TVA2WBoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:38:09.239Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:38:09.749Z\",\"readSessionPrepDuration\":197,\"readSessionCreationDuration\":313,\"readSessionDuration\":510}\n",
      "24/09/30 17:38:09 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEg3ZElkUzY5TVA2WBoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:38:09 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEg3ZElkUzY5TVA2WBoCdngaAnVo\n",
      "24/09/30 17:38:09 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B007)]\n",
      "24/09/30 17:38:10 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFFpN3g5eXl6Sm9nNBoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:38:09.751Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:38:10.271Z\",\"readSessionPrepDuration\":195,\"readSessionCreationDuration\":325,\"readSessionDuration\":520}\n",
      "24/09/30 17:38:10 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFFpN3g5eXl6Sm9nNBoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:38:10 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFFpN3g5eXl6Sm9nNBoCdngaAnVo\n",
      "24/09/30 17:38:10 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,C01011)]\n",
      "24/09/30 17:38:10 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDE51SmFOLW5PV3dIaBoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:38:10.274Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:38:10.789Z\",\"readSessionPrepDuration\":215,\"readSessionCreationDuration\":300,\"readSessionDuration\":515}\n",
      "24/09/30 17:38:10 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDE51SmFOLW5PV3dIaBoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:38:10 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDE51SmFOLW5PV3dIaBoCdngaAnVo\n",
      "24/09/30 17:38:10 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,F001)]\n",
      "24/09/30 17:38:11 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHpvQVR0QVVkbXBhTRoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:38:10.791Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:38:11.297Z\",\"readSessionPrepDuration\":220,\"readSessionCreationDuration\":286,\"readSessionDuration\":506}\n",
      "24/09/30 17:38:11 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHpvQVR0QVVkbXBhTRoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:38:11 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHpvQVR0QVVkbXBhTRoCdngaAnVo\n",
      "24/09/30 17:38:12 INFO CodeGenerator: Code generated in 195.928099 ms\n",
      "24/09/30 17:38:12 INFO CodeGenerator: Code generated in 196.4124 ms\n",
      "24/09/30 17:38:12 INFO CodeGenerator: Code generated in 198.294098 ms\n",
      "24/09/30 17:38:12 INFO CodeGenerator: Code generated in 198.141974 ms\n",
      "24/09/30 17:38:12 INFO CodeGenerator: Code generated in 200.008576 ms\n",
      "24/09/30 17:38:12 INFO CodeGenerator: Code generated in 195.839384 ms\n",
      "24/09/30 17:38:12 INFO CodeGenerator: Code generated in 201.4322 ms\n",
      "24/09/30 17:38:12 INFO CodeGenerator: Code generated in 207.577815 ms\n",
      "24/09/30 17:38:12 INFO CodeGenerator: Code generated in 214.951035 ms\n",
      "24/09/30 17:38:12 INFO CodeGenerator: Code generated in 216.778079 ms\n",
      "24/09/30 17:38:12 INFO CodeGenerator: Code generated in 215.998579 ms\n",
      "24/09/30 17:38:12 INFO CodeGenerator: Code generated in 223.193114 ms\n",
      "24/09/30 17:38:12 INFO CodeGenerator: Code generated in 223.202483 ms\n",
      "24/09/30 17:38:12 INFO CodeGenerator: Code generated in 224.226955 ms\n",
      "24/09/30 17:38:12 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:12 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:12 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:12 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:12 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:12 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:12 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:12 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:12 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:12 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:12 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:12 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:12 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:12 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Got job 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Final stage: ResultStage 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[33] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 20.5 KiB, free 366.3 MiB)\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 366.3 MiB)\n",
      "24/09/30 17:38:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:34323 (size: 10.2 KiB, free: 366.3 MiB)\n",
      "24/09/30 17:38:12 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[33] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:38:12 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Got job 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Final stage: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[42] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 20.5 KiB, free 366.3 MiB)\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 366.2 MiB)\n",
      "24/09/30 17:38:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:34323 (size: 10.2 KiB, free: 366.3 MiB)\n",
      "24/09/30 17:38:12 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[42] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:38:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:38:12 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:38:12 INFO DAGScheduler: Got job 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Final stage: ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[22] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 20.5 KiB, free 366.2 MiB)\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 366.2 MiB)\n",
      "24/09/30 17:38:12 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.2.15:34323 (size: 10.2 KiB, free: 366.3 MiB)\n",
      "24/09/30 17:38:12 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[22] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:38:12 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Got job 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Final stage: ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[28] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 20.5 KiB, free 366.2 MiB)\n",
      "24/09/30 17:38:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:38:12 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 366.2 MiB)\n",
      "24/09/30 17:38:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.2.15:34323 (size: 10.2 KiB, free: 366.3 MiB)\n",
      "24/09/30 17:38:12 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[28] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:38:12 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[29] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 20.5 KiB, free 366.2 MiB)\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 366.2 MiB)\n",
      "24/09/30 17:38:12 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:38:12 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.2.15:34323 (size: 10.2 KiB, free: 366.3 MiB)\n",
      "24/09/30 17:38:12 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[29] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:38:12 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Got job 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Final stage: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[26] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:38:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "24/09/30 17:38:12 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 20.5 KiB, free 366.1 MiB)\n",
      "24/09/30 17:38:12 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "24/09/30 17:38:12 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 366.1 MiB)\n",
      "24/09/30 17:38:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.2.15:34323 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:38:12 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[26] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:38:12 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Got job 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Final stage: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[37] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 20.5 KiB, free 366.1 MiB)\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 366.1 MiB)\n",
      "24/09/30 17:38:12 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.2.15:34323 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:38:12 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[37] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:38:12 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Got job 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Final stage: ResultStage 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[38] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 20.5 KiB, free 366.1 MiB)\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 366.1 MiB)\n",
      "24/09/30 17:38:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.2.15:34323 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:38:12 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[38] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:38:12 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Got job 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Final stage: ResultStage 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[34] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 20.5 KiB, free 366.0 MiB)\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 366.0 MiB)\n",
      "24/09/30 17:38:12 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.2.15:34323 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:38:12 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[34] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:38:12 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Got job 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Final stage: ResultStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[23] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 20.5 KiB, free 366.0 MiB)\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 366.0 MiB)\n",
      "24/09/30 17:38:12 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.2.15:34323 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:38:12 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[23] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:38:12 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Got job 10 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Final stage: ResultStage 10 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[24] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 20.5 KiB, free 366.0 MiB)\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 366.0 MiB)\n",
      "24/09/30 17:38:12 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.2.15:34323 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:38:12 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[24] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:38:12 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Got job 11 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Final stage: ResultStage 11 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[18] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 20.5 KiB, free 366.0 MiB)\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 365.9 MiB)\n",
      "24/09/30 17:38:12 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.2.15:34323 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:38:12 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[18] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:38:12 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Got job 12 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Final stage: ResultStage 12 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[40] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 20.5 KiB, free 365.9 MiB)\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 365.9 MiB)\n",
      "24/09/30 17:38:12 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.2.15:34323 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:38:12 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[40] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:38:12 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Got job 13 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Final stage: ResultStage 13 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[30] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 20.5 KiB, free 365.9 MiB)\n",
      "24/09/30 17:38:12 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 365.9 MiB)\n",
      "24/09/30 17:38:12 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.2.15:34323 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:38:12 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:38:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[30] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/30 17:38:12 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
      "24/09/30 17:38:13 INFO CodeGenerator: Code generated in 21.345094 ms(0 + 1) / 1]\n",
      "24/09/30 17:38:13 INFO CodeGenerator: Code generated in 26.386991 ms\n",
      "24/09/30 17:38:13 INFO CodeGenerator: Code generated in 26.893338 ms\n",
      "24/09/30 17:38:13 INFO CodeGenerator: Code generated in 27.834228 ms\n",
      "24/09/30 17:38:13 INFO BaseAllocator: Debug mode disabled. 2:>    (0 + 1) / 1]\n",
      "24/09/30 17:38:13 INFO DefaultAllocationManagerOption: allocation manager type not specified, using netty as the default type\n",
      "24/09/30 17:38:13 INFO CheckAllocator: Using DefaultAllocationManager at memory/DefaultAllocationManagerFactory.class\n",
      "24/09/30 17:38:14 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEg3ZElkUzY5TVA2WBoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:38:12.994Z\",\"Ended\":\"2024-09-30T20:38:14.457Z\",\"Parse Timings\":\"Average: PT0.520299413S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.719662198S Samples: 1\",\"Bytes/s\":489,\"Rows/s\":9,\"Bytes\":352,\"Rows\":5,\"I/O time\":719}\n",
      "24/09/30 17:38:14 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFFpN3g5eXl6Sm9nNBoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:38:12.993Z\",\"Ended\":\"2024-09-30T20:38:14.458Z\",\"Parse Timings\":\"Average: PT0.519373071S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.702816175S Samples: 1\",\"Bytes/s\":398,\"Rows/s\":5,\"Bytes\":280,\"Rows\":3,\"I/O time\":702}\n",
      "24/09/30 17:38:14 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDG9tVl9SM215YjdxchoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:38:12.993Z\",\"Ended\":\"2024-09-30T20:38:14.457Z\",\"Parse Timings\":\"Average: PT0.520249832S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.694011012S Samples: 1\",\"Bytes/s\":403,\"Rows/s\":5,\"Bytes\":280,\"Rows\":3,\"I/O time\":694}\n",
      "24/09/30 17:38:14 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFR2MlhESklmT3RmTBoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:38:12.992Z\",\"Ended\":\"2024-09-30T20:38:14.457Z\",\"Parse Timings\":\"Average: PT0.52002278S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.672915294S Samples: 1\",\"Bytes/s\":714,\"Rows/s\":15,\"Bytes\":480,\"Rows\":8,\"I/O time\":672}\n",
      "24/09/30 17:38:14 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1682 bytes result sent to driver\n",
      "24/09/30 17:38:14 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1607 bytes result sent to driver\n",
      "24/09/30 17:38:14 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1791 bytes result sent to driver\n",
      "24/09/30 17:38:14 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1607 bytes result sent to driver\n",
      "24/09/30 17:38:14 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:38:14 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
      "24/09/30 17:38:14 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:38:14 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
      "24/09/30 17:38:14 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:38:14 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
      "24/09/30 17:38:14 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:38:14 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
      "24/09/30 17:38:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1975 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:38:14 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:38:14 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1950 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:38:14 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:38:14 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1951 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:38:14 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:38:14 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1942 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:38:14 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:38:14 INFO DAGScheduler: ResultStage 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,165 s\n",
      "24/09/30 17:38:14 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:38:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/09/30 17:38:14 INFO CodeGenerator: Code generated in 6.217913 ms\n",
      "24/09/30 17:38:14 INFO DAGScheduler: ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1,985 s\n",
      "24/09/30 17:38:14 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:38:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "24/09/30 17:38:14 INFO DAGScheduler: Job 2 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2,272110 s\n",
      "24/09/30 17:38:14 INFO DAGScheduler: Job 0 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2,278168 s\n",
      "24/09/30 17:38:14 INFO DAGScheduler: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,021 s\n",
      "24/09/30 17:38:14 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:38:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/09/30 17:38:14 INFO DAGScheduler: Job 1 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2,277870 s\n",
      "24/09/30 17:38:14 INFO DAGScheduler: ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1,987 s\n",
      "24/09/30 17:38:14 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:38:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "24/09/30 17:38:14 INFO DAGScheduler: Job 3 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2,277680 s\n",
      "24/09/30 17:38:14 INFO CodeGenerator: Code generated in 42.688569 ms\n",
      "24/09/30 17:38:14 INFO CodeGenerator: Code generated in 53.394413 ms\n",
      "24/09/30 17:38:14 INFO CodeGenerator: Code generated in 33.935655 ms\n",
      "24/09/30 17:38:14 INFO CodeGenerator: Code generated in 16.530838 ms\n",
      "24/09/30 17:38:14 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 4.0 MiB, free 361.9 MiB)\n",
      "24/09/30 17:38:14 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 4.0 MiB, free 357.9 MiB)\n",
      "24/09/30 17:38:14 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 4.0 MiB, free 349.9 MiB)\n",
      "24/09/30 17:38:14 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 4.0 MiB, free 349.9 MiB)\n",
      "24/09/30 17:38:14 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 348.0 B, free 349.9 MiB)\n",
      "24/09/30 17:38:14 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 492.0 B, free 349.9 MiB)\n",
      "24/09/30 17:38:14 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 244.0 B, free 349.9 MiB)\n",
      "24/09/30 17:38:14 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 244.0 B, free 349.9 MiB)\n",
      "24/09/30 17:38:14 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.2.15:34323 (size: 348.0 B, free: 366.2 MiB)\n",
      "24/09/30 17:38:14 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.2.15:34323 (size: 492.0 B, free: 366.2 MiB)\n",
      "24/09/30 17:38:14 INFO SparkContext: Created broadcast 14 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:14 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.2.15:34323 (size: 244.0 B, free: 366.2 MiB)\n",
      "24/09/30 17:38:14 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.2.15:34323 (size: 244.0 B, free: 366.2 MiB)\n",
      "24/09/30 17:38:14 INFO SparkContext: Created broadcast 17 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:14 INFO SparkContext: Created broadcast 16 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:14 INFO SparkContext: Created broadcast 15 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:14 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx001_data, parameters sent from Spark:|requiredColumns=[UF,Ano,V1013,V1012,A001B3,A003,A004,V1023,A005,B002,B0031,B005,B009B,B009D,B009F,B007,C01011,F001,B0011,B0012,B0013,B0014,B0015,B0016,B0017,B0018,B0019,B00110,B00111,B00112,B00113],|filters=[]\n",
      "24/09/30 17:38:14 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDE51SmFOLW5PV3dIaBoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:38:14.536Z\",\"Ended\":\"2024-09-30T20:38:14.915Z\",\"Parse Timings\":\"Average: PT0.000708561S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.321257328S Samples: 1\",\"Bytes/s\":1470,\"Rows/s\":0,\"Bytes\":472,\"Rows\":10,\"I/O time\":321}\n",
      "24/09/30 17:38:14 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1684 bytes result sent to driver\n",
      "24/09/30 17:38:14 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 8) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:38:14 INFO Executor: Running task 0.0 in stage 9.0 (TID 8)\n",
      "24/09/30 17:38:14 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 394 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:38:14 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:38:14 INFO DAGScheduler: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,304 s\n",
      "24/09/30 17:38:14 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDENMUGdoYy10M1UwcxoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:38:14.537Z\",\"Ended\":\"2024-09-30T20:38:14.926Z\",\"Parse Timings\":\"Average: PT0.000529477S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.296030324S Samples: 1\",\"Bytes/s\":1189,\"Rows/s\":0,\"Bytes\":352,\"Rows\":5,\"I/O time\":296}\n",
      "24/09/30 17:38:14 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:38:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "24/09/30 17:38:14 INFO DAGScheduler: Job 6 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2,633199 s\n",
      "24/09/30 17:38:14 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDBRVS12T3Brc1F3eRoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:38:14.540Z\",\"Ended\":\"2024-09-30T20:38:14.932Z\",\"Parse Timings\":\"Average: PT0.000856136S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.301088955S Samples: 1\",\"Bytes/s\":1116,\"Rows/s\":0,\"Bytes\":336,\"Rows\":6,\"I/O time\":301}\n",
      "24/09/30 17:38:14 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 4.0 MiB, free 345.9 MiB)\n",
      "24/09/30 17:38:14 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1639 bytes result sent to driver\n",
      "24/09/30 17:38:14 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDgzT09fYjE2bURnMBoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:38:14.540Z\",\"Ended\":\"2024-09-30T20:38:14.940Z\",\"Parse Timings\":\"Average: PT0.000548119S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.286985884S Samples: 1\",\"Bytes/s\":2713,\"Rows/s\":0,\"Bytes\":776,\"Rows\":27,\"I/O time\":286}\n",
      "24/09/30 17:38:14 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2060 bytes result sent to driver\n",
      "24/09/30 17:38:14 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 482.0 B, free 345.9 MiB)\n",
      "24/09/30 17:38:14 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 9) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:38:14 INFO Executor: Running task 0.0 in stage 8.0 (TID 9)\n",
      "24/09/30 17:38:14 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1602 bytes result sent to driver\n",
      "24/09/30 17:38:14 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.2.15:34323 (size: 482.0 B, free: 366.2 MiB)\n",
      "24/09/30 17:38:14 INFO SparkContext: Created broadcast 18 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:14 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:38:14 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)\n",
      "24/09/30 17:38:14 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:38:14 INFO CodeGenerator: Code generated in 21.384359 ms\n",
      "24/09/30 17:38:14 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 431 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:38:14 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:38:14 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 436 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:38:14 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:38:14 INFO DAGScheduler: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,349 s\n",
      "24/09/30 17:38:14 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)\n",
      "24/09/30 17:38:14 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 430 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:38:14 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:38:14 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:38:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "24/09/30 17:38:14 INFO DAGScheduler: Job 5 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2,665415 s\n",
      "24/09/30 17:38:14 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,365 s\n",
      "24/09/30 17:38:14 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:38:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "24/09/30 17:38:14 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2,666016 s\n",
      "24/09/30 17:38:14 INFO DAGScheduler: ResultStage 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,322 s\n",
      "24/09/30 17:38:14 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:38:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
      "24/09/30 17:38:14 INFO DAGScheduler: Job 7 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2,668844 s\n",
      "24/09/30 17:38:14 INFO CodeGenerator: Code generated in 14.652141 ms\n",
      "24/09/30 17:38:14 INFO CodeGenerator: Code generated in 22.205919 ms\n",
      "24/09/30 17:38:14 INFO CodeGenerator: Code generated in 10.896754 ms\n",
      "24/09/30 17:38:14 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 4.0 MiB, free 341.9 MiB)\n",
      "24/09/30 17:38:15 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 4.0 MiB, free 337.9 MiB)\n",
      "24/09/30 17:38:15 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 1038.0 B, free 337.9 MiB)\n",
      "24/09/30 17:38:15 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.2.15:34323 (size: 1038.0 B, free: 366.2 MiB)\n",
      "24/09/30 17:38:15 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 348.0 B, free 337.9 MiB)\n",
      "24/09/30 17:38:15 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.2.15:34323 (size: 348.0 B, free: 366.2 MiB)\n",
      "24/09/30 17:38:15 INFO SparkContext: Created broadcast 19 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:15 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 4.0 MiB, free 333.9 MiB)\n",
      "24/09/30 17:38:15 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 359.0 B, free 333.9 MiB)\n",
      "24/09/30 17:38:15 INFO SparkContext: Created broadcast 20 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:15 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.2.15:34323 (size: 359.0 B, free: 366.2 MiB)\n",
      "24/09/30 17:38:15 INFO SparkContext: Created broadcast 21 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:15 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFVCVFZ3SklCamJ2TRoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:38:14.731Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:38:15.246Z\",\"readSessionPrepDuration\":206,\"readSessionCreationDuration\":309,\"readSessionDuration\":515}\n",
      "24/09/30 17:38:15 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 4 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFVCVFZ3SklCamJ2TRoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:38:15 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx001_data': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFVCVFZ3SklCamJ2TRoCdngaAnVo\n",
      "24/09/30 17:38:15 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx001_data, parameters sent from Spark:|requiredColumns=[UF,Ano,V1013,V1012,A001B3,A003,A004,V1023,A005,B002,B0031,B005,B009B,B009D,B009F,B007,C01011,F001,B0011,B0012,B0013,B0014,B0015,B0016,B0017,B0018,B0019,B00110,B00111,B00112,B00113],|filters=[]\n",
      "24/09/30 17:38:15 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHpvQVR0QVVkbXBhTRoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:38:14.930Z\",\"Ended\":\"2024-09-30T20:38:15.276Z\",\"Parse Timings\":\"Average: PT0.001520084S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.289813003S Samples: 1\",\"Bytes/s\":1494,\"Rows/s\":7000,\"Bytes\":432,\"Rows\":7,\"I/O time\":289}\n",
      "24/09/30 17:38:15 INFO Executor: Finished task 0.0 in stage 9.0 (TID 8). 1687 bytes result sent to driver\n",
      "24/09/30 17:38:15 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:38:15 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 8) in 362 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:38:15 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:38:15 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)\n",
      "24/09/30 17:38:15 INFO DAGScheduler: ResultStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,614 s\n",
      "24/09/30 17:38:15 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:38:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
      "24/09/30 17:38:15 INFO DAGScheduler: Job 8 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2,987746 s\n",
      "24/09/30 17:38:15 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHdaalZSUUZ3Q25faBoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:38:14.960Z\",\"Ended\":\"2024-09-30T20:38:15.300Z\",\"Parse Timings\":\"Average: PT0.000390847S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.28418211S Samples: 1\",\"Bytes/s\":1126,\"Rows/s\":0,\"Bytes\":320,\"Rows\":4,\"I/O time\":284}\n",
      "24/09/30 17:38:15 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlLZGZDUENTVXdhShoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:38:14.954Z\",\"Ended\":\"2024-09-30T20:38:15.300Z\",\"Parse Timings\":\"Average: PT0.000405514S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.289645177S Samples: 1\",\"Bytes/s\":968,\"Rows/s\":0,\"Bytes\":280,\"Rows\":2,\"I/O time\":289}\n",
      "24/09/30 17:38:15 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 1562 bytes result sent to driver\n",
      "24/09/30 17:38:15 INFO Executor: Finished task 0.0 in stage 8.0 (TID 9). 1503 bytes result sent to driver\n",
      "24/09/30 17:38:15 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 4.0 MiB, free 329.9 MiB)\n",
      "24/09/30 17:38:15 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:38:15 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 351 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:38:15 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:38:15 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 9) in 361 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:38:15 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:38:15 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)\n",
      "24/09/30 17:38:15 INFO DAGScheduler: ResultStage 11 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,631 s\n",
      "24/09/30 17:38:15 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:38:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
      "24/09/30 17:38:15 INFO DAGScheduler: Job 11 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 3,010838 s\n",
      "24/09/30 17:38:15 INFO DAGScheduler: ResultStage 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,662 s\n",
      "24/09/30 17:38:15 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:38:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
      "24/09/30 17:38:15 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDJLNl9QaFppYldLdRoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:38:14.967Z\",\"Ended\":\"2024-09-30T20:38:15.315Z\",\"Parse Timings\":\"Average: PT0.005255457S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.290343472S Samples: 1\",\"Bytes/s\":1213,\"Rows/s\":1000,\"Bytes\":352,\"Rows\":5,\"I/O time\":290}\n",
      "24/09/30 17:38:15 INFO DAGScheduler: Job 9 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 3,013659 s\n",
      "24/09/30 17:38:15 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 1596 bytes result sent to driver\n",
      "24/09/30 17:38:15 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 467.0 B, free 329.9 MiB)\n",
      "24/09/30 17:38:15 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 371 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:38:15 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:38:15 INFO DAGScheduler: ResultStage 10 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,649 s\n",
      "24/09/30 17:38:15 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:38:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
      "24/09/30 17:38:15 INFO DAGScheduler: Job 10 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 3,022682 s\n",
      "24/09/30 17:38:15 INFO CodeGenerator: Code generated in 21.342867 ms\n",
      "24/09/30 17:38:15 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 4.0 MiB, free 325.9 MiB)\n",
      "24/09/30 17:38:15 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.2.15:34323 (size: 467.0 B, free: 366.2 MiB)\n",
      "24/09/30 17:38:15 INFO SparkContext: Created broadcast 22 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:15 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 215.0 B, free 325.9 MiB)\n",
      "24/09/30 17:38:15 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 4.0 MiB, free 317.9 MiB)\n",
      "24/09/30 17:38:15 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 4.0 MiB, free 321.9 MiB)\n",
      "24/09/30 17:38:15 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.2.15:34323 (size: 215.0 B, free: 366.2 MiB)\n",
      "24/09/30 17:38:15 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 295.0 B, free 317.9 MiB)\n",
      "24/09/30 17:38:15 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 348.0 B, free 317.9 MiB)\n",
      "24/09/30 17:38:15 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.2.15:34323 (size: 295.0 B, free: 366.2 MiB)\n",
      "24/09/30 17:38:15 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.2.15:34323 (size: 348.0 B, free: 366.2 MiB)\n",
      "24/09/30 17:38:15 INFO SparkContext: Created broadcast 23 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:15 INFO SparkContext: Created broadcast 24 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:15 INFO SparkContext: Created broadcast 25 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:15 INFO CodeGenerator: Code generated in 13.515139 ms\n",
      "24/09/30 17:38:15 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGRQUldCdjUyOUtPbhoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:38:15.287Z\",\"Ended\":\"2024-09-30T20:38:15.625Z\",\"Parse Timings\":\"Average: PT0.000639826S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.279873479S Samples: 1\",\"Bytes/s\":1003,\"Rows/s\":0,\"Bytes\":280,\"Rows\":3,\"I/O time\":279}\n",
      "24/09/30 17:38:15 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 1521 bytes result sent to driver\n",
      "24/09/30 17:38:15 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 351 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:38:15 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:38:15 INFO DAGScheduler: ResultStage 12 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,943 s\n",
      "24/09/30 17:38:15 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:38:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
      "24/09/30 17:38:15 INFO DAGScheduler: Job 12 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 3,330565 s\n",
      "24/09/30 17:38:15 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 4.0 MiB, free 313.9 MiB)\n",
      "24/09/30 17:38:15 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 244.0 B, free 313.9 MiB)\n",
      "24/09/30 17:38:15 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.2.15:34323 (size: 244.0 B, free: 366.2 MiB)\n",
      "24/09/30 17:38:15 INFO SparkContext: Created broadcast 26 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:15 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEZIbG5USE9haHVmTRoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-30T20:38:15.310Z\",\"Ended\":\"2024-09-30T20:38:15.651Z\",\"Parse Timings\":\"Average: PT0.000674834S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.279359702S Samples: 1\",\"Bytes/s\":1835,\"Rows/s\":0,\"Bytes\":512,\"Rows\":4,\"I/O time\":279}\n",
      "24/09/30 17:38:15 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 1702 bytes result sent to driver\n",
      "24/09/30 17:38:15 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 349 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/30 17:38:15 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:38:15 INFO DAGScheduler: ResultStage 13 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,958 s\n",
      "24/09/30 17:38:15 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:38:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
      "24/09/30 17:38:15 INFO DAGScheduler: Job 13 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 3,354899 s\n",
      "24/09/30 17:38:15 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 4.0 MiB, free 309.9 MiB)\n",
      "24/09/30 17:38:15 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 446.0 B, free 309.9 MiB)\n",
      "24/09/30 17:38:15 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.2.15:34323 (size: 446.0 B, free: 366.2 MiB)\n",
      "24/09/30 17:38:15 INFO SparkContext: Created broadcast 27 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/30 17:38:16 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHZuUldTajBNcERXMhoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:38:15.270Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:38:16.137Z\",\"readSessionPrepDuration\":225,\"readSessionCreationDuration\":642,\"readSessionDuration\":867}\n",
      "24/09/30 17:38:16 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 4 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHZuUldTajBNcERXMhoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:38:16 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx001_data': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHZuUldTajBNcERXMhoCdngaAnVo\n",
      "24/09/30 17:38:16 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx001_data, parameters sent from Spark:|requiredColumns=[UF,Ano,V1013,V1012,A001B3,A003,A004,V1023,A005,B002,B0031,B005,B009B,B009D,B009F,B007,C01011,F001,B0011,B0012,B0013,B0014,B0015,B0016,B0017,B0018,B0019,B00110,B00111,B00112,B00113],|filters=[]\n",
      "24/09/30 17:38:16 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHRoUUxFT0g2MlUtQxoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-30T20:38:16.161Z\",\"readSessionCreationEndTime\":\"2024-09-30T20:38:16.698Z\",\"readSessionPrepDuration\":220,\"readSessionCreationDuration\":317,\"readSessionDuration\":537}\n",
      "24/09/30 17:38:16 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 4 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHRoUUxFT0g2MlUtQxoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/30 17:38:16 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx001_data': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHRoUUxFT0g2MlUtQxoCdngaAnVo\n",
      "24/09/30 17:38:17 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/30 17:38:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/30 17:38:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/30 17:38:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/30 17:38:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/30 17:38:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/30 17:38:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/30 17:38:18 INFO GhfsStorageStatistics: Detected potential high latency for operation op_mkdirs. latencyMs=876; previousMaxLatencyMs=0; operationCount=1; context=gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2/_temporary/0\n",
      "24/09/30 17:38:18 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.2.15:34323 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:38:18 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.2.15:34323 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:38:18 INFO CodeGenerator: Generated method too long to be JIT compiled: org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage15.processNext is 8483 bytes\n",
      "24/09/30 17:38:18 INFO CodeGenerator: Code generated in 93.436648 ms\n",
      "24/09/30 17:38:18 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.2.15:34323 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:38:18 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105\n",
      "24/09/30 17:38:18 INFO DAGScheduler: Got job 14 (save at BigQueryWriteHelper.java:105) with 4 output partitions\n",
      "24/09/30 17:38:18 INFO DAGScheduler: Final stage: ResultStage 14 (save at BigQueryWriteHelper.java:105)\n",
      "24/09/30 17:38:18 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/30 17:38:19 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/30 17:38:19 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[47] at save at BigQueryWriteHelper.java:105), which has no missing parents\n",
      "24/09/30 17:38:19 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.2.15:34323 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:38:19 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:34323 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:38:19 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.2.15:34323 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:38:19 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:34323 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:38:19 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.2.15:34323 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:38:19 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.2.15:34323 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:38:19 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 295.3 KiB, free 309.8 MiB)\n",
      "24/09/30 17:38:19 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 99.6 KiB, free 309.7 MiB)\n",
      "24/09/30 17:38:19 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.2.15:34323 (size: 99.6 KiB, free: 366.1 MiB)\n",
      "24/09/30 17:38:19 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/30 17:38:19 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 14 (MapPartitionsRDD[47] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "24/09/30 17:38:19 INFO TaskSchedulerImpl: Adding task set 14.0 with 4 tasks resource profile 0\n",
      "24/09/30 17:38:19 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.2.15:34323 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:38:19 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/30 17:38:19 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 15) (10.0.2.15, executor driver, partition 1, PROCESS_LOCAL, 9270 bytes) \n",
      "24/09/30 17:38:19 INFO TaskSetManager: Starting task 2.0 in stage 14.0 (TID 16) (10.0.2.15, executor driver, partition 2, PROCESS_LOCAL, 9270 bytes) \n",
      "24/09/30 17:38:19 INFO TaskSetManager: Starting task 3.0 in stage 14.0 (TID 17) (10.0.2.15, executor driver, partition 3, PROCESS_LOCAL, 9270 bytes) \n",
      "24/09/30 17:38:19 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)\n",
      "24/09/30 17:38:19 INFO Executor: Running task 2.0 in stage 14.0 (TID 16)\n",
      "24/09/30 17:38:19 INFO Executor: Running task 1.0 in stage 14.0 (TID 15)\n",
      "24/09/30 17:38:19 INFO Executor: Running task 3.0 in stage 14.0 (TID 17)\n",
      "24/09/30 17:38:19 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.2.15:34323 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:38:19 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.2.15:34323 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:38:19 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.2.15:34323 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:38:19 INFO CodeGenerator: Generated method too long to be JIT compiled: org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage15.processNext is 8483 bytes\n",
      "24/09/30 17:38:19 INFO CodeGenerator: Code generated in 101.474029 ms\n",
      "24/09/30 17:38:19 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.2.15:34323 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/30 17:38:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/30 17:38:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/30 17:38:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/30 17:38:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/30 17:38:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/30 17:38:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/30 17:38:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/30 17:38:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/30 17:38:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/30 17:38:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/30 17:38:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/30 17:38:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/30 17:38:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/30 17:38:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/30 17:38:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/30 17:38:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/30 17:38:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/30 17:38:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/30 17:38:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/30 17:38:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/30 17:38:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/30 17:38:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/30 17:38:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/30 17:38:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/30 17:38:19 INFO CodecConfig: Compression: SNAPPY\n",
      "24/09/30 17:38:19 INFO CodecConfig: Compression: SNAPPY\n",
      "24/09/30 17:38:19 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/09/30 17:38:19 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"estado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ano\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"mes\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"semana\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ano_nascimento\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"faixa_etaria\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"sexo\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"cor_raca\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tipo_area\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"escolaridade\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_sintomas_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"foi_posto_saude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ficou_em_casa\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ficou_internado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"foi_ao_posto_ou_internado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"resultado_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tem_plano_saude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"faixa_rendimento\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"situacao_domicilio\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary estado (STRING);\n",
      "  optional int32 ano;\n",
      "  optional int32 mes;\n",
      "  optional int32 semana;\n",
      "  optional int32 ano_nascimento;\n",
      "  required binary faixa_etaria (STRING);\n",
      "  optional binary sexo (STRING);\n",
      "  optional binary cor_raca (STRING);\n",
      "  optional binary tipo_area (STRING);\n",
      "  optional binary escolaridade (STRING);\n",
      "  optional binary teve_sintomas_covid (STRING);\n",
      "  optional binary foi_posto_saude (STRING);\n",
      "  optional binary ficou_em_casa (STRING);\n",
      "  optional binary ficou_internado (STRING);\n",
      "  optional binary foi_ao_posto_ou_internado (STRING);\n",
      "  optional binary teve_covid (STRING);\n",
      "  optional binary resultado_covid (STRING);\n",
      "  optional binary tem_plano_saude (STRING);\n",
      "  optional binary faixa_rendimento (STRING);\n",
      "  optional binary situacao_domicilio (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/09/30 17:38:19 INFO CodecConfig: Compression: SNAPPY             (0 + 4) / 4]\n",
      "24/09/30 17:38:19 INFO CodecConfig: Compression: SNAPPY\n",
      "24/09/30 17:38:19 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/09/30 17:38:19 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"estado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ano\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"mes\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"semana\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ano_nascimento\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"faixa_etaria\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"sexo\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"cor_raca\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tipo_area\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"escolaridade\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_sintomas_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"foi_posto_saude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ficou_em_casa\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ficou_internado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"foi_ao_posto_ou_internado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"resultado_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tem_plano_saude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"faixa_rendimento\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"situacao_domicilio\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary estado (STRING);\n",
      "  optional int32 ano;\n",
      "  optional int32 mes;\n",
      "  optional int32 semana;\n",
      "  optional int32 ano_nascimento;\n",
      "  required binary faixa_etaria (STRING);\n",
      "  optional binary sexo (STRING);\n",
      "  optional binary cor_raca (STRING);\n",
      "  optional binary tipo_area (STRING);\n",
      "  optional binary escolaridade (STRING);\n",
      "  optional binary teve_sintomas_covid (STRING);\n",
      "  optional binary foi_posto_saude (STRING);\n",
      "  optional binary ficou_em_casa (STRING);\n",
      "  optional binary ficou_internado (STRING);\n",
      "  optional binary foi_ao_posto_ou_internado (STRING);\n",
      "  optional binary teve_covid (STRING);\n",
      "  optional binary resultado_covid (STRING);\n",
      "  optional binary tem_plano_saude (STRING);\n",
      "  optional binary faixa_rendimento (STRING);\n",
      "  optional binary situacao_domicilio (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/09/30 17:38:19 INFO CodecConfig: Compression: SNAPPY\n",
      "24/09/30 17:38:19 INFO CodecConfig: Compression: SNAPPY\n",
      "24/09/30 17:38:19 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/09/30 17:38:19 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"estado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ano\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"mes\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"semana\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ano_nascimento\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"faixa_etaria\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"sexo\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"cor_raca\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tipo_area\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"escolaridade\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_sintomas_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"foi_posto_saude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ficou_em_casa\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ficou_internado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"foi_ao_posto_ou_internado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"resultado_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tem_plano_saude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"faixa_rendimento\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"situacao_domicilio\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary estado (STRING);\n",
      "  optional int32 ano;\n",
      "  optional int32 mes;\n",
      "  optional int32 semana;\n",
      "  optional int32 ano_nascimento;\n",
      "  required binary faixa_etaria (STRING);\n",
      "  optional binary sexo (STRING);\n",
      "  optional binary cor_raca (STRING);\n",
      "  optional binary tipo_area (STRING);\n",
      "  optional binary escolaridade (STRING);\n",
      "  optional binary teve_sintomas_covid (STRING);\n",
      "  optional binary foi_posto_saude (STRING);\n",
      "  optional binary ficou_em_casa (STRING);\n",
      "  optional binary ficou_internado (STRING);\n",
      "  optional binary foi_ao_posto_ou_internado (STRING);\n",
      "  optional binary teve_covid (STRING);\n",
      "  optional binary resultado_covid (STRING);\n",
      "  optional binary tem_plano_saude (STRING);\n",
      "  optional binary faixa_rendimento (STRING);\n",
      "  optional binary situacao_domicilio (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/09/30 17:38:19 INFO CodecConfig: Compression: SNAPPY\n",
      "24/09/30 17:38:19 INFO CodecConfig: Compression: SNAPPY\n",
      "24/09/30 17:38:19 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/09/30 17:38:19 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"estado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ano\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"mes\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"semana\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ano_nascimento\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"faixa_etaria\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"sexo\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"cor_raca\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tipo_area\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"escolaridade\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_sintomas_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"foi_posto_saude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ficou_em_casa\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ficou_internado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"foi_ao_posto_ou_internado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"resultado_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tem_plano_saude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"faixa_rendimento\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"situacao_domicilio\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary estado (STRING);\n",
      "  optional int32 ano;\n",
      "  optional int32 mes;\n",
      "  optional int32 semana;\n",
      "  optional int32 ano_nascimento;\n",
      "  required binary faixa_etaria (STRING);\n",
      "  optional binary sexo (STRING);\n",
      "  optional binary cor_raca (STRING);\n",
      "  optional binary tipo_area (STRING);\n",
      "  optional binary escolaridade (STRING);\n",
      "  optional binary teve_sintomas_covid (STRING);\n",
      "  optional binary foi_posto_saude (STRING);\n",
      "  optional binary ficou_em_casa (STRING);\n",
      "  optional binary ficou_internado (STRING);\n",
      "  optional binary foi_ao_posto_ou_internado (STRING);\n",
      "  optional binary teve_covid (STRING);\n",
      "  optional binary resultado_covid (STRING);\n",
      "  optional binary tem_plano_saude (STRING);\n",
      "  optional binary faixa_rendimento (STRING);\n",
      "  optional binary situacao_domicilio (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/09/30 17:38:20 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "24/09/30 17:38:20 INFO GhfsStorageStatistics: Detected potential high latency for operation op_create. latencyMs=777; previousMaxLatencyMs=497; operationCount=4; context=gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2/_temporary/0/_temporary/attempt_202409301738184928490435107356348_0014_m_000001_15/part-00001-35926181-2dc3-41af-98ae-cbc2724e66e5-c000.snappy.parquet\n",
      "24/09/30 17:38:20 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "24/09/30 17:38:20 INFO GhfsStorageStatistics: Detected potential high latency for operation op_create. latencyMs=868; previousMaxLatencyMs=777; operationCount=4; context=gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2/_temporary/0/_temporary/attempt_202409301738184928490435107356348_0014_m_000002_16/part-00002-35926181-2dc3-41af-98ae-cbc2724e66e5-c000.snappy.parquet\n",
      "24/09/30 17:38:20 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "24/09/30 17:38:21 INFO GhfsStorageStatistics: Detected potential high latency for operation op_create. latencyMs=1743; previousMaxLatencyMs=868; operationCount=4; context=gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2/_temporary/0/_temporary/attempt_202409301738184928490435107356348_0014_m_000003_17/part-00003-35926181-2dc3-41af-98ae-cbc2724e66e5-c000.snappy.parquet\n",
      "24/09/30 17:38:21 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "24/09/30 17:38:50 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHRoUUxFT0g2MlUtQxoCdngaAnVo/streams/GgJ2eBoCdWgoAg,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHRoUUxFT0g2MlUtQxoCdngaAnVo/streams/CAEaAnZ4GgJ1aCgC,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHRoUUxFT0g2MlUtQxoCdngaAnVo/streams/CAIaAnZ4GgJ1aCgC,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHRoUUxFT0g2MlUtQxoCdngaAnVo/streams/CAMaAnZ4GgJ1aCgC\",\"Started\":\"2024-09-30T20:38:19.245Z\",\"Ended\":\"2024-09-30T20:38:50.466Z\",\"Parse Timings\":\"Average: PT0.000446827S Samples: 691\",\"Time in Spark\":\"Average: PT0.043178355S\",\"Time waiting for service\":\"Average: PT0.023998288S Samples: 691\",\"Bytes/s\":6339728,\"Rows/s\":2151344,\"Bytes\":105125384,\"Rows\":662614,\"I/O time\":16582}\n",
      "24/09/30 17:38:52 INFO GhfsStorageStatistics: Detected potential high latency for operation stream_write_close_operations. latencyMs=1531; previousMaxLatencyMs=0; operationCount=1; context=gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2/_temporary/0/_temporary/attempt_202409301738184928490435107356348_0014_m_000000_14/part-00000-35926181-2dc3-41af-98ae-cbc2724e66e5-c000.snappy.parquet\n",
      "24/09/30 17:38:55 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2/_temporary/0/_temporary/' directory.\n",
      "24/09/30 17:38:55 INFO GhfsStorageStatistics: Detected potential high latency for operation op_rename. latencyMs=1897; previousMaxLatencyMs=0; operationCount=1; context=rename(gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2/_temporary/0/_temporary/attempt_202409301738184928490435107356348_0014_m_000000_14 -> gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2/_temporary/0/task_202409301738184928490435107356348_0014_m_000000)\n",
      "24/09/30 17:38:55 INFO FileOutputCommitter: Saved output of task 'attempt_202409301738184928490435107356348_0014_m_000000_14' to gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2/_temporary/0/task_202409301738184928490435107356348_0014_m_000000\n",
      "24/09/30 17:38:55 INFO SparkHadoopMapRedUtil: attempt_202409301738184928490435107356348_0014_m_000000_14: Committed. Elapsed time: 2639 ms.\n",
      "24/09/30 17:38:55 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 3354 bytes result sent to driver\n",
      "24/09/30 17:38:55 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 36752 ms on 10.0.2.15 (executor driver) (1/4)\n",
      "24/09/30 17:38:59 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHRoUUxFT0g2MlUtQxoCdngaAnVo/streams/GgJ2eBoCdWgoAg,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHRoUUxFT0g2MlUtQxoCdngaAnVo/streams/CAEaAnZ4GgJ1aCgC,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHRoUUxFT0g2MlUtQxoCdngaAnVo/streams/CAIaAnZ4GgJ1aCgC,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHRoUUxFT0g2MlUtQxoCdngaAnVo/streams/CAMaAnZ4GgJ1aCgC\",\"Started\":\"2024-09-30T20:38:19.247Z\",\"Ended\":\"2024-09-30T20:38:59.400Z\",\"Parse Timings\":\"Average: PT0.000224325S Samples: 691\",\"Time in Spark\":\"Average: PT0.057122174S\",\"Time waiting for service\":\"Average: PT0.038388532S Samples: 691\",\"Bytes/s\":3977498,\"Rows/s\":4274935,\"Bytes\":105507120,\"Rows\":662615,\"I/O time\":26526}\n",
      "24/09/30 17:38:59 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHRoUUxFT0g2MlUtQxoCdngaAnVo/streams/GgJ2eBoCdWgoAg,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHRoUUxFT0g2MlUtQxoCdngaAnVo/streams/CAEaAnZ4GgJ1aCgC,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHRoUUxFT0g2MlUtQxoCdngaAnVo/streams/CAIaAnZ4GgJ1aCgC,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHRoUUxFT0g2MlUtQxoCdngaAnVo/streams/CAMaAnZ4GgJ1aCgC\",\"Started\":\"2024-09-30T20:38:19.247Z\",\"Ended\":\"2024-09-30T20:38:59.491Z\",\"Parse Timings\":\"Average: PT0.000261087S Samples: 691\",\"Time in Spark\":\"Average: PT0.05724929S\",\"Time waiting for service\":\"Average: PT0.041807382S Samples: 691\",\"Bytes/s\":3673756,\"Rows/s\":3681194,\"Bytes\":106127488,\"Rows\":662615,\"I/O time\":28888}\n",
      "24/09/30 17:39:00 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHRoUUxFT0g2MlUtQxoCdngaAnVo/streams/GgJ2eBoCdWgoAg,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHRoUUxFT0g2MlUtQxoCdngaAnVo/streams/CAEaAnZ4GgJ1aCgC,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHRoUUxFT0g2MlUtQxoCdngaAnVo/streams/CAIaAnZ4GgJ1aCgC,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHRoUUxFT0g2MlUtQxoCdngaAnVo/streams/CAMaAnZ4GgJ1aCgC\",\"Started\":\"2024-09-30T20:38:19.198Z\",\"Ended\":\"2024-09-30T20:39:00.836Z\",\"Parse Timings\":\"Average: PT0.000928724S Samples: 691\",\"Time in Spark\":\"Average: PT0.058530919S\",\"Time waiting for service\":\"Average: PT0.042248209S Samples: 691\",\"Bytes/s\":3609023,\"Rows/s\":1033720,\"Bytes\":105358232,\"Rows\":662615,\"I/O time\":29193}\n",
      "24/09/30 17:39:01 INFO GhfsStorageStatistics: Detected potential high latency for operation stream_write_close_operations. latencyMs=1593; previousMaxLatencyMs=1531; operationCount=4; context=gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2/_temporary/0/_temporary/attempt_202409301738184928490435107356348_0014_m_000001_15/part-00001-35926181-2dc3-41af-98ae-cbc2724e66e5-c000.snappy.parquet\n",
      "24/09/30 17:39:03 INFO FileOutputCommitter: Saved output of task 'attempt_202409301738184928490435107356348_0014_m_000003_17' to gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2/_temporary/0/task_202409301738184928490435107356348_0014_m_000003\n",
      "24/09/30 17:39:03 INFO SparkHadoopMapRedUtil: attempt_202409301738184928490435107356348_0014_m_000003_17: Committed. Elapsed time: 2314 ms.\n",
      "24/09/30 17:39:03 INFO Executor: Finished task 3.0 in stage 14.0 (TID 17). 3311 bytes result sent to driver\n",
      "24/09/30 17:39:03 INFO TaskSetManager: Finished task 3.0 in stage 14.0 (TID 17) in 44679 ms on 10.0.2.15 (executor driver) (2/4)\n",
      "24/09/30 17:39:04 INFO FileOutputCommitter: Saved output of task 'attempt_202409301738184928490435107356348_0014_m_000001_15' to gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2/_temporary/0/task_202409301738184928490435107356348_0014_m_000001\n",
      "24/09/30 17:39:04 INFO SparkHadoopMapRedUtil: attempt_202409301738184928490435107356348_0014_m_000001_15: Committed. Elapsed time: 2255 ms.\n",
      "24/09/30 17:39:04 INFO Executor: Finished task 1.0 in stage 14.0 (TID 15). 3354 bytes result sent to driver\n",
      "24/09/30 17:39:04 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 15) in 44945 ms on 10.0.2.15 (executor driver) (3/4)\n",
      "24/09/30 17:39:05 INFO FileOutputCommitter: Saved output of task 'attempt_202409301738184928490435107356348_0014_m_000002_16' to gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2/_temporary/0/task_202409301738184928490435107356348_0014_m_000002\n",
      "24/09/30 17:39:05 INFO SparkHadoopMapRedUtil: attempt_202409301738184928490435107356348_0014_m_000002_16: Committed. Elapsed time: 2240 ms.\n",
      "24/09/30 17:39:05 INFO Executor: Finished task 2.0 in stage 14.0 (TID 16). 3311 bytes result sent to driver\n",
      "24/09/30 17:39:05 INFO TaskSetManager: Finished task 2.0 in stage 14.0 (TID 16) in 46228 ms on 10.0.2.15 (executor driver) (4/4)\n",
      "24/09/30 17:39:05 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "24/09/30 17:39:05 INFO DAGScheduler: ResultStage 14 (save at BigQueryWriteHelper.java:105) finished in 46,381 s\n",
      "24/09/30 17:39:05 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/30 17:39:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished\n",
      "24/09/30 17:39:05 INFO DAGScheduler: Job 14 finished: save at BigQueryWriteHelper.java:105, took 46,389162 s\n",
      "24/09/30 17:39:05 INFO FileFormatWriter: Start to commit write Job 8511fda8-cb6e-4d5f-af51-8486bdaf8f6a.\n",
      "24/09/30 17:39:08 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2/_temporary/0/task_202409301738184928490435107356348_0014_m_000000/' directory.\n",
      "24/09/30 17:39:09 INFO GhfsStorageStatistics: Detected potential high latency for operation op_list_status. latencyMs=546; previousMaxLatencyMs=231; operationCount=3; context=gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2/_temporary/0/task_202409301738184928490435107356348_0014_m_000001\n",
      "24/09/30 17:39:10 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2/_temporary/0/task_202409301738184928490435107356348_0014_m_000001/' directory.\n",
      "24/09/30 17:39:13 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2/_temporary/0/task_202409301738184928490435107356348_0014_m_000002/' directory.\n",
      "24/09/30 17:39:14 INFO GhfsStorageStatistics: Detected potential high latency for operation op_list_status. latencyMs=567; previousMaxLatencyMs=546; operationCount=5; context=gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2/_temporary/0/task_202409301738184928490435107356348_0014_m_000003\n",
      "24/09/30 17:39:16 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2/_temporary/0/task_202409301738184928490435107356348_0014_m_000003/' directory.\n",
      "24/09/30 17:39:17 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2/' directory.\n",
      "24/09/30 17:39:17 INFO GhfsStorageStatistics: Detected potential high latency for operation op_delete. latencyMs=1512; previousMaxLatencyMs=0; operationCount=1; context=gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2/_temporary\n",
      "24/09/30 17:39:19 INFO FileFormatWriter: Write Job 8511fda8-cb6e-4d5f-af51-8486bdaf8f6a committed. Elapsed time: 14275 ms.\n",
      "24/09/30 17:39:19 INFO FileFormatWriter: Finished processing stats for write job 8511fda8-cb6e-4d5f-af51-8486bdaf8f6a.\n",
      "24/09/30 17:39:20 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=SPEC, tableId=tbx001_data}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=estado, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ano, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=mes, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=semana, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ano_nascimento, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=faixa_etaria, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=sexo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=cor_raca, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=tipo_area, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=escolaridade, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teve_sintomas_covid, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=foi_posto_saude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ficou_em_casa, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ficou_internado, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=foi_ao_posto_ou_internado, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teve_covid, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=resultado_covid, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=tem_plano_saude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=faixa_rendimento, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=situacao_domicilio, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2/part-00002-35926181-2dc3-41af-98ae-cbc2724e66e5-c000.snappy.parquet, gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2/part-00003-35926181-2dc3-41af-98ae-cbc2724e66e5-c000.snappy.parquet, gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2/part-00000-35926181-2dc3-41af-98ae-cbc2724e66e5-c000.snappy.parquet, gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727728670946-92487ec5-3579-492d-9777-f9ccd7a1a0b2/part-00001-35926181-2dc3-41af-98ae-cbc2724e66e5-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=my-project-1508437523553, job=267a8b44-4195-4688-a929-41f9950a3c0b, location=us-east1}\n",
      "24/09/30 17:39:36 INFO BigQueryClient: Done loading to SPEC.tbx001_data. jobId: JobId{project=my-project-1508437523553, job=267a8b44-4195-4688-a929-41f9950a3c0b, location=us-east1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 18:08:03 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.2.15:34323 in memory (size: 348.0 B, free: 366.2 MiB)\n",
      "24/09/30 18:08:03 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 10.0.2.15:34323 in memory (size: 244.0 B, free: 366.2 MiB)\n",
      "24/09/30 18:08:03 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 10.0.2.15:34323 in memory (size: 99.6 KiB, free: 366.3 MiB)\n",
      "24/09/30 18:08:03 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 10.0.2.15:34323 in memory (size: 359.0 B, free: 366.3 MiB)\n",
      "24/09/30 18:08:03 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 10.0.2.15:34323 in memory (size: 348.0 B, free: 366.3 MiB)\n",
      "24/09/30 18:08:03 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.2.15:34323 in memory (size: 1038.0 B, free: 366.3 MiB)\n",
      "24/09/30 18:08:03 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.2.15:34323 in memory (size: 492.0 B, free: 366.3 MiB)\n",
      "24/09/30 18:08:03 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.2.15:34323 in memory (size: 244.0 B, free: 366.3 MiB)\n",
      "24/09/30 18:08:03 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 10.0.2.15:34323 in memory (size: 467.0 B, free: 366.3 MiB)\n",
      "24/09/30 18:08:03 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 10.0.2.15:34323 in memory (size: 295.0 B, free: 366.3 MiB)\n",
      "24/09/30 18:08:03 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 10.0.2.15:34323 in memory (size: 215.0 B, free: 366.3 MiB)\n",
      "24/09/30 18:08:03 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.2.15:34323 in memory (size: 244.0 B, free: 366.3 MiB)\n",
      "24/09/30 18:08:03 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 10.0.2.15:34323 in memory (size: 446.0 B, free: 366.3 MiB)\n",
      "24/09/30 18:08:03 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.2.15:34323 in memory (size: 348.0 B, free: 366.3 MiB)\n",
      "24/09/30 18:08:03 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.2.15:34323 in memory (size: 482.0 B, free: 366.3 MiB)\n"
     ]
    }
   ],
   "source": [
    "save_to_bigquery(df_joined, \"SPEC\", \"tbx001_data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
