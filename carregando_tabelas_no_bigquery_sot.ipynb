{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/28 14:22:18 WARN Utils: Your hostname, spark-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/09/28 14:22:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/spark/.ivy2/cache\n",
      "The jars for the packages stored in: /home/spark/.ivy2/jars\n",
      "com.google.cloud.spark#spark-bigquery-with-dependencies_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-315bab2c-9ea1-47cd-8b66-2540eda9f648;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.23.2 in central\n",
      ":: resolution report :: resolve 903ms :: artifacts dl 22ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.23.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-315bab2c-9ea1-47cd-8b66-2540eda9f648\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/17ms)\n",
      "24/09/28 14:22:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Criação da sessão Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark BigQuery Connection\") \\\n",
    "    .config('spark.jars.packages', 'com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2') \\\n",
    "    .config(\"spark.jars\", \"/usr/local/lib/spark-connectors/bigquery-connector-hadoop2-latest.jar\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true -Dio.netty.noUnsafe=true\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true -Dio.netty.noUnsafe=true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"viewsEnabled\", True)\n",
    "spark.conf.set(\"materializationDataset\", \"SOR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n",
    "sc._jsc.hadoopConfiguration().set('fs.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem')\n",
    "sc._jsc.hadoopConfiguration().set('fs.gs.auth.service.account.json.keyfile', '/usr/local/lib/gcp/credentials/my-project-1508437523553-e9bafe7e3368.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para salvar DataFrame em formato Parquet\n",
    "def save_to_bigquery(df, dataset, table_name):\n",
    "    # Salva o DataFrame em formato Parquet\n",
    "    df.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"table\", f\"{dataset.upper()}.{table_name}\") \\\n",
    "    .option(\"temporaryGcsBucket\", \"meu-bucket-temporario-spark\") \\\n",
    "    .option(\"credentialsFile\", \"/usr/local/lib/gcp/credentials/my-project-1508437523553-e9bafe7e3368.json\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para ler dados do BigQuery\n",
    "def read_from_bigquery(dataset, table_name):  \n",
    "    df = spark.read \\\n",
    "        .format('bigquery') \\\n",
    "        .option('table', f\"{dataset.upper()}.{table_name}\") \\\n",
    "        .option(\"credentialsFile\", \"/usr/local/lib/gcp/credentials/my-project-1508437523553-e9bafe7e3368.json\") \\\n",
    "        .load()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lista de tabelas do BigQuery\n",
    "# A001B3|2650459       |\n",
    "# |A001B1|2650459       |\n",
    "# |A001B2|2650459       |\n",
    "# |UF    |2650459       |\n",
    "# |Ano   |2650459       |\n",
    "# |V1012 |2650459       |\n",
    "# |A002  |2650459       |\n",
    "# |V1016 |2650459       |\n",
    "# |A003  |2650459       |\n",
    "# |UPA   |2650459       |\n",
    "# |A004  |2650459       |\n",
    "# |V1023 |2650459       |\n",
    "# |A005  |2650459       |\n",
    "# |V1031 |2650459       |\n",
    "# |posest|2650459       |\n",
    "# |B0018 |2650459       |\n",
    "# |A001  |2650459       |\n",
    "# |B0017 |2650459       |\n",
    "# |A001A |2650459       |\n",
    "# |V1013 |2650459  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dimensao = read_from_bigquery('SOR', 'tbx002_dimensao_geral')\n",
    "df_dimensao.createOrReplaceTempView(\"tbx002_dimensao_geral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_from_bigquery('SOR', 'tbx001_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário de mapeamento de renomeação de colunas\n",
    "col_rename_map = {\n",
    "    \"UF\": \"uf\",\n",
    "    \"Ano\": \"ano\",\n",
    "    \"V1013\": \"mes\",\n",
    "    \"V1012\": \"semana\",\n",
    "    \"A001B3\": \"ano_nascimento\",\n",
    "    \"A003\": \"sexo\",\n",
    "    \"A004\": \"cor_raca\",\n",
    "    \"V1023\": \"tipo_area\",\n",
    "    \"A005\": \"escolaridade\",\n",
    "    \"B0011\": \"teve_febre\",\n",
    "    \"B0014\": \"teve_dificuldade_respirar\",\n",
    "    \"B0015\": \"teve_dor_cabeca\",\n",
    "    \"B0019\": \"teve_fadiga\",\n",
    "    \"B00111\": \"teve_perda_cheiro\",\n",
    "    \"B002\": \"foi_posto_saude\",\n",
    "    \"B0031\": \"ficou_em_casa\",\n",
    "    \"B005\": \"ficou_internado\",\n",
    "    \"B009B\": \"resultado_covid\",\n",
    "    \"B007\": \"tem_plano_saude\",\n",
    "    \"C01011\": \"faixa_rendimento\",\n",
    "    \"F001\": \"situacao_domicilio\"\n",
    "}\n",
    "\n",
    "# Aplicar a renomeação das colunas e selecionar apenas as colunas renomeadas\n",
    "df = df.select([F.col(old_name).alias(new_name) for old_name, new_name in col_rename_map.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"tbx001_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# realizar o join entre os dataframes, dimensionando a tabela de fatos\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "T2.categoria_descricao AS uf,\n",
    "T1.ano,\n",
    "T1.mes,\n",
    "T1.semana,\n",
    "T1.ano_nascimento,\n",
    "T3.categoria_descricao AS sexo,\n",
    "T4.categoria_descricao AS cor_raca,\n",
    "T5.categoria_descricao AS tipo_area,\n",
    "T6.categoria_descricao AS escolaridade,\n",
    "T7.categoria_descricao AS teve_febre,\n",
    "T8.categoria_descricao AS teve_dificuldade_respirar,\n",
    "T9.categoria_descricao AS teve_dor_cabeca,\n",
    "T10.categoria_descricao AS teve_fadiga,\n",
    "T11.categoria_descricao AS teve_perda_cheiro,\n",
    "T12.categoria_descricao AS foi_posto_saude,\n",
    "T13.categoria_descricao AS ficou_em_casa,\n",
    "T14.categoria_descricao AS ficou_internado,\n",
    "T15.categoria_descricao AS resultado_covid,\n",
    "T16.categoria_descricao AS tem_plano_saude,\n",
    "T17.categoria_descricao AS faixa_rendimento,\n",
    "T18.categoria_descricao AS situacao_domicilio\n",
    "FROM tbx001_data T1\n",
    "LEFT JOIN tbx002_dimensao_geral T2 on T2.codigo_variavel = 'UF' and T1.uf = T2.categoria_tipo\n",
    "LEFT JOIN tbx002_dimensao_geral T3 on T3.codigo_variavel = 'A003' and T1.sexo = T3.categoria_tipo\n",
    "LEFT JOIN tbx002_dimensao_geral T4 on T4.codigo_variavel = 'A004' and T1.cor_raca = T4.categoria_tipo\n",
    "LEFT JOIN tbx002_dimensao_geral T5 on T5.codigo_variavel = 'V1023' and T1.tipo_area = T5.categoria_tipo\n",
    "LEFT JOIN tbx002_dimensao_geral T6 on T6.codigo_variavel = 'A005' and T1.escolaridade = T6.categoria_tipo\n",
    "LEFT JOIN tbx002_dimensao_geral T7 on T7.codigo_variavel = 'B0011' and T1.teve_febre = T7.categoria_tipo\n",
    "LEFT JOIN tbx002_dimensao_geral T8 on T8.codigo_variavel = 'B0014' and T1.teve_dificuldade_respirar = T8.categoria_tipo\n",
    "LEFT JOIN tbx002_dimensao_geral T9 on T9.codigo_variavel = 'B0015' and T1.teve_dor_cabeca = T9.categoria_tipo\n",
    "LEFT JOIN tbx002_dimensao_geral T10 on T10.codigo_variavel = 'B0019' and T1.teve_fadiga = T10.categoria_tipo\n",
    "LEFT JOIN tbx002_dimensao_geral T11 on T11.codigo_variavel = 'B00111' and T1.teve_perda_cheiro = T11.categoria_tipo\n",
    "LEFT JOIN tbx002_dimensao_geral T12 on T12.codigo_variavel = 'B002' and T1.foi_posto_saude = T12.categoria_tipo\n",
    "LEFT JOIN tbx002_dimensao_geral T13 on T13.codigo_variavel = 'B0031' and T1.ficou_em_casa = T13.categoria_tipo\n",
    "LEFT JOIN tbx002_dimensao_geral T14 on T14.codigo_variavel = 'B005' and T1.ficou_internado = T14.categoria_tipo\n",
    "LEFT JOIN tbx002_dimensao_geral T15 on T15.codigo_variavel = 'B009B' and T1.resultado_covid = T15.categoria_tipo\n",
    "LEFT JOIN tbx002_dimensao_geral T16 on T16.codigo_variavel = 'B007' and T1.tem_plano_saude = T16.categoria_tipo\n",
    "LEFT JOIN tbx002_dimensao_geral T17 on T17.codigo_variavel = 'C01011' and T1.faixa_rendimento = T17.categoria_tipo\n",
    "LEFT JOIN tbx002_dimensao_geral T18 on T18.codigo_variavel = 'F001' and T1.situacao_domicilio = T18.categoria_tipo\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar a consulta SQL\n",
    "df_joined = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uf: string (nullable = true)\n",
      " |-- ano: string (nullable = true)\n",
      " |-- mes: string (nullable = true)\n",
      " |-- semana: string (nullable = true)\n",
      " |-- ano_nascimento: string (nullable = true)\n",
      " |-- sexo: string (nullable = true)\n",
      " |-- cor_raca: string (nullable = true)\n",
      " |-- tipo_area: string (nullable = true)\n",
      " |-- escolaridade: string (nullable = true)\n",
      " |-- teve_febre: string (nullable = true)\n",
      " |-- teve_dificuldade_respirar: string (nullable = true)\n",
      " |-- teve_dor_cabeca: string (nullable = true)\n",
      " |-- teve_fadiga: string (nullable = true)\n",
      " |-- teve_perda_cheiro: string (nullable = true)\n",
      " |-- foi_posto_saude: string (nullable = true)\n",
      " |-- ficou_em_casa: string (nullable = true)\n",
      " |-- ficou_internado: string (nullable = true)\n",
      " |-- resultado_covid: string (nullable = true)\n",
      " |-- tem_plano_saude: string (nullable = true)\n",
      " |-- faixa_rendimento: string (nullable = true)\n",
      " |-- situacao_domicilio: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/28 15:51:44 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx001_data, parameters sent from Spark:|requiredColumns=[UF,Ano,V1013,V1012,A001B3,A003,A004,V1023,A005,B0011,B0014,B0015,B0019,B00111,B002,B0031,B005,B009B,B007,C01011,F001],|filters=[]\n",
      "24/09/28 15:51:45 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDNCc1IwVUtlRExMORoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:51:44.151Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:51:45.084Z\",\"readSessionPrepDuration\":414,\"readSessionCreationDuration\":519,\"readSessionDuration\":933}\n",
      "24/09/28 15:51:45 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 4 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDNCc1IwVUtlRExMORoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:51:45 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx001_data': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDNCc1IwVUtlRExMORoCdngaAnVo\n",
      "24/09/28 15:51:45 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,UF),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:51:45 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGhWUDdlZWh4SmZSYhoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:51:45.089Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:51:45.698Z\",\"readSessionPrepDuration\":249,\"readSessionCreationDuration\":360,\"readSessionDuration\":609}\n",
      "24/09/28 15:51:45 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGhWUDdlZWh4SmZSYhoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:51:45 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGhWUDdlZWh4SmZSYhoCdngaAnVo\n",
      "24/09/28 15:51:45 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,A003),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:51:46 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDB4X2I2b0M4bFZ4VRoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:51:45.718Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:51:46.356Z\",\"readSessionPrepDuration\":233,\"readSessionCreationDuration\":405,\"readSessionDuration\":638}\n",
      "24/09/28 15:51:46 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDB4X2I2b0M4bFZ4VRoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:51:46 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDB4X2I2b0M4bFZ4VRoCdngaAnVo\n",
      "24/09/28 15:51:46 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,A004),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:51:46 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDVJSVQ4Q3hweHBaMBoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:51:46.375Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:51:46.898Z\",\"readSessionPrepDuration\":224,\"readSessionCreationDuration\":299,\"readSessionDuration\":523}\n",
      "24/09/28 15:51:46 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDVJSVQ4Q3hweHBaMBoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:51:46 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDVJSVQ4Q3hweHBaMBoCdngaAnVo\n",
      "24/09/28 15:51:46 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,V1023),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:51:47 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGpJZXdlVHk1QjB2aBoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:51:46.904Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:51:47.483Z\",\"readSessionPrepDuration\":265,\"readSessionCreationDuration\":314,\"readSessionDuration\":579}\n",
      "24/09/28 15:51:47 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGpJZXdlVHk1QjB2aBoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:51:47 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGpJZXdlVHk1QjB2aBoCdngaAnVo\n",
      "24/09/28 15:51:47 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,A005),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:51:48 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJqdTlWR2hwZzYwORoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:51:47.495Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:51:48.020Z\",\"readSessionPrepDuration\":245,\"readSessionCreationDuration\":280,\"readSessionDuration\":525}\n",
      "24/09/28 15:51:48 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJqdTlWR2hwZzYwORoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:51:48 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJqdTlWR2hwZzYwORoCdngaAnVo\n",
      "24/09/28 15:51:48 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B0011),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:51:48 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHQ1dUk0TUQ2d29yRxoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:51:48.033Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:51:48.626Z\",\"readSessionPrepDuration\":243,\"readSessionCreationDuration\":350,\"readSessionDuration\":593}\n",
      "24/09/28 15:51:48 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHQ1dUk0TUQ2d29yRxoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:51:48 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHQ1dUk0TUQ2d29yRxoCdngaAnVo\n",
      "24/09/28 15:51:48 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B0014),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:51:49 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFhoVFJPR3o1Z09ZYRoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:51:48.631Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:51:49.156Z\",\"readSessionPrepDuration\":210,\"readSessionCreationDuration\":315,\"readSessionDuration\":525}\n",
      "24/09/28 15:51:49 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFhoVFJPR3o1Z09ZYRoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:51:49 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFhoVFJPR3o1Z09ZYRoCdngaAnVo\n",
      "24/09/28 15:51:49 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B0015),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:51:49 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHpkWjMzRWRzekNodRoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:51:49.160Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:51:49.662Z\",\"readSessionPrepDuration\":202,\"readSessionCreationDuration\":300,\"readSessionDuration\":502}\n",
      "24/09/28 15:51:49 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHpkWjMzRWRzekNodRoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:51:49 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHpkWjMzRWRzekNodRoCdngaAnVo\n",
      "24/09/28 15:51:49 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B0019),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:51:50 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEd5M2JvUlVnTkQxVBoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:51:49.668Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:51:50.255Z\",\"readSessionPrepDuration\":238,\"readSessionCreationDuration\":349,\"readSessionDuration\":587}\n",
      "24/09/28 15:51:50 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEd5M2JvUlVnTkQxVBoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:51:50 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEd5M2JvUlVnTkQxVBoCdngaAnVo\n",
      "24/09/28 15:51:50 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B00111),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:51:50 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDG8xMTZaQWJwZ0JzWRoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:51:50.258Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:51:50.792Z\",\"readSessionPrepDuration\":226,\"readSessionCreationDuration\":308,\"readSessionDuration\":534}\n",
      "24/09/28 15:51:50 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDG8xMTZaQWJwZ0JzWRoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:51:50 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDG8xMTZaQWJwZ0JzWRoCdngaAnVo\n",
      "24/09/28 15:51:50 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B002),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:51:51 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlaYUZHYk5yNEVYShoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:51:50.801Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:51:51.350Z\",\"readSessionPrepDuration\":245,\"readSessionCreationDuration\":304,\"readSessionDuration\":549}\n",
      "24/09/28 15:51:51 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlaYUZHYk5yNEVYShoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:51:51 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlaYUZHYk5yNEVYShoCdngaAnVo\n",
      "24/09/28 15:51:51 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B0031),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:51:51 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlJaGZxV25QaVVMZBoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:51:51.368Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:51:51.915Z\",\"readSessionPrepDuration\":222,\"readSessionCreationDuration\":325,\"readSessionDuration\":547}\n",
      "24/09/28 15:51:51 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlJaGZxV25QaVVMZBoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:51:51 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlJaGZxV25QaVVMZBoCdngaAnVo\n",
      "24/09/28 15:51:51 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B005),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:51:52 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDUtQVVpc0N5YWxsZxoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:51:51.924Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:51:52.410Z\",\"readSessionPrepDuration\":203,\"readSessionCreationDuration\":283,\"readSessionDuration\":486}\n",
      "24/09/28 15:51:52 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDUtQVVpc0N5YWxsZxoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:51:52 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDUtQVVpc0N5YWxsZxoCdngaAnVo\n",
      "24/09/28 15:51:52 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B009B),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:51:52 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEhVTUZ6aEhRWlo2TRoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:51:52.418Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:51:52.926Z\",\"readSessionPrepDuration\":195,\"readSessionCreationDuration\":313,\"readSessionDuration\":508}\n",
      "24/09/28 15:51:52 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEhVTUZ6aEhRWlo2TRoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:51:52 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEhVTUZ6aEhRWlo2TRoCdngaAnVo\n",
      "24/09/28 15:51:52 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B007),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:51:53 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJ1VXpQcGFKWUlXShoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:51:52.932Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:51:53.432Z\",\"readSessionPrepDuration\":209,\"readSessionCreationDuration\":291,\"readSessionDuration\":500}\n",
      "24/09/28 15:51:53 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJ1VXpQcGFKWUlXShoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:51:53 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJ1VXpQcGFKWUlXShoCdngaAnVo\n",
      "24/09/28 15:51:53 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,C01011),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:51:53 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDZDMjdCZmkxa2VaeBoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:51:53.444Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:51:53.975Z\",\"readSessionPrepDuration\":238,\"readSessionCreationDuration\":293,\"readSessionDuration\":531}\n",
      "24/09/28 15:51:53 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDZDMjdCZmkxa2VaeBoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:51:53 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDZDMjdCZmkxa2VaeBoCdngaAnVo\n",
      "24/09/28 15:51:53 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,F001),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:51:54 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDExJcV9vWTdqQUthQRoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:51:53.977Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:51:54.487Z\",\"readSessionPrepDuration\":219,\"readSessionCreationDuration\":291,\"readSessionDuration\":510}\n",
      "24/09/28 15:51:54 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDExJcV9vWTdqQUthQRoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:51:54 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDExJcV9vWTdqQUthQRoCdngaAnVo\n",
      "24/09/28 15:51:54 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:54 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:54 INFO DAGScheduler: Got job 219 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:51:54 INFO DAGScheduler: Final stage: ResultStage 366 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:51:54 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:51:54 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:51:54 INFO DAGScheduler: Submitting ResultStage 366 (MapPartitionsRDD[1000] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:51:54 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:54 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:54 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:54 INFO MemoryStore: Block broadcast_274 stored as values in memory (estimated size 20.5 KiB, free 222.0 MiB)\n",
      "24/09/28 15:51:54 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:54 INFO MemoryStore: Block broadcast_274_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 221.9 MiB)\n",
      "24/09/28 15:51:54 INFO BlockManagerInfo: Added broadcast_274_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:51:54 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:54 INFO SparkContext: Created broadcast 274 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:51:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 366 (MapPartitionsRDD[1000] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:51:54 INFO TaskSchedulerImpl: Adding task set 366.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:51:54 INFO DAGScheduler: Got job 220 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:51:54 INFO DAGScheduler: Final stage: ResultStage 367 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:51:54 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:51:54 INFO CodeGenerator: Code generated in 77.256058 ms\n",
      "24/09/28 15:51:54 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:51:54 INFO TaskSetManager: Starting task 0.0 in stage 366.0 (TID 372) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:51:54 INFO DAGScheduler: Submitting ResultStage 367 (MapPartitionsRDD[1002] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:51:54 INFO MemoryStore: Block broadcast_275 stored as values in memory (estimated size 20.5 KiB, free 221.9 MiB)\n",
      "24/09/28 15:51:54 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:54 INFO CodeGenerator: Code generated in 26.304135 ms\n",
      "24/09/28 15:51:55 INFO Executor: Running task 0.0 in stage 366.0 (TID 372)\n",
      "24/09/28 15:51:55 INFO MemoryStore: Block broadcast_275_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 221.9 MiB)\n",
      "24/09/28 15:51:55 INFO BlockManagerInfo: Added broadcast_275_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:51:55 INFO SparkContext: Created broadcast 275 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 367 (MapPartitionsRDD[1002] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:51:55 INFO TaskSchedulerImpl: Adding task set 367.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:51:55 INFO CodeGenerator: Code generated in 165.480182 ms\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Got job 221 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Final stage: ResultStage 368 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:51:55 INFO TaskSetManager: Starting task 0.0 in stage 367.0 (TID 373) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:51:55 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:55 INFO CodeGenerator: Code generated in 76.606359 ms\n",
      "24/09/28 15:51:55 INFO CodeGenerator: Code generated in 116.00399 ms\n",
      "24/09/28 15:51:55 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:55 INFO CodeGenerator: Code generated in 253.225038 ms\n",
      "24/09/28 15:51:55 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:55 INFO CodeGenerator: Code generated in 136.02096 ms\n",
      "24/09/28 15:51:55 INFO CodeGenerator: Code generated in 268.097494 ms\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Submitting ResultStage 368 (MapPartitionsRDD[1008] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:51:55 INFO Executor: Running task 0.0 in stage 367.0 (TID 373)\n",
      "24/09/28 15:51:55 INFO MemoryStore: Block broadcast_276 stored as values in memory (estimated size 20.5 KiB, free 225.9 MiB)\n",
      "24/09/28 15:51:55 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:55 INFO MemoryStore: Block broadcast_276_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 225.9 MiB)\n",
      "24/09/28 15:51:55 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:55 INFO BlockManagerInfo: Added broadcast_276_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:51:55 INFO SparkContext: Created broadcast 276 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 368 (MapPartitionsRDD[1008] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:51:55 INFO TaskSchedulerImpl: Adding task set 368.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Got job 222 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Final stage: ResultStage 369 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Submitting ResultStage 369 (MapPartitionsRDD[1004] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:51:55 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:55 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:55 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:55 INFO TaskSetManager: Starting task 0.0 in stage 368.0 (TID 374) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:51:55 INFO MemoryStore: Block broadcast_277 stored as values in memory (estimated size 20.5 KiB, free 225.9 MiB)\n",
      "24/09/28 15:51:55 INFO MemoryStore: Block broadcast_277_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 225.9 MiB)\n",
      "24/09/28 15:51:55 INFO BlockManagerInfo: Added broadcast_277_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:51:55 INFO SparkContext: Created broadcast 277 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 369 (MapPartitionsRDD[1004] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:51:55 INFO TaskSchedulerImpl: Adding task set 369.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Got job 223 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Final stage: ResultStage 370 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:51:55 INFO TaskSetManager: Starting task 0.0 in stage 369.0 (TID 375) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:51:55 INFO BlockManagerInfo: Removed broadcast_262_piece0 on 10.0.2.15:38963 in memory (size: 223.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Submitting ResultStage 370 (MapPartitionsRDD[1012] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:51:55 INFO Executor: Running task 0.0 in stage 368.0 (TID 374)\n",
      "24/09/28 15:51:55 INFO Executor: Running task 0.0 in stage 369.0 (TID 375)\n",
      "24/09/28 15:51:55 INFO CodeGenerator: Code generated in 233.569662 ms\n",
      "24/09/28 15:51:55 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:55 INFO MemoryStore: Block broadcast_278 stored as values in memory (estimated size 20.5 KiB, free 225.8 MiB)\n",
      "24/09/28 15:51:55 INFO MemoryStore: Block broadcast_278_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 229.8 MiB)\n",
      "24/09/28 15:51:55 INFO BlockManagerInfo: Added broadcast_278_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:51:55 INFO SparkContext: Created broadcast 278 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 370 (MapPartitionsRDD[1012] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:51:55 INFO TaskSchedulerImpl: Adding task set 370.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Got job 224 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Final stage: ResultStage 371 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Submitting ResultStage 371 (MapPartitionsRDD[1007] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:51:55 INFO BlockManagerInfo: Removed broadcast_258_piece0 on 10.0.2.15:38963 in memory (size: 189.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:51:55 INFO MemoryStore: Block broadcast_279 stored as values in memory (estimated size 20.5 KiB, free 229.8 MiB)\n",
      "24/09/28 15:51:55 INFO MemoryStore: Block broadcast_279_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 229.8 MiB)\n",
      "24/09/28 15:51:55 INFO BlockManagerInfo: Added broadcast_279_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:51:55 INFO SparkContext: Created broadcast 279 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 371 (MapPartitionsRDD[1007] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:51:55 INFO TaskSchedulerImpl: Adding task set 371.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:51:55 INFO BlockManagerInfo: Removed broadcast_266_piece0 on 10.0.2.15:38963 in memory (size: 223.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Got job 225 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Final stage: ResultStage 372 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Submitting ResultStage 372 (MapPartitionsRDD[1014] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:51:55 INFO MemoryStore: Block broadcast_280 stored as values in memory (estimated size 20.5 KiB, free 233.8 MiB)\n",
      "24/09/28 15:51:55 INFO MemoryStore: Block broadcast_280_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 233.8 MiB)\n",
      "24/09/28 15:51:55 INFO BlockManagerInfo: Added broadcast_280_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:51:55 INFO SparkContext: Created broadcast 280 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 372 (MapPartitionsRDD[1014] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:51:55 INFO TaskSchedulerImpl: Adding task set 372.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Got job 226 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Final stage: ResultStage 373 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:51:55 INFO BlockManagerInfo: Removed broadcast_269_piece0 on 10.0.2.15:38963 in memory (size: 223.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Submitting ResultStage 373 (MapPartitionsRDD[1010] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:51:55 INFO MemoryStore: Block broadcast_281 stored as values in memory (estimated size 20.5 KiB, free 237.8 MiB)\n",
      "24/09/28 15:51:55 INFO MemoryStore: Block broadcast_281_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 237.7 MiB)\n",
      "24/09/28 15:51:55 INFO BlockManagerInfo: Added broadcast_281_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:51:55 INFO SparkContext: Created broadcast 281 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 373 (MapPartitionsRDD[1010] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:51:55 INFO TaskSchedulerImpl: Adding task set 373.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Got job 227 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Final stage: ResultStage 374 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:51:55 INFO DAGScheduler: Submitting ResultStage 374 (MapPartitionsRDD[1016] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:51:56 INFO MemoryStore: Block broadcast_282 stored as values in memory (estimated size 20.5 KiB, free 241.7 MiB)\n",
      "24/09/28 15:51:56 INFO MemoryStore: Block broadcast_282_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 241.7 MiB)\n",
      "24/09/28 15:51:56 INFO BlockManagerInfo: Added broadcast_282_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:51:56 INFO BlockManagerInfo: Removed broadcast_272_piece0 on 10.0.2.15:38963 in memory (size: 277.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:51:56 INFO SparkContext: Created broadcast 282 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 374 (MapPartitionsRDD[1016] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:51:56 INFO TaskSchedulerImpl: Adding task set 374.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Got job 228 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Final stage: ResultStage 375 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Submitting ResultStage 375 (MapPartitionsRDD[1020] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:51:56 INFO MemoryStore: Block broadcast_283 stored as values in memory (estimated size 20.5 KiB, free 241.7 MiB)\n",
      "24/09/28 15:51:56 INFO MemoryStore: Block broadcast_283_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 241.7 MiB)\n",
      "24/09/28 15:51:56 INFO BlockManagerInfo: Added broadcast_283_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:51:56 INFO BlockManagerInfo: Removed broadcast_263_piece0 on 10.0.2.15:38963 in memory (size: 206.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:51:56 INFO SparkContext: Created broadcast 283 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 375 (MapPartitionsRDD[1020] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:51:56 INFO TaskSchedulerImpl: Adding task set 375.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Got job 229 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Final stage: ResultStage 376 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Submitting ResultStage 376 (MapPartitionsRDD[1022] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:51:56 INFO MemoryStore: Block broadcast_284 stored as values in memory (estimated size 20.5 KiB, free 245.7 MiB)\n",
      "24/09/28 15:51:56 INFO MemoryStore: Block broadcast_284_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 245.7 MiB)\n",
      "24/09/28 15:51:56 INFO BlockManagerInfo: Added broadcast_284_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:51:56 INFO BlockManagerInfo: Removed broadcast_261_piece0 on 10.0.2.15:38963 in memory (size: 223.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:51:56 INFO SparkContext: Created broadcast 284 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 376 (MapPartitionsRDD[1022] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:51:56 INFO TaskSchedulerImpl: Adding task set 376.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Got job 230 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Final stage: ResultStage 377 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Submitting ResultStage 377 (MapPartitionsRDD[1024] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:51:56 INFO MemoryStore: Block broadcast_285 stored as values in memory (estimated size 20.5 KiB, free 249.6 MiB)\n",
      "24/09/28 15:51:56 INFO BlockManagerInfo: Removed broadcast_271_piece0 on 10.0.2.15:38963 in memory (size: 206.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:51:56 INFO MemoryStore: Block broadcast_285_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 253.6 MiB)\n",
      "24/09/28 15:51:56 INFO BlockManagerInfo: Added broadcast_285_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:51:56 INFO SparkContext: Created broadcast 285 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 377 (MapPartitionsRDD[1024] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:51:56 INFO TaskSchedulerImpl: Adding task set 377.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Got job 231 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Final stage: ResultStage 378 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Submitting ResultStage 378 (MapPartitionsRDD[1018] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:51:56 INFO MemoryStore: Block broadcast_286 stored as values in memory (estimated size 20.5 KiB, free 253.6 MiB)\n",
      "24/09/28 15:51:56 INFO MemoryStore: Block broadcast_286_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 253.6 MiB)\n",
      "24/09/28 15:51:56 INFO BlockManagerInfo: Added broadcast_286_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:51:56 INFO SparkContext: Created broadcast 286 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 378 (MapPartitionsRDD[1018] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:51:56 INFO TaskSchedulerImpl: Adding task set 378.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:51:56 INFO BlockManagerInfo: Removed broadcast_259_piece0 on 10.0.2.15:38963 in memory (size: 294.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Got job 233 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Final stage: ResultStage 379 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Submitting ResultStage 379 (MapPartitionsRDD[1028] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:51:56 INFO MemoryStore: Block broadcast_287 stored as values in memory (estimated size 20.5 KiB, free 257.6 MiB)\n",
      "24/09/28 15:51:56 INFO MemoryStore: Block broadcast_287_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 257.6 MiB)\n",
      "24/09/28 15:51:56 INFO BlockManagerInfo: Added broadcast_287_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:51:56 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGhWUDdlZWh4SmZSYhoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:51:55.082Z\",\"Ended\":\"2024-09-28T18:51:56.346Z\",\"Parse Timings\":\"Average: PT0.000219554S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT1.108029859S Samples: 1\",\"Bytes/s\":700,\"Rows/s\":0,\"Bytes\":776,\"Rows\":27,\"I/O time\":1108}\n",
      "24/09/28 15:51:56 INFO SparkContext: Created broadcast 287 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 379 (MapPartitionsRDD[1028] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:51:56 INFO TaskSchedulerImpl: Adding task set 379.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:51:56 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGpJZXdlVHk1QjB2aBoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:51:55.456Z\",\"Ended\":\"2024-09-28T18:51:56.348Z\",\"Parse Timings\":\"Average: PT0.077485845S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.796186246S Samples: 1\",\"Bytes/s\":643,\"Rows/s\":51,\"Bytes\":512,\"Rows\":4,\"I/O time\":796}\n",
      "24/09/28 15:51:56 INFO Executor: Finished task 0.0 in stage 366.0 (TID 372). 2060 bytes result sent to driver\n",
      "24/09/28 15:51:56 INFO Executor: Finished task 0.0 in stage 369.0 (TID 375). 1745 bytes result sent to driver\n",
      "24/09/28 15:51:56 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDVJSVQ4Q3hweHBaMBoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:51:55.459Z\",\"Ended\":\"2024-09-28T18:51:56.388Z\",\"Parse Timings\":\"Average: PT0.000302042S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.82528393S Samples: 1\",\"Bytes/s\":407,\"Rows/s\":0,\"Bytes\":336,\"Rows\":6,\"I/O time\":825}\n",
      "24/09/28 15:51:56 INFO Executor: Finished task 0.0 in stage 368.0 (TID 374). 1602 bytes result sent to driver\n",
      "24/09/28 15:51:56 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDB4X2I2b0M4bFZ4VRoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:51:55.221Z\",\"Ended\":\"2024-09-28T18:51:56.371Z\",\"Parse Timings\":\"Average: PT0.001385221S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.915516912S Samples: 1\",\"Bytes/s\":306,\"Rows/s\":2000,\"Bytes\":280,\"Rows\":2,\"I/O time\":915}\n",
      "24/09/28 15:51:56 INFO TaskSetManager: Starting task 0.0 in stage 370.0 (TID 376) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:51:56 INFO TaskSetManager: Starting task 0.0 in stage 371.0 (TID 377) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:51:56 INFO Executor: Finished task 0.0 in stage 367.0 (TID 373). 1503 bytes result sent to driver\n",
      "24/09/28 15:51:56 INFO Executor: Running task 0.0 in stage 370.0 (TID 376)\n",
      "24/09/28 15:51:56 INFO Executor: Running task 0.0 in stage 371.0 (TID 377)\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Got job 232 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Final stage: ResultStage 380 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Submitting ResultStage 380 (MapPartitionsRDD[1030] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:51:56 INFO MemoryStore: Block broadcast_288 stored as values in memory (estimated size 20.5 KiB, free 257.5 MiB)\n",
      "24/09/28 15:51:56 INFO MemoryStore: Block broadcast_288_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 257.5 MiB)\n",
      "24/09/28 15:51:56 INFO BlockManagerInfo: Added broadcast_288_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:51:56 INFO SparkContext: Created broadcast 288 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 380 (MapPartitionsRDD[1030] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:51:56 INFO TaskSchedulerImpl: Adding task set 380.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:51:56 INFO TaskSetManager: Starting task 0.0 in stage 372.0 (TID 378) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:51:56 INFO Executor: Running task 0.0 in stage 372.0 (TID 378)\n",
      "24/09/28 15:51:56 INFO TaskSetManager: Starting task 0.0 in stage 373.0 (TID 379) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:51:56 INFO Executor: Running task 0.0 in stage 373.0 (TID 379)\n",
      "24/09/28 15:51:56 INFO TaskSetManager: Finished task 0.0 in stage 367.0 (TID 373) in 1389 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:51:56 INFO TaskSchedulerImpl: Removed TaskSet 367.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:51:56 INFO TaskSetManager: Finished task 0.0 in stage 368.0 (TID 374) in 1236 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:51:56 INFO TaskSchedulerImpl: Removed TaskSet 368.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:51:56 INFO TaskSetManager: Finished task 0.0 in stage 369.0 (TID 375) in 1129 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:51:56 INFO TaskSchedulerImpl: Removed TaskSet 369.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:51:56 INFO TaskSetManager: Finished task 0.0 in stage 366.0 (TID 372) in 1556 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:51:56 INFO TaskSchedulerImpl: Removed TaskSet 366.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:51:56 INFO DAGScheduler: Got job 234 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Final stage: ResultStage 381 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Submitting ResultStage 381 (MapPartitionsRDD[1026] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:51:56 INFO BlockManagerInfo: Removed broadcast_268_piece0 on 10.0.2.15:38963 in memory (size: 223.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:51:56 INFO MemoryStore: Block broadcast_289 stored as values in memory (estimated size 20.5 KiB, free 261.5 MiB)\n",
      "24/09/28 15:51:56 INFO MemoryStore: Block broadcast_289_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 261.5 MiB)\n",
      "24/09/28 15:51:56 INFO BlockManagerInfo: Added broadcast_289_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.0 MiB)\n",
      "24/09/28 15:51:56 INFO SparkContext: Created broadcast 289 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 381 (MapPartitionsRDD[1026] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:51:56 INFO TaskSchedulerImpl: Adding task set 381.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Got job 235 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Final stage: ResultStage 382 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Submitting ResultStage 382 (MapPartitionsRDD[1032] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:51:56 INFO MemoryStore: Block broadcast_290 stored as values in memory (estimated size 20.5 KiB, free 261.5 MiB)\n",
      "24/09/28 15:51:56 INFO MemoryStore: Block broadcast_290_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 261.5 MiB)\n",
      "24/09/28 15:51:56 INFO BlockManagerInfo: Removed broadcast_260_piece0 on 10.0.2.15:38963 in memory (size: 223.0 B, free: 366.0 MiB)\n",
      "24/09/28 15:51:56 INFO BlockManagerInfo: Added broadcast_290_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.0 MiB)\n",
      "24/09/28 15:51:56 INFO SparkContext: Created broadcast 290 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 382 (MapPartitionsRDD[1032] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:51:56 INFO TaskSchedulerImpl: Adding task set 382.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:51:56 INFO DAGScheduler: ResultStage 367 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1,897 s\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Job 220 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:51:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 367: Stage finished\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Job 220 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2,061449 s\n",
      "24/09/28 15:51:56 INFO DAGScheduler: ResultStage 368 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1,651 s\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Job 221 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:51:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 368: Stage finished\n",
      "24/09/28 15:51:56 INFO DAGScheduler: ResultStage 369 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1,635 s\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Job 222 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:51:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 369: Stage finished\n",
      "24/09/28 15:51:56 INFO MemoryStore: Block broadcast_291 stored as values in memory (estimated size 4.0 MiB, free 261.5 MiB)\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Job 221 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2,058279 s\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Job 222 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2,065342 s\n",
      "24/09/28 15:51:56 INFO MemoryStore: Block broadcast_291_piece0 stored as bytes in memory (estimated size 215.0 B, free 261.5 MiB)\n",
      "24/09/28 15:51:56 INFO DAGScheduler: ResultStage 366 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,106 s\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Job 219 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:51:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 366: Stage finished\n",
      "24/09/28 15:51:56 INFO DAGScheduler: Job 219 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2,238989 s\n",
      "24/09/28 15:51:56 INFO MemoryStore: Block broadcast_292 stored as values in memory (estimated size 4.0 MiB, free 257.5 MiB)\n",
      "24/09/28 15:51:56 INFO MemoryStore: Block broadcast_293 stored as values in memory (estimated size 4.0 MiB, free 253.5 MiB)\n",
      "24/09/28 15:51:56 INFO MemoryStore: Block broadcast_292_piece0 stored as bytes in memory (estimated size 446.0 B, free 253.5 MiB)\n",
      "24/09/28 15:51:56 INFO MemoryStore: Block broadcast_293_piece0 stored as bytes in memory (estimated size 359.0 B, free 253.5 MiB)\n",
      "24/09/28 15:51:56 INFO BlockManagerInfo: Added broadcast_291_piece0 in memory on 10.0.2.15:38963 (size: 215.0 B, free: 366.0 MiB)\n",
      "24/09/28 15:51:56 INFO SparkContext: Created broadcast 291 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:56 INFO BlockManagerInfo: Added broadcast_292_piece0 in memory on 10.0.2.15:38963 (size: 446.0 B, free: 366.0 MiB)\n",
      "24/09/28 15:51:56 INFO SparkContext: Created broadcast 292 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:56 INFO BlockManagerInfo: Added broadcast_293_piece0 in memory on 10.0.2.15:38963 (size: 359.0 B, free: 366.0 MiB)\n",
      "24/09/28 15:51:56 INFO SparkContext: Created broadcast 293 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:57 INFO BlockManagerInfo: Removed broadcast_256_piece0 on 10.0.2.15:38963 in memory (size: 260.0 B, free: 366.0 MiB)\n",
      "24/09/28 15:51:57 INFO MemoryStore: Block broadcast_294 stored as values in memory (estimated size 4.0 MiB, free 253.5 MiB)\n",
      "24/09/28 15:51:57 INFO MemoryStore: Block broadcast_294_piece0 stored as bytes in memory (estimated size 1038.0 B, free 253.5 MiB)\n",
      "24/09/28 15:51:57 INFO BlockManagerInfo: Added broadcast_294_piece0 in memory on 10.0.2.15:38963 (size: 1038.0 B, free: 366.0 MiB)\n",
      "24/09/28 15:51:57 INFO SparkContext: Created broadcast 294 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:57 INFO BlockManagerInfo: Removed broadcast_267_piece0 on 10.0.2.15:38963 in memory (size: 243.0 B, free: 366.0 MiB)\n",
      "24/09/28 15:51:57 INFO BlockManagerInfo: Removed broadcast_264_piece0 on 10.0.2.15:38963 in memory (size: 206.0 B, free: 366.0 MiB)\n",
      "24/09/28 15:51:57 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx001_data, parameters sent from Spark:|requiredColumns=[UF,Ano,V1013,V1012,A001B3,A003,A004,V1023,A005,B0011,B0014,B0015,B0019,B00111,B002,B0031,B005,B009B,B007,C01011,F001],|filters=[]\n",
      "24/09/28 15:51:57 INFO BlockManagerInfo: Removed broadcast_270_piece0 on 10.0.2.15:38963 in memory (size: 291.0 B, free: 366.0 MiB)\n",
      "24/09/28 15:51:57 INFO BlockManagerInfo: Removed broadcast_257_piece0 on 10.0.2.15:38963 in memory (size: 1038.0 B, free: 366.0 MiB)\n",
      "24/09/28 15:51:57 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHpkWjMzRWRzekNodRoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:51:56.439Z\",\"Ended\":\"2024-09-28T18:51:57.220Z\",\"Parse Timings\":\"Average: PT0.00051538S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.690989635S Samples: 1\",\"Bytes/s\":452,\"Rows/s\":0,\"Bytes\":312,\"Rows\":4,\"I/O time\":690}\n",
      "24/09/28 15:51:57 INFO Executor: Finished task 0.0 in stage 370.0 (TID 376). 1600 bytes result sent to driver\n",
      "24/09/28 15:51:57 INFO BlockManagerInfo: Removed broadcast_273_piece0 on 10.0.2.15:38963 in memory (size: 17.0 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:51:57 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHQ1dUk0TUQ2d29yRxoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:51:56.611Z\",\"Ended\":\"2024-09-28T18:51:57.276Z\",\"Parse Timings\":\"Average: PT0.000327284S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.612452865S Samples: 1\",\"Bytes/s\":509,\"Rows/s\":0,\"Bytes\":312,\"Rows\":4,\"I/O time\":612}\n",
      "24/09/28 15:51:57 INFO Executor: Finished task 0.0 in stage 373.0 (TID 379). 1557 bytes result sent to driver\n",
      "24/09/28 15:51:57 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFhoVFJPR3o1Z09ZYRoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:51:56.464Z\",\"Ended\":\"2024-09-28T18:51:57.270Z\",\"Parse Timings\":\"Average: PT0.000159067S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.672573561S Samples: 1\",\"Bytes/s\":464,\"Rows/s\":0,\"Bytes\":312,\"Rows\":4,\"I/O time\":672}\n",
      "24/09/28 15:51:57 INFO TaskSetManager: Starting task 0.0 in stage 374.0 (TID 380) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:51:57 INFO TaskSetManager: Finished task 0.0 in stage 370.0 (TID 376) in 916 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:51:57 INFO TaskSchedulerImpl: Removed TaskSet 370.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:51:57 INFO DAGScheduler: ResultStage 370 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1,918 s\n",
      "24/09/28 15:51:57 INFO DAGScheduler: Job 223 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:51:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 370: Stage finished\n",
      "24/09/28 15:51:57 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJqdTlWR2hwZzYwORoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:51:56.484Z\",\"Ended\":\"2024-09-28T18:51:57.326Z\",\"Parse Timings\":\"Average: PT0.000142606S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.668512544S Samples: 1\",\"Bytes/s\":718,\"Rows/s\":0,\"Bytes\":480,\"Rows\":8,\"I/O time\":668}\n",
      "24/09/28 15:51:57 INFO DAGScheduler: Job 223 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2,483213 s\n",
      "24/09/28 15:51:57 INFO Executor: Finished task 0.0 in stage 372.0 (TID 378). 1643 bytes result sent to driver\n",
      "24/09/28 15:51:57 INFO Executor: Finished task 0.0 in stage 371.0 (TID 377). 1748 bytes result sent to driver\n",
      "24/09/28 15:51:57 INFO TaskSetManager: Starting task 0.0 in stage 375.0 (TID 381) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:51:57 INFO TaskSetManager: Finished task 0.0 in stage 373.0 (TID 379) in 875 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:51:57 INFO TaskSchedulerImpl: Removed TaskSet 373.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:51:57 INFO DAGScheduler: ResultStage 373 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1,381 s\n",
      "24/09/28 15:51:57 INFO DAGScheduler: Job 226 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:51:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 373: Stage finished\n",
      "24/09/28 15:51:57 INFO Executor: Running task 0.0 in stage 374.0 (TID 380)\n",
      "24/09/28 15:51:57 INFO Executor: Running task 0.0 in stage 375.0 (TID 381)\n",
      "24/09/28 15:51:57 INFO TaskSetManager: Starting task 0.0 in stage 376.0 (TID 382) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:51:57 INFO Executor: Running task 0.0 in stage 376.0 (TID 382)\n",
      "24/09/28 15:51:57 INFO TaskSetManager: Starting task 0.0 in stage 377.0 (TID 383) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:51:57 INFO DAGScheduler: Job 226 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2,440568 s\n",
      "24/09/28 15:51:57 INFO TaskSetManager: Finished task 0.0 in stage 372.0 (TID 378) in 929 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:51:57 INFO TaskSchedulerImpl: Removed TaskSet 372.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:51:57 INFO DAGScheduler: ResultStage 372 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1,552 s\n",
      "24/09/28 15:51:57 INFO DAGScheduler: Job 225 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:51:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 372: Stage finished\n",
      "24/09/28 15:51:57 INFO Executor: Running task 0.0 in stage 377.0 (TID 383)\n",
      "24/09/28 15:51:57 INFO MemoryStore: Block broadcast_295 stored as values in memory (estimated size 4.0 MiB, free 265.6 MiB)\n",
      "24/09/28 15:51:57 INFO TaskSetManager: Finished task 0.0 in stage 371.0 (TID 377) in 982 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:51:57 INFO TaskSchedulerImpl: Removed TaskSet 371.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:51:57 INFO BlockManagerInfo: Removed broadcast_265_piece0 on 10.0.2.15:38963 in memory (size: 223.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:51:57 INFO DAGScheduler: ResultStage 371 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1,720 s\n",
      "24/09/28 15:51:57 INFO DAGScheduler: Job 224 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:51:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 371: Stage finished\n",
      "24/09/28 15:51:57 INFO DAGScheduler: Job 225 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2,501604 s\n",
      "24/09/28 15:51:57 INFO MemoryStore: Block broadcast_295_piece0 stored as bytes in memory (estimated size 295.0 B, free 269.6 MiB)\n",
      "24/09/28 15:51:57 INFO BlockManagerInfo: Added broadcast_295_piece0 in memory on 10.0.2.15:38963 (size: 295.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:51:57 INFO SparkContext: Created broadcast 295 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:57 INFO DAGScheduler: Job 224 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2,528141 s\n",
      "24/09/28 15:51:57 INFO MemoryStore: Block broadcast_296 stored as values in memory (estimated size 4.0 MiB, free 265.6 MiB)\n",
      "24/09/28 15:51:57 INFO MemoryStore: Block broadcast_297 stored as values in memory (estimated size 4.0 MiB, free 261.6 MiB)\n",
      "24/09/28 15:51:57 INFO MemoryStore: Block broadcast_296_piece0 stored as bytes in memory (estimated size 295.0 B, free 261.6 MiB)\n",
      "24/09/28 15:51:57 INFO MemoryStore: Block broadcast_298 stored as values in memory (estimated size 4.0 MiB, free 257.6 MiB)\n",
      "24/09/28 15:51:57 INFO CodeGenerator: Code generated in 40.078482 ms\n",
      "24/09/28 15:51:57 INFO BlockManagerInfo: Added broadcast_296_piece0 in memory on 10.0.2.15:38963 (size: 295.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:51:57 INFO MemoryStore: Block broadcast_298_piece0 stored as bytes in memory (estimated size 492.0 B, free 257.6 MiB)\n",
      "24/09/28 15:51:57 INFO MemoryStore: Block broadcast_297_piece0 stored as bytes in memory (estimated size 295.0 B, free 257.5 MiB)\n",
      "24/09/28 15:51:57 INFO SparkContext: Created broadcast 296 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:57 INFO BlockManagerInfo: Added broadcast_298_piece0 in memory on 10.0.2.15:38963 (size: 492.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:51:57 INFO SparkContext: Created broadcast 298 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:57 INFO BlockManagerInfo: Added broadcast_297_piece0 in memory on 10.0.2.15:38963 (size: 295.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:51:57 INFO SparkContext: Created broadcast 297 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:57 INFO CodeGenerator: Code generated in 156.178084 ms\n",
      "24/09/28 15:51:57 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHVTOVRsZFU5UzZyOBoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:51:57.129Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:51:57.887Z\",\"readSessionPrepDuration\":235,\"readSessionCreationDuration\":523,\"readSessionDuration\":758}\n",
      "24/09/28 15:51:57 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 4 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHVTOVRsZFU5UzZyOBoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:51:57 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx001_data': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHVTOVRsZFU5UzZyOBoCdngaAnVo\n",
      "24/09/28 15:51:57 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEd5M2JvUlVnTkQxVBoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:51:57.371Z\",\"Ended\":\"2024-09-28T18:51:57.923Z\",\"Parse Timings\":\"Average: PT0.000243904S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.395533919S Samples: 1\",\"Bytes/s\":789,\"Rows/s\":0,\"Bytes\":312,\"Rows\":4,\"I/O time\":395}\n",
      "24/09/28 15:51:57 INFO Executor: Finished task 0.0 in stage 374.0 (TID 380). 1557 bytes result sent to driver\n",
      "24/09/28 15:51:57 INFO TaskSetManager: Starting task 0.0 in stage 378.0 (TID 384) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:51:57 INFO CodeGenerator: Code generated in 335.637259 ms\n",
      "24/09/28 15:51:57 INFO Executor: Running task 0.0 in stage 378.0 (TID 384)\n",
      "24/09/28 15:51:57 INFO TaskSetManager: Finished task 0.0 in stage 374.0 (TID 380) in 655 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:51:57 INFO TaskSchedulerImpl: Removed TaskSet 374.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:51:57 INFO DAGScheduler: ResultStage 374 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1,957 s\n",
      "24/09/28 15:51:57 INFO DAGScheduler: Job 227 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:51:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 374: Stage finished\n",
      "24/09/28 15:51:57 INFO DAGScheduler: Job 227 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2,887113 s\n",
      "24/09/28 15:51:57 INFO MemoryStore: Block broadcast_299 stored as values in memory (estimated size 4.0 MiB, free 253.5 MiB)\n",
      "24/09/28 15:51:57 INFO CodeGenerator: Code generated in 310.427941 ms\n",
      "24/09/28 15:51:57 INFO CodeGenerator: Code generated in 6.954914 ms\n",
      "24/09/28 15:51:58 INFO MemoryStore: Block broadcast_299_piece0 stored as bytes in memory (estimated size 295.0 B, free 253.5 MiB)\n",
      "24/09/28 15:51:58 INFO BlockManagerInfo: Added broadcast_299_piece0 in memory on 10.0.2.15:38963 (size: 295.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:51:58 INFO SparkContext: Created broadcast 299 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:58 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx001_data, parameters sent from Spark:|requiredColumns=[UF,Ano,V1013,V1012,A001B3,A003,A004,V1023,A005,B0011,B0014,B0015,B0019,B00111,B002,B0031,B005,B009B,B007,C01011,F001],|filters=[]\n",
      "24/09/28 15:51:58 INFO BlockManagerInfo: Removed broadcast_276_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:51:58 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDG8xMTZaQWJwZ0JzWRoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:51:57.405Z\",\"Ended\":\"2024-09-28T18:51:58.243Z\",\"Parse Timings\":\"Average: PT0.000315906S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.255056865S Samples: 1\",\"Bytes/s\":1223,\"Rows/s\":0,\"Bytes\":312,\"Rows\":4,\"I/O time\":255}\n",
      "24/09/28 15:51:58 INFO Executor: Finished task 0.0 in stage 375.0 (TID 381). 1643 bytes result sent to driver\n",
      "24/09/28 15:51:58 INFO TaskSetManager: Starting task 0.0 in stage 380.0 (TID 385) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:51:58 INFO Executor: Running task 0.0 in stage 380.0 (TID 385)\n",
      "24/09/28 15:51:58 INFO TaskSetManager: Finished task 0.0 in stage 375.0 (TID 381) in 980 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:51:58 INFO TaskSchedulerImpl: Removed TaskSet 375.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:51:58 INFO DAGScheduler: ResultStage 375 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,243 s\n",
      "24/09/28 15:51:58 INFO DAGScheduler: Job 228 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:51:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 375: Stage finished\n",
      "24/09/28 15:51:58 INFO BlockManagerInfo: Removed broadcast_277_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:51:58 INFO DAGScheduler: Job 228 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 3,143769 s\n",
      "24/09/28 15:51:58 INFO MemoryStore: Block broadcast_300 stored as values in memory (estimated size 4.0 MiB, free 249.6 MiB)\n",
      "24/09/28 15:51:58 INFO MemoryStore: Block broadcast_300_piece0 stored as bytes in memory (estimated size 295.0 B, free 249.6 MiB)\n",
      "24/09/28 15:51:58 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDExJcV9vWTdqQUthQRoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:51:57.400Z\",\"Ended\":\"2024-09-28T18:51:58.430Z\",\"Parse Timings\":\"Average: PT0.000130505S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.45835965S Samples: 1\",\"Bytes/s\":943,\"Rows/s\":0,\"Bytes\":432,\"Rows\":7,\"I/O time\":458}\n",
      "24/09/28 15:51:58 INFO BlockManagerInfo: Added broadcast_300_piece0 in memory on 10.0.2.15:38963 (size: 295.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:51:58 INFO SparkContext: Created broadcast 300 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:58 INFO Executor: Finished task 0.0 in stage 376.0 (TID 382). 1730 bytes result sent to driver\n",
      "24/09/28 15:51:58 INFO TaskSetManager: Starting task 0.0 in stage 379.0 (TID 386) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:51:58 INFO TaskSetManager: Finished task 0.0 in stage 376.0 (TID 382) in 1114 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:51:58 INFO TaskSchedulerImpl: Removed TaskSet 376.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:51:58 INFO Executor: Running task 0.0 in stage 379.0 (TID 386)\n",
      "24/09/28 15:51:58 INFO DAGScheduler: ResultStage 376 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,321 s\n",
      "24/09/28 15:51:58 INFO DAGScheduler: Job 229 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:51:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 376: Stage finished\n",
      "24/09/28 15:51:58 INFO DAGScheduler: Job 229 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 3,319892 s\n",
      "24/09/28 15:51:58 INFO MemoryStore: Block broadcast_301 stored as values in memory (estimated size 4.0 MiB, free 245.6 MiB)\n",
      "24/09/28 15:51:58 INFO MemoryStore: Block broadcast_301_piece0 stored as bytes in memory (estimated size 467.0 B, free 245.6 MiB)\n",
      "24/09/28 15:51:58 INFO BlockManagerInfo: Added broadcast_301_piece0 in memory on 10.0.2.15:38963 (size: 467.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:51:58 INFO BlockManagerInfo: Removed broadcast_280_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:51:58 INFO CodeGenerator: Code generated in 22.017387 ms\n",
      "24/09/28 15:51:58 INFO SparkContext: Created broadcast 301 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:58 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDUtQVVpc0N5YWxsZxoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:51:57.456Z\",\"Ended\":\"2024-09-28T18:51:58.537Z\",\"Parse Timings\":\"Average: PT0.000106706S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.458246136S Samples: 1\",\"Bytes/s\":698,\"Rows/s\":0,\"Bytes\":320,\"Rows\":4,\"I/O time\":458}\n",
      "24/09/28 15:51:58 INFO BlockManagerInfo: Removed broadcast_279_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:51:58 INFO Executor: Finished task 0.0 in stage 377.0 (TID 383). 1648 bytes result sent to driver\n",
      "24/09/28 15:51:58 INFO TaskSetManager: Starting task 0.0 in stage 381.0 (TID 387) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:51:58 INFO BlockManagerInfo: Removed broadcast_275_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:51:58 INFO Executor: Running task 0.0 in stage 381.0 (TID 387)\n",
      "24/09/28 15:51:58 INFO TaskSetManager: Finished task 0.0 in stage 377.0 (TID 383) in 1252 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:51:58 INFO TaskSchedulerImpl: Removed TaskSet 377.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:51:58 INFO DAGScheduler: ResultStage 377 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,411 s\n",
      "24/09/28 15:51:58 INFO DAGScheduler: Job 230 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:51:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 377: Stage finished\n",
      "24/09/28 15:51:58 INFO DAGScheduler: Job 230 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 3,413373 s\n",
      "24/09/28 15:51:58 INFO MemoryStore: Block broadcast_302 stored as values in memory (estimated size 4.0 MiB, free 241.7 MiB)\n",
      "24/09/28 15:51:58 INFO MemoryStore: Block broadcast_302_piece0 stored as bytes in memory (estimated size 295.0 B, free 241.7 MiB)\n",
      "24/09/28 15:51:58 INFO CodeGenerator: Code generated in 198.100019 ms\n",
      "24/09/28 15:51:58 INFO BlockManagerInfo: Added broadcast_302_piece0 in memory on 10.0.2.15:38963 (size: 295.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:51:58 INFO SparkContext: Created broadcast 302 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:58 INFO BlockManagerInfo: Removed broadcast_281_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:51:58 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlaYUZHYk5yNEVYShoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:51:57.953Z\",\"Ended\":\"2024-09-28T18:51:58.721Z\",\"Parse Timings\":\"Average: PT0.000109111S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.625417511S Samples: 1\",\"Bytes/s\":448,\"Rows/s\":0,\"Bytes\":280,\"Rows\":3,\"I/O time\":625}\n",
      "24/09/28 15:51:58 INFO CodeGenerator: Code generated in 24.560332 ms\n",
      "24/09/28 15:51:58 INFO Executor: Finished task 0.0 in stage 378.0 (TID 384). 1564 bytes result sent to driver\n",
      "24/09/28 15:51:58 INFO TaskSetManager: Starting task 0.0 in stage 382.0 (TID 388) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:51:58 INFO Executor: Running task 0.0 in stage 382.0 (TID 388)\n",
      "24/09/28 15:51:58 INFO TaskSetManager: Finished task 0.0 in stage 378.0 (TID 384) in 828 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:51:58 INFO TaskSchedulerImpl: Removed TaskSet 378.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:51:58 INFO DAGScheduler: ResultStage 378 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,520 s\n",
      "24/09/28 15:51:58 INFO DAGScheduler: Job 231 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:51:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 378: Stage finished\n",
      "24/09/28 15:51:58 INFO DAGScheduler: Job 231 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 3,551937 s\n",
      "24/09/28 15:51:58 INFO MemoryStore: Block broadcast_303 stored as values in memory (estimated size 4.0 MiB, free 237.7 MiB)\n",
      "24/09/28 15:51:58 INFO MemoryStore: Block broadcast_303_piece0 stored as bytes in memory (estimated size 244.0 B, free 237.8 MiB)\n",
      "24/09/28 15:51:58 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHlUdWxSenJEc1RHMhoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:51:58.173Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:51:58.795Z\",\"readSessionPrepDuration\":294,\"readSessionCreationDuration\":328,\"readSessionDuration\":622}\n",
      "24/09/28 15:51:58 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 4 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHlUdWxSenJEc1RHMhoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:51:58 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx001_data': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHlUdWxSenJEc1RHMhoCdngaAnVo\n",
      "24/09/28 15:51:58 INFO BlockManagerInfo: Removed broadcast_274_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:51:58 INFO BlockManagerInfo: Added broadcast_303_piece0 in memory on 10.0.2.15:38963 (size: 244.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:51:58 INFO SparkContext: Created broadcast 303 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:58 INFO CodeGenerator: Code generated in 16.377807 ms\n",
      "24/09/28 15:51:58 INFO BlockManagerInfo: Removed broadcast_278_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:51:58 INFO BlockManagerInfo: Removed broadcast_282_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:51:58 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx001_data, parameters sent from Spark:|requiredColumns=[UF,Ano,V1013,V1012,A001B3,A003,A004,V1023,A005,B0011,B0014,B0015,B0019,B00111,B002,B0031,B005,B009B,B007,C01011,F001],|filters=[]\n",
      "24/09/28 15:51:59 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEhVTUZ6aEhRWlo2TRoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:51:58.486Z\",\"Ended\":\"2024-09-28T18:51:59.084Z\",\"Parse Timings\":\"Average: PT0.000111801S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.495299936S Samples: 1\",\"Bytes/s\":711,\"Rows/s\":0,\"Bytes\":352,\"Rows\":5,\"I/O time\":495}\n",
      "24/09/28 15:51:59 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEJ1VXpQcGFKWUlXShoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:51:58.365Z\",\"Ended\":\"2024-09-28T18:51:59.080Z\",\"Parse Timings\":\"Average: PT0.000133591S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.37451451S Samples: 1\",\"Bytes/s\":748,\"Rows/s\":0,\"Bytes\":280,\"Rows\":3,\"I/O time\":374}\n",
      "24/09/28 15:51:59 INFO Executor: Finished task 0.0 in stage 380.0 (TID 385). 1521 bytes result sent to driver\n",
      "24/09/28 15:51:59 INFO Executor: Finished task 0.0 in stage 379.0 (TID 386). 1596 bytes result sent to driver\n",
      "24/09/28 15:51:59 INFO TaskSetManager: Finished task 0.0 in stage 380.0 (TID 385) in 821 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:51:59 INFO TaskSchedulerImpl: Removed TaskSet 380.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:51:59 INFO DAGScheduler: ResultStage 380 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,667 s\n",
      "24/09/28 15:51:59 INFO DAGScheduler: Job 232 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:51:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 380: Stage finished\n",
      "24/09/28 15:51:59 INFO TaskSetManager: Finished task 0.0 in stage 379.0 (TID 386) in 744 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:51:59 INFO DAGScheduler: Job 232 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 3,956613 s\n",
      "24/09/28 15:51:59 INFO TaskSchedulerImpl: Removed TaskSet 379.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:51:59 INFO DAGScheduler: ResultStage 379 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,930 s\n",
      "24/09/28 15:51:59 INFO DAGScheduler: Job 233 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:51:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 379: Stage finished\n",
      "24/09/28 15:51:59 INFO DAGScheduler: Job 233 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 3,948645 s\n",
      "24/09/28 15:51:59 INFO MemoryStore: Block broadcast_304 stored as values in memory (estimated size 4.0 MiB, free 233.8 MiB)\n",
      "24/09/28 15:51:59 INFO MemoryStore: Block broadcast_304_piece0 stored as bytes in memory (estimated size 244.0 B, free 233.8 MiB)\n",
      "24/09/28 15:51:59 INFO BlockManagerInfo: Added broadcast_304_piece0 in memory on 10.0.2.15:38963 (size: 244.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:51:59 INFO SparkContext: Created broadcast 304 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:59 INFO MemoryStore: Block broadcast_305 stored as values in memory (estimated size 4.0 MiB, free 229.8 MiB)\n",
      "24/09/28 15:51:59 INFO MemoryStore: Block broadcast_305_piece0 stored as bytes in memory (estimated size 348.0 B, free 229.8 MiB)\n",
      "24/09/28 15:51:59 INFO BlockManagerInfo: Added broadcast_305_piece0 in memory on 10.0.2.15:38963 (size: 348.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:51:59 INFO SparkContext: Created broadcast 305 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:59 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDZDMjdCZmkxa2VaeBoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:51:58.771Z\",\"Ended\":\"2024-09-28T18:51:59.310Z\",\"Parse Timings\":\"Average: PT0.000107669S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.419105502S Samples: 1\",\"Bytes/s\":1126,\"Rows/s\":0,\"Bytes\":472,\"Rows\":10,\"I/O time\":419}\n",
      "24/09/28 15:51:59 INFO Executor: Finished task 0.0 in stage 382.0 (TID 388). 1684 bytes result sent to driver\n",
      "24/09/28 15:51:59 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlJaGZxV25QaVVMZBoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:51:58.641Z\",\"Ended\":\"2024-09-28T18:51:59.339Z\",\"Parse Timings\":\"Average: PT0.000350309S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.532100157S Samples: 1\",\"Bytes/s\":526,\"Rows/s\":0,\"Bytes\":280,\"Rows\":3,\"I/O time\":532}\n",
      "24/09/28 15:51:59 INFO Executor: Finished task 0.0 in stage 381.0 (TID 387). 1521 bytes result sent to driver\n",
      "24/09/28 15:51:59 INFO TaskSetManager: Finished task 0.0 in stage 382.0 (TID 388) in 600 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:51:59 INFO TaskSchedulerImpl: Removed TaskSet 382.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:51:59 INFO TaskSetManager: Finished task 0.0 in stage 381.0 (TID 387) in 747 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:51:59 INFO TaskSchedulerImpl: Removed TaskSet 381.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:51:59 INFO DAGScheduler: ResultStage 382 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,626 s\n",
      "24/09/28 15:51:59 INFO DAGScheduler: Job 235 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:51:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 382: Stage finished\n",
      "24/09/28 15:51:59 INFO DAGScheduler: ResultStage 381 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,768 s\n",
      "24/09/28 15:51:59 INFO DAGScheduler: Job 234 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:51:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 381: Stage finished\n",
      "24/09/28 15:51:59 INFO DAGScheduler: Job 234 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 4,103310 s\n",
      "24/09/28 15:51:59 INFO DAGScheduler: Job 235 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 3,754173 s\n",
      "24/09/28 15:51:59 INFO MemoryStore: Block broadcast_306 stored as values in memory (estimated size 4.0 MiB, free 221.8 MiB)\n",
      "24/09/28 15:51:59 INFO MemoryStore: Block broadcast_307 stored as values in memory (estimated size 4.0 MiB, free 221.8 MiB)\n",
      "24/09/28 15:51:59 INFO MemoryStore: Block broadcast_306_piece0 stored as bytes in memory (estimated size 244.0 B, free 221.8 MiB)\n",
      "24/09/28 15:51:59 INFO MemoryStore: Block broadcast_307_piece0 stored as bytes in memory (estimated size 430.0 B, free 221.8 MiB)\n",
      "24/09/28 15:51:59 INFO BlockManagerInfo: Added broadcast_306_piece0 in memory on 10.0.2.15:38963 (size: 244.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:51:59 INFO BlockManagerInfo: Added broadcast_307_piece0 in memory on 10.0.2.15:38963 (size: 430.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:51:59 INFO SparkContext: Created broadcast 307 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:59 INFO SparkContext: Created broadcast 306 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:51:59 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGFwdDhlaWdJdkFacRoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:51:58.966Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:51:59.613Z\",\"readSessionPrepDuration\":345,\"readSessionCreationDuration\":302,\"readSessionDuration\":647}\n",
      "24/09/28 15:51:59 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 4 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGFwdDhlaWdJdkFacRoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:51:59 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx001_data': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGFwdDhlaWdJdkFacRoCdngaAnVo\n",
      "24/09/28 15:51:59 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx001_data, parameters sent from Spark:|requiredColumns=[UF,Ano,V1013,V1012,A001B3,A003,A004,V1023,A005,B0011,B0014,B0015,B0019,B00111,B002,B0031,B005,B009B,B007,C01011,F001],|filters=[]\n",
      "24/09/28 15:52:00 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHBpTVBTTWVPbEtWUhoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:51:59.651Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:52:00.208Z\",\"readSessionPrepDuration\":208,\"readSessionCreationDuration\":349,\"readSessionDuration\":557}\n",
      "24/09/28 15:52:00 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 4 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHBpTVBTTWVPbEtWUhoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:52:00 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx001_data': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHBpTVBTTWVPbEtWUhoCdngaAnVo\n",
      "24/09/28 15:52:00 INFO BlockManagerInfo: Removed broadcast_290_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:52:00 INFO BlockManagerInfo: Removed broadcast_288_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:52:00 INFO BlockManagerInfo: Removed broadcast_286_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:52:00 INFO BlockManagerInfo: Removed broadcast_287_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:52:00 INFO BlockManagerInfo: Removed broadcast_284_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:52:00 INFO BlockManagerInfo: Removed broadcast_283_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:52:00 INFO CodeGenerator: Code generated in 96.572021 ms\n",
      "24/09/28 15:52:00 INFO BlockManagerInfo: Removed broadcast_289_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:52:00 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "24/09/28 15:52:00 INFO DAGScheduler: Got job 236 (showString at <unknown>:0) with 1 output partitions\n",
      "24/09/28 15:52:00 INFO DAGScheduler: Final stage: ResultStage 383 (showString at <unknown>:0)\n",
      "24/09/28 15:52:00 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:52:00 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:52:00 INFO DAGScheduler: Submitting ResultStage 383 (MapPartitionsRDD[1038] at showString at <unknown>:0), which has no missing parents\n",
      "24/09/28 15:52:00 INFO BlockManagerInfo: Removed broadcast_285_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:52:00 INFO MemoryStore: Block broadcast_308 stored as values in memory (estimated size 64.6 KiB, free 222.0 MiB)\n",
      "24/09/28 15:52:00 INFO MemoryStore: Block broadcast_308_piece0 stored as bytes in memory (estimated size 18.8 KiB, free 222.0 MiB)\n",
      "24/09/28 15:52:00 INFO BlockManagerInfo: Added broadcast_308_piece0 in memory on 10.0.2.15:38963 (size: 18.8 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:52:00 INFO SparkContext: Created broadcast 308 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:52:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 383 (MapPartitionsRDD[1038] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:52:00 INFO TaskSchedulerImpl: Adding task set 383.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:52:00 INFO TaskSetManager: Starting task 0.0 in stage 383.0 (TID 389) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:52:00 INFO Executor: Running task 0.0 in stage 383.0 (TID 389)\n",
      "24/09/28 15:52:00 INFO CodeGenerator: Code generated in 62.359511 ms\n",
      "[Stage 383:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---+------+--------------+------+--------+---------+--------------------+----------+-------------------------+---------------+-----------+-----------------+---------------+-------------+---------------+---------------+---------------+----------------+--------------------+\n",
      "|      uf| ano|mes|semana|ano_nascimento|  sexo|cor_raca|tipo_area|        escolaridade|teve_febre|teve_dificuldade_respirar|teve_dor_cabeca|teve_fadiga|teve_perda_cheiro|foi_posto_saude|ficou_em_casa|ficou_internado|resultado_covid|tem_plano_saude|faixa_rendimento|  situacao_domicilio|\n",
      "+--------+----+---+------+--------------+------+--------+---------+--------------------+----------+-------------------------+---------------+-----------+-----------------+---------------+-------------+---------------+---------------+---------------+----------------+--------------------+\n",
      "|Rondônia|2020| 07|     3|          1958| Homem|   Parda|  Capital|       Sem instrução|      Não |                     Não |           Não |       Não |             Não |           NULL|         NULL|           NULL|           NULL|           Não |            NULL|Cedido por familiar |\n",
      "|Rondônia|2020| 07|     4|          9999| Homem|   Parda|  Capital|Fundamental completa|      Não |                     Não |           Não |       Não |             Não |           NULL|         NULL|           NULL|           NULL|           Não |            NULL|  Próprio - já pago |\n",
      "|Rondônia|2020| 07|     1|          1976| Homem|  Branca|  Capital|      Médio completo|      Não |                     Não |           Não |       Não |             Não |           NULL|         NULL|           NULL|           NULL|           Não |            NULL|  Próprio - já pago |\n",
      "|Rondônia|2020| 07|     1|          1994| Homem|   Parda|  Capital|Fundamental completa|      Não |                     Não |           Não |       Não |             Não |           NULL|         NULL|           NULL|           NULL|           Não |            NULL|Próprio - ainda p...|\n",
      "|Rondônia|2020| 07|     1|          9999| Homem|Indígena|  Capital|      Médio completo|      Não |                     Não |           Não |       Não |             Não |           NULL|         NULL|           NULL|           NULL|           Não |            NULL|Próprio - ainda p...|\n",
      "|Rondônia|2020| 07|     3|          1981|Mulher|   Parda|  Capital|      Médio completo|      Não |                     Não |           Não |       Não |             Não |           NULL|         NULL|           NULL|           NULL|            Sim|            NULL|  Próprio - já pago |\n",
      "|Rondônia|2020| 07|     3|          1982| Homem|   Preta|  Capital|      Médio completo|      Não |                     Não |           Não |       Não |             Não |           NULL|         NULL|           NULL|           NULL|           Não |            NULL|  Próprio - já pago |\n",
      "|Rondônia|2020| 07|     1|          1968| Homem|   Parda|  Capital|Fundamental incom...|      Não |                     Não |           Não |       Não |             Não |           NULL|         NULL|           NULL|           NULL|           Não |            NULL|  Próprio - já pago |\n",
      "|Rondônia|2020| 07|     4|          9999| Homem|   Preta|  Capital|       Sem instrução|      Não |                     Não |           Não |       Não |             Não |           NULL|         NULL|           NULL|           NULL|           Não |            NULL|  Próprio - já pago |\n",
      "|Rondônia|2020| 07|     3|          1992| Homem|   Parda|  Capital|    Médio incompleto|      Não |                     Não |           Não |       Não |             Não |           NULL|         NULL|           NULL|           NULL|           Não |            NULL|Cedido por familiar |\n",
      "|Rondônia|2020| 07|     2|          1987| Homem|  Branca|  Capital|      Médio completo|      Não |                     Não |           Não |       Não |             Não |           NULL|         NULL|           NULL|       Negativo|           Não |            NULL|Cedido por familiar |\n",
      "|Rondônia|2020| 07|     1|          1964| Homem|   Parda|  Capital|      Médio completo|      Não |                     Não |           Não |       Não |             Não |           NULL|         NULL|           NULL|           NULL|            Sim|            NULL|  Próprio - já pago |\n",
      "|Rondônia|2020| 07|     1|          1997| Homem|   Parda|  Capital|      Médio completo|      Não |                     Não |           Não |       Não |             Não |           NULL|         NULL|           NULL|           NULL|           Não |            NULL|  Próprio - já pago |\n",
      "|Rondônia|2020| 07|     3|          1996|Mulher|  Branca|  Capital|      Médio completo|      Não |                     Não |           Não |       Não |             Não |           NULL|         NULL|           NULL|           NULL|           Não |            NULL|  Próprio - já pago |\n",
      "|Rondônia|2020| 07|     4|          1993|Mulher|   Preta|  Capital|Fundamental completa|      Não |                     Não |           Não |       Não |             Não |           NULL|         NULL|           NULL|           NULL|           Não |            NULL|Cedido por familiar |\n",
      "|Rondônia|2020| 07|     1|          9999|Mulher|   Parda|  Capital|      Médio completo|      Não |                     Não |           Não |       Não |             Não |           NULL|         NULL|           NULL|           NULL|            Sim|            NULL|  Próprio - já pago |\n",
      "|Rondônia|2020| 07|     2|          1989| Homem|   Parda|  Capital|      Médio completo|      Não |                     Não |           Não |       Não |             Não |           NULL|         NULL|           NULL|           NULL|           Não |            NULL|  Próprio - já pago |\n",
      "|Rondônia|2020| 07|     3|          1971| Homem|   Preta|  Capital|Fundamental incom...|      Não |                     Não |           Não |       Não |             Não |           NULL|         NULL|           NULL|           NULL|           Não |            NULL|Cedido por familiar |\n",
      "|Rondônia|2020| 07|     3|          1977|Mulher|   Parda|  Capital|   Superior completo|      Não |                     Não |           Não |       Não |             Não |           NULL|         NULL|           NULL|           NULL|           Não |            NULL|  Próprio - já pago |\n",
      "|Rondônia|2020| 07|     4|          1969| Homem|  Branca|  Capital|Fundamental incom...|      Não |                     Não |           Não |       Não |             Não |           NULL|         NULL|           NULL|           NULL|           Não |            NULL|Próprio - ainda p...|\n",
      "+--------+----+---+------+--------------+------+--------+---------+--------------------+----------+-------------------------+---------------+-----------+-----------------+---------------+-------------+---------------+---------------+---------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/28 15:52:01 INFO Executor: Finished task 0.0 in stage 383.0 (TID 389). 3651 bytes result sent to driver\n",
      "24/09/28 15:52:01 INFO TaskSetManager: Finished task 0.0 in stage 383.0 (TID 389) in 759 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:52:01 INFO TaskSchedulerImpl: Removed TaskSet 383.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:52:01 INFO DAGScheduler: ResultStage 383 (showString at <unknown>:0) finished in 0,834 s\n",
      "24/09/28 15:52:01 INFO DAGScheduler: Job 236 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:52:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 383: Stage finished\n",
      "24/09/28 15:52:01 INFO DAGScheduler: Job 236 finished: showString at <unknown>:0, took 0,863838 s\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/28 15:52:33 INFO BlockManagerInfo: Removed broadcast_302_piece0 on 10.0.2.15:38963 in memory (size: 295.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:33 INFO BlockManagerInfo: Removed broadcast_293_piece0 on 10.0.2.15:38963 in memory (size: 359.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:33 INFO BlockManagerInfo: Removed broadcast_307_piece0 on 10.0.2.15:38963 in memory (size: 430.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:33 INFO BlockManagerInfo: Removed broadcast_291_piece0 on 10.0.2.15:38963 in memory (size: 215.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:33 INFO BlockManagerInfo: Removed broadcast_308_piece0 on 10.0.2.15:38963 in memory (size: 18.8 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_297_piece0 on 10.0.2.15:38963 in memory (size: 295.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_301_piece0 on 10.0.2.15:38963 in memory (size: 467.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_305_piece0 on 10.0.2.15:38963 in memory (size: 348.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_306_piece0 on 10.0.2.15:38963 in memory (size: 244.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_303_piece0 on 10.0.2.15:38963 in memory (size: 244.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_299_piece0 on 10.0.2.15:38963 in memory (size: 295.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_304_piece0 on 10.0.2.15:38963 in memory (size: 244.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_295_piece0 on 10.0.2.15:38963 in memory (size: 295.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_292_piece0 on 10.0.2.15:38963 in memory (size: 446.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_294_piece0 on 10.0.2.15:38963 in memory (size: 1038.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_298_piece0 on 10.0.2.15:38963 in memory (size: 492.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_300_piece0 on 10.0.2.15:38963 in memory (size: 295.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_296_piece0 on 10.0.2.15:38963 in memory (size: 295.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_227_piece0 on 10.0.2.15:38963 in memory (size: 212.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_225_piece0 on 10.0.2.15:38963 in memory (size: 223.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_220_piece0 on 10.0.2.15:38963 in memory (size: 120.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_230_piece0 on 10.0.2.15:38963 in memory (size: 294.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_237_piece0 on 10.0.2.15:38963 in memory (size: 223.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_231_piece0 on 10.0.2.15:38963 in memory (size: 589.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_253_piece0 on 10.0.2.15:38963 in memory (size: 10.0 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_218_piece0 on 10.0.2.15:38963 in memory (size: 206.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_252_piece0 on 10.0.2.15:38963 in memory (size: 10.0 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_224_piece0 on 10.0.2.15:38963 in memory (size: 223.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_221_piece0 on 10.0.2.15:38963 in memory (size: 189.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_222_piece0 on 10.0.2.15:38963 in memory (size: 223.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_235_piece0 on 10.0.2.15:38963 in memory (size: 206.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_251_piece0 on 10.0.2.15:38963 in memory (size: 10.0 KiB, free: 366.3 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_233_piece0 on 10.0.2.15:38963 in memory (size: 223.0 B, free: 366.3 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_232_piece0 on 10.0.2.15:38963 in memory (size: 277.0 B, free: 366.3 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_229_piece0 on 10.0.2.15:38963 in memory (size: 206.0 B, free: 366.3 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_219_piece0 on 10.0.2.15:38963 in memory (size: 120.0 B, free: 366.3 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_234_piece0 on 10.0.2.15:38963 in memory (size: 291.0 B, free: 366.3 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_226_piece0 on 10.0.2.15:38963 in memory (size: 243.0 B, free: 366.3 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_214_piece0 on 10.0.2.15:38963 in memory (size: 10.0 KiB, free: 366.3 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_236_piece0 on 10.0.2.15:38963 in memory (size: 223.0 B, free: 366.3 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_223_piece0 on 10.0.2.15:38963 in memory (size: 260.0 B, free: 366.3 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_216_piece0 on 10.0.2.15:38963 in memory (size: 10.0 KiB, free: 366.3 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_228_piece0 on 10.0.2.15:38963 in memory (size: 223.0 B, free: 366.3 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_255_piece0 on 10.0.2.15:38963 in memory (size: 10.0 KiB, free: 366.3 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_217_piece0 on 10.0.2.15:38963 in memory (size: 180.0 B, free: 366.3 MiB)\n",
      "24/09/28 15:52:34 INFO BlockManagerInfo: Removed broadcast_254_piece0 on 10.0.2.15:38963 in memory (size: 10.0 KiB, free: 366.3 MiB)\n"
     ]
    }
   ],
   "source": [
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/28 15:57:29 INFO GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=2169; previousMaxLatencyMs=0; operationCount=1; context=gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef\n",
      "24/09/28 15:57:29 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx001_data, parameters sent from Spark:|requiredColumns=[UF,Ano,V1013,V1012,A001B3,A003,A004,V1023,A005,B0011,B0014,B0015,B0019,B00111,B002,B0031,B005,B009B,B007,C01011,F001],|filters=[]\n",
      "24/09/28 15:57:30 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHpsZEFTZEUzU004LRoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:57:29.883Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:57:30.642Z\",\"readSessionPrepDuration\":234,\"readSessionCreationDuration\":525,\"readSessionDuration\":759}\n",
      "24/09/28 15:57:30 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 4 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHpsZEFTZEUzU004LRoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:57:30 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx001_data': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHpsZEFTZEUzU004LRoCdngaAnVo\n",
      "24/09/28 15:57:30 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,UF),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:57:31 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDC1oMC1nOXo2MzhRRRoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:57:30.667Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:57:31.405Z\",\"readSessionPrepDuration\":231,\"readSessionCreationDuration\":507,\"readSessionDuration\":738}\n",
      "24/09/28 15:57:31 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDC1oMC1nOXo2MzhRRRoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:57:31 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDC1oMC1nOXo2MzhRRRoCdngaAnVo\n",
      "24/09/28 15:57:31 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,A003),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:57:32 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFRia3o4Rl9HbU5oSxoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:57:31.424Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:57:32.041Z\",\"readSessionPrepDuration\":253,\"readSessionCreationDuration\":364,\"readSessionDuration\":617}\n",
      "24/09/28 15:57:32 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFRia3o4Rl9HbU5oSxoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:57:32 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFRia3o4Rl9HbU5oSxoCdngaAnVo\n",
      "24/09/28 15:57:32 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,A004),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:57:32 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGQtNEtUdFl4WTZ2TxoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:57:32.050Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:57:32.638Z\",\"readSessionPrepDuration\":283,\"readSessionCreationDuration\":305,\"readSessionDuration\":588}\n",
      "24/09/28 15:57:32 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGQtNEtUdFl4WTZ2TxoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:57:32 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGQtNEtUdFl4WTZ2TxoCdngaAnVo\n",
      "24/09/28 15:57:32 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,V1023),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:57:33 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGROaDBiNFlEX1RTRxoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:57:32.653Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:57:33.195Z\",\"readSessionPrepDuration\":237,\"readSessionCreationDuration\":305,\"readSessionDuration\":542}\n",
      "24/09/28 15:57:33 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGROaDBiNFlEX1RTRxoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:57:33 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGROaDBiNFlEX1RTRxoCdngaAnVo\n",
      "24/09/28 15:57:33 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,A005),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:57:33 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHVtV2ppNy02T0w4dxoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:57:33.206Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:57:33.765Z\",\"readSessionPrepDuration\":250,\"readSessionCreationDuration\":309,\"readSessionDuration\":559}\n",
      "24/09/28 15:57:33 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHVtV2ppNy02T0w4dxoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:57:33 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHVtV2ppNy02T0w4dxoCdngaAnVo\n",
      "24/09/28 15:57:33 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B0011),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:57:34 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFBfTnJSOGhycU5qTRoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:57:33.773Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:57:34.289Z\",\"readSessionPrepDuration\":210,\"readSessionCreationDuration\":306,\"readSessionDuration\":516}\n",
      "24/09/28 15:57:34 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFBfTnJSOGhycU5qTRoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:57:34 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFBfTnJSOGhycU5qTRoCdngaAnVo\n",
      "24/09/28 15:57:34 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B0014),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:57:34 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDG1JTDQ3WTNIZkFoMRoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:57:34.299Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:57:34.857Z\",\"readSessionPrepDuration\":239,\"readSessionCreationDuration\":319,\"readSessionDuration\":558}\n",
      "24/09/28 15:57:34 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDG1JTDQ3WTNIZkFoMRoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:57:34 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDG1JTDQ3WTNIZkFoMRoCdngaAnVo\n",
      "24/09/28 15:57:34 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B0015),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:57:35 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFJ1M0hScnFVZzVvehoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:57:34.863Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:57:35.401Z\",\"readSessionPrepDuration\":217,\"readSessionCreationDuration\":321,\"readSessionDuration\":538}\n",
      "24/09/28 15:57:35 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFJ1M0hScnFVZzVvehoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:57:35 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFJ1M0hScnFVZzVvehoCdngaAnVo\n",
      "24/09/28 15:57:35 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B0019),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:57:35 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGZSMXVCYWcwSWk4RhoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:57:35.414Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:57:35.991Z\",\"readSessionPrepDuration\":273,\"readSessionCreationDuration\":304,\"readSessionDuration\":577}\n",
      "24/09/28 15:57:35 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGZSMXVCYWcwSWk4RhoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:57:35 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGZSMXVCYWcwSWk4RhoCdngaAnVo\n",
      "24/09/28 15:57:35 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B00111),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:57:36 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDC1yUlpWcWdYNXZQcBoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:57:35.998Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:57:36.493Z\",\"readSessionPrepDuration\":230,\"readSessionCreationDuration\":265,\"readSessionDuration\":495}\n",
      "24/09/28 15:57:36 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDC1yUlpWcWdYNXZQcBoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:57:36 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDC1yUlpWcWdYNXZQcBoCdngaAnVo\n",
      "24/09/28 15:57:36 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B002),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:57:37 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDJ6Z0VERkxpT3N3ShoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:57:36.497Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:57:37.003Z\",\"readSessionPrepDuration\":208,\"readSessionCreationDuration\":298,\"readSessionDuration\":506}\n",
      "24/09/28 15:57:37 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDJ6Z0VERkxpT3N3ShoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:57:37 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDJ6Z0VERkxpT3N3ShoCdngaAnVo\n",
      "24/09/28 15:57:37 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B0031),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:57:37 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDE9MSEs1WWF2ZEp1bxoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:57:37.013Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:57:37.543Z\",\"readSessionPrepDuration\":249,\"readSessionCreationDuration\":281,\"readSessionDuration\":530}\n",
      "24/09/28 15:57:37 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDE9MSEs1WWF2ZEp1bxoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:57:37 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDE9MSEs1WWF2ZEp1bxoCdngaAnVo\n",
      "24/09/28 15:57:37 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B005),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:57:38 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEMySlBBdGFGNWxsWhoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:57:37.560Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:57:38.087Z\",\"readSessionPrepDuration\":241,\"readSessionCreationDuration\":286,\"readSessionDuration\":527}\n",
      "24/09/28 15:57:38 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEMySlBBdGFGNWxsWhoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:57:38 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEMySlBBdGFGNWxsWhoCdngaAnVo\n",
      "24/09/28 15:57:38 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B009B),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:57:38 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEw1ZUxRMkVzRmIycBoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:57:38.102Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:57:38.631Z\",\"readSessionPrepDuration\":214,\"readSessionCreationDuration\":315,\"readSessionDuration\":529}\n",
      "24/09/28 15:57:38 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEw1ZUxRMkVzRmIycBoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:57:38 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEw1ZUxRMkVzRmIycBoCdngaAnVo\n",
      "24/09/28 15:57:38 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,B007),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:57:39 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDERwVERJWWhMb0RJRxoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:57:38.636Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:57:39.142Z\",\"readSessionPrepDuration\":212,\"readSessionCreationDuration\":294,\"readSessionDuration\":506}\n",
      "24/09/28 15:57:39 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDERwVERJWWhMb0RJRxoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:57:39 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDERwVERJWWhMb0RJRxoCdngaAnVo\n",
      "24/09/28 15:57:39 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,C01011),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:57:39 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDJFQnNNV1pTTDhaQhoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:57:39.147Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:57:39.688Z\",\"readSessionPrepDuration\":220,\"readSessionCreationDuration\":321,\"readSessionDuration\":541}\n",
      "24/09/28 15:57:39 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDJFQnNNV1pTTDhaQhoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:57:39 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDJFQnNNV1pTTDhaQhoCdngaAnVo\n",
      "24/09/28 15:57:39 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx002_dimensao_geral, parameters sent from Spark:|requiredColumns=[categoria_tipo,categoria_descricao],|filters=[IsNotNull(codigo_variavel),EqualTo(codigo_variavel,F001),IsNotNull(categoria_tipo)]\n",
      "24/09/28 15:57:40 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEFNTDBoQ2tQVnlNMhoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:57:39.722Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:57:40.257Z\",\"readSessionPrepDuration\":227,\"readSessionCreationDuration\":308,\"readSessionDuration\":535}\n",
      "24/09/28 15:57:40 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 1 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEFNTDBoQ2tQVnlNMhoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:57:40 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx002_dimensao_geral': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEFNTDBoQ2tQVnlNMhoCdngaAnVo\n",
      "24/09/28 15:57:40 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:40 INFO DAGScheduler: Got job 237 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:57:40 INFO DAGScheduler: Final stage: ResultStage 384 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:57:40 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:57:40 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:57:40 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:40 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:40 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:40 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:40 INFO DAGScheduler: Submitting ResultStage 384 (MapPartitionsRDD[1059] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:57:40 INFO MemoryStore: Block broadcast_309 stored as values in memory (estimated size 20.5 KiB, free 366.3 MiB)\n",
      "24/09/28 15:57:40 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:40 INFO MemoryStore: Block broadcast_309_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 366.3 MiB)\n",
      "24/09/28 15:57:40 INFO BlockManagerInfo: Added broadcast_309_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.3 MiB)\n",
      "24/09/28 15:57:40 INFO SparkContext: Created broadcast 309 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:57:40 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 384 (MapPartitionsRDD[1059] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:57:40 INFO TaskSchedulerImpl: Adding task set 384.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:57:40 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:40 INFO TaskSetManager: Starting task 0.0 in stage 384.0 (TID 390) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:57:40 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:41 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:41 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:41 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:41 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:41 INFO Executor: Running task 0.0 in stage 384.0 (TID 390)\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Got job 238 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Final stage: ResultStage 385 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Submitting ResultStage 385 (MapPartitionsRDD[1068] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:57:41 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:41 INFO MemoryStore: Block broadcast_310 stored as values in memory (estimated size 20.5 KiB, free 366.3 MiB)\n",
      "24/09/28 15:57:41 INFO MemoryStore: Block broadcast_310_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 366.2 MiB)\n",
      "24/09/28 15:57:41 INFO BlockManagerInfo: Added broadcast_310_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.3 MiB)\n",
      "24/09/28 15:57:41 INFO SparkContext: Created broadcast 310 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 385 (MapPartitionsRDD[1068] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:57:41 INFO TaskSchedulerImpl: Adding task set 385.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:57:41 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:41 INFO TaskSetManager: Starting task 0.0 in stage 385.0 (TID 391) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:57:41 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Got job 239 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Final stage: ResultStage 386 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Submitting ResultStage 386 (MapPartitionsRDD[1075] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:57:41 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:41 INFO Executor: Running task 0.0 in stage 385.0 (TID 391)\n",
      "24/09/28 15:57:41 INFO MemoryStore: Block broadcast_311 stored as values in memory (estimated size 20.5 KiB, free 366.2 MiB)\n",
      "24/09/28 15:57:41 INFO MemoryStore: Block broadcast_311_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 366.2 MiB)\n",
      "24/09/28 15:57:41 INFO BlockManagerInfo: Added broadcast_311_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.3 MiB)\n",
      "24/09/28 15:57:41 INFO SparkContext: Created broadcast 311 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 386 (MapPartitionsRDD[1075] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:57:41 INFO TaskSchedulerImpl: Adding task set 386.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:57:41 INFO TaskSetManager: Starting task 0.0 in stage 386.0 (TID 392) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:57:41 INFO DAGScheduler: Got job 240 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Final stage: ResultStage 387 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:57:41 INFO Executor: Running task 0.0 in stage 386.0 (TID 392)\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Submitting ResultStage 387 (MapPartitionsRDD[1065] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:57:41 INFO MemoryStore: Block broadcast_312 stored as values in memory (estimated size 20.5 KiB, free 366.2 MiB)\n",
      "24/09/28 15:57:41 INFO MemoryStore: Block broadcast_312_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 366.2 MiB)\n",
      "24/09/28 15:57:41 INFO BlockManagerInfo: Added broadcast_312_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.3 MiB)\n",
      "24/09/28 15:57:41 INFO SparkContext: Created broadcast 312 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 387 (MapPartitionsRDD[1065] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:57:41 INFO TaskSchedulerImpl: Adding task set 387.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Got job 241 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Final stage: ResultStage 388 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:57:41 INFO TaskSetManager: Starting task 0.0 in stage 387.0 (TID 393) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:57:41 INFO DAGScheduler: Submitting ResultStage 388 (MapPartitionsRDD[1062] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:57:41 INFO MemoryStore: Block broadcast_313 stored as values in memory (estimated size 20.5 KiB, free 366.2 MiB)\n",
      "24/09/28 15:57:41 INFO Executor: Running task 0.0 in stage 387.0 (TID 393)\n",
      "24/09/28 15:57:41 INFO MemoryStore: Block broadcast_313_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 366.2 MiB)\n",
      "24/09/28 15:57:41 INFO BlockManagerInfo: Added broadcast_313_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.3 MiB)\n",
      "24/09/28 15:57:41 INFO SparkContext: Created broadcast 313 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 388 (MapPartitionsRDD[1062] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:57:41 INFO TaskSchedulerImpl: Adding task set 388.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Got job 242 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Final stage: ResultStage 389 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Submitting ResultStage 389 (MapPartitionsRDD[1078] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:57:41 INFO MemoryStore: Block broadcast_314 stored as values in memory (estimated size 20.5 KiB, free 366.1 MiB)\n",
      "24/09/28 15:57:41 INFO MemoryStore: Block broadcast_314_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 366.1 MiB)\n",
      "24/09/28 15:57:41 INFO BlockManagerInfo: Added broadcast_314_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:57:41 INFO SparkContext: Created broadcast 314 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 389 (MapPartitionsRDD[1078] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:57:41 INFO TaskSchedulerImpl: Adding task set 389.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Got job 243 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Final stage: ResultStage 390 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Submitting ResultStage 390 (MapPartitionsRDD[1077] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:57:41 INFO MemoryStore: Block broadcast_315 stored as values in memory (estimated size 20.5 KiB, free 366.1 MiB)\n",
      "24/09/28 15:57:41 INFO MemoryStore: Block broadcast_315_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 366.1 MiB)\n",
      "24/09/28 15:57:41 INFO BlockManagerInfo: Added broadcast_315_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:57:41 INFO SparkContext: Created broadcast 315 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 390 (MapPartitionsRDD[1077] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:57:41 INFO TaskSchedulerImpl: Adding task set 390.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Got job 244 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Final stage: ResultStage 391 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Submitting ResultStage 391 (MapPartitionsRDD[1070] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:57:41 INFO MemoryStore: Block broadcast_316 stored as values in memory (estimated size 20.5 KiB, free 366.1 MiB)\n",
      "24/09/28 15:57:41 INFO MemoryStore: Block broadcast_316_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 366.1 MiB)\n",
      "24/09/28 15:57:41 INFO BlockManagerInfo: Added broadcast_316_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:57:41 INFO SparkContext: Created broadcast 316 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 391 (MapPartitionsRDD[1070] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:57:41 INFO TaskSchedulerImpl: Adding task set 391.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Got job 245 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Final stage: ResultStage 392 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Submitting ResultStage 392 (MapPartitionsRDD[1066] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:57:41 INFO MemoryStore: Block broadcast_317 stored as values in memory (estimated size 20.5 KiB, free 366.0 MiB)\n",
      "24/09/28 15:57:41 INFO MemoryStore: Block broadcast_317_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 366.0 MiB)\n",
      "24/09/28 15:57:41 INFO BlockManagerInfo: Added broadcast_317_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:57:41 INFO SparkContext: Created broadcast 317 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 392 (MapPartitionsRDD[1066] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:57:41 INFO TaskSchedulerImpl: Adding task set 392.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Got job 247 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Final stage: ResultStage 393 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Submitting ResultStage 393 (MapPartitionsRDD[1060] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:57:41 INFO MemoryStore: Block broadcast_318 stored as values in memory (estimated size 20.5 KiB, free 366.0 MiB)\n",
      "24/09/28 15:57:41 INFO MemoryStore: Block broadcast_318_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 366.0 MiB)\n",
      "24/09/28 15:57:41 INFO BlockManagerInfo: Added broadcast_318_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:57:41 INFO SparkContext: Created broadcast 318 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 393 (MapPartitionsRDD[1060] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:57:41 INFO TaskSchedulerImpl: Adding task set 393.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Got job 246 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Final stage: ResultStage 394 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:57:41 INFO DAGScheduler: Submitting ResultStage 394 (MapPartitionsRDD[1072] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:57:41 INFO MemoryStore: Block broadcast_319 stored as values in memory (estimated size 20.5 KiB, free 366.0 MiB)\n",
      "24/09/28 15:57:41 INFO MemoryStore: Block broadcast_319_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 366.0 MiB)\n",
      "24/09/28 15:57:41 INFO BlockManagerInfo: Added broadcast_319_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:57:42 INFO SparkContext: Created broadcast 319 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 394 (MapPartitionsRDD[1072] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:57:42 INFO TaskSchedulerImpl: Adding task set 394.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Got job 248 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Final stage: ResultStage 395 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Submitting ResultStage 395 (MapPartitionsRDD[1082] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:57:42 INFO MemoryStore: Block broadcast_320 stored as values in memory (estimated size 20.5 KiB, free 366.0 MiB)\n",
      "24/09/28 15:57:42 INFO MemoryStore: Block broadcast_320_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 365.9 MiB)\n",
      "24/09/28 15:57:42 INFO BlockManagerInfo: Added broadcast_320_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:57:42 INFO SparkContext: Created broadcast 320 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 395 (MapPartitionsRDD[1082] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:57:42 INFO TaskSchedulerImpl: Adding task set 395.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Got job 249 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Final stage: ResultStage 396 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Submitting ResultStage 396 (MapPartitionsRDD[1080] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:57:42 INFO MemoryStore: Block broadcast_321 stored as values in memory (estimated size 20.5 KiB, free 365.9 MiB)\n",
      "24/09/28 15:57:42 INFO MemoryStore: Block broadcast_321_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 365.9 MiB)\n",
      "24/09/28 15:57:42 INFO BlockManagerInfo: Added broadcast_321_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:57:42 INFO SparkContext: Created broadcast 321 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 396 (MapPartitionsRDD[1080] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:57:42 INFO TaskSchedulerImpl: Adding task set 396.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Got job 250 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Final stage: ResultStage 397 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Submitting ResultStage 397 (MapPartitionsRDD[1090] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:57:42 INFO MemoryStore: Block broadcast_322 stored as values in memory (estimated size 20.5 KiB, free 365.9 MiB)\n",
      "24/09/28 15:57:42 INFO MemoryStore: Block broadcast_322_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 365.9 MiB)\n",
      "24/09/28 15:57:42 INFO BlockManagerInfo: Added broadcast_322_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:57:42 INFO SparkContext: Created broadcast 322 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 397 (MapPartitionsRDD[1090] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:57:42 INFO TaskSchedulerImpl: Adding task set 397.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Got job 251 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Final stage: ResultStage 398 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Submitting ResultStage 398 (MapPartitionsRDD[1084] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:57:42 INFO MemoryStore: Block broadcast_323 stored as values in memory (estimated size 20.5 KiB, free 365.9 MiB)\n",
      "24/09/28 15:57:42 INFO MemoryStore: Block broadcast_323_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 365.9 MiB)\n",
      "24/09/28 15:57:42 INFO BlockManagerInfo: Added broadcast_323_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:57:42 INFO SparkContext: Created broadcast 323 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 398 (MapPartitionsRDD[1084] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:57:42 INFO TaskSchedulerImpl: Adding task set 398.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Got job 252 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Final stage: ResultStage 399 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Submitting ResultStage 399 (MapPartitionsRDD[1087] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:57:42 INFO MemoryStore: Block broadcast_324 stored as values in memory (estimated size 20.5 KiB, free 365.8 MiB)\n",
      "24/09/28 15:57:42 INFO MemoryStore: Block broadcast_324_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 365.8 MiB)\n",
      "24/09/28 15:57:42 INFO BlockManagerInfo: Added broadcast_324_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:57:42 INFO SparkContext: Created broadcast 324 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 399 (MapPartitionsRDD[1087] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:57:42 INFO TaskSchedulerImpl: Adding task set 399.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:57:42 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDHVtV2ppNy02T0w4dxoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:57:41.318Z\",\"Ended\":\"2024-09-28T18:57:42.192Z\",\"Parse Timings\":\"Average: PT0.002051638S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.696915598S Samples: 1\",\"Bytes/s\":689,\"Rows/s\":4000,\"Bytes\":480,\"Rows\":8,\"I/O time\":696}\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Got job 253 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Final stage: ResultStage 400 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Submitting ResultStage 400 (MapPartitionsRDD[1088] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "24/09/28 15:57:42 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDC1yUlpWcWdYNXZQcBoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:57:41.421Z\",\"Ended\":\"2024-09-28T18:57:42.212Z\",\"Parse Timings\":\"Average: PT0.000789808S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.564184411S Samples: 1\",\"Bytes/s\":553,\"Rows/s\":0,\"Bytes\":312,\"Rows\":4,\"I/O time\":564}\n",
      "24/09/28 15:57:42 INFO Executor: Finished task 0.0 in stage 385.0 (TID 391). 1705 bytes result sent to driver\n",
      "24/09/28 15:57:42 INFO Executor: Finished task 0.0 in stage 386.0 (TID 392). 1600 bytes result sent to driver\n",
      "24/09/28 15:57:42 INFO TaskSetManager: Starting task 0.0 in stage 388.0 (TID 394) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:57:42 INFO TaskSetManager: Starting task 0.0 in stage 389.0 (TID 395) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:57:42 INFO MemoryStore: Block broadcast_325 stored as values in memory (estimated size 20.5 KiB, free 365.8 MiB)\n",
      "24/09/28 15:57:42 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFBfTnJSOGhycU5qTRoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:57:41.563Z\",\"Ended\":\"2024-09-28T18:57:42.327Z\",\"Parse Timings\":\"Average: PT0.001258243S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.563823885S Samples: 1\",\"Bytes/s\":554,\"Rows/s\":4000,\"Bytes\":312,\"Rows\":4,\"I/O time\":563}\n",
      "24/09/28 15:57:42 INFO TaskSetManager: Finished task 0.0 in stage 385.0 (TID 391) in 1182 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:57:42 INFO TaskSchedulerImpl: Removed TaskSet 385.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:57:42 INFO Executor: Running task 0.0 in stage 389.0 (TID 395)\n",
      "24/09/28 15:57:42 INFO Executor: Finished task 0.0 in stage 387.0 (TID 393). 1557 bytes result sent to driver\n",
      "24/09/28 15:57:42 INFO Executor: Running task 0.0 in stage 388.0 (TID 394)\n",
      "24/09/28 15:57:42 INFO TaskSetManager: Starting task 0.0 in stage 390.0 (TID 396) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:57:42 INFO MemoryStore: Block broadcast_325_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 365.8 MiB)\n",
      "24/09/28 15:57:42 INFO Executor: Running task 0.0 in stage 390.0 (TID 396)\n",
      "24/09/28 15:57:42 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFRia3o4Rl9HbU5oSxoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:57:41.320Z\",\"Ended\":\"2024-09-28T18:57:42.356Z\",\"Parse Timings\":\"Average: PT0.001305337S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.615251407S Samples: 1\",\"Bytes/s\":455,\"Rows/s\":2000,\"Bytes\":280,\"Rows\":2,\"I/O time\":615}\n",
      "24/09/28 15:57:42 INFO TaskSetManager: Finished task 0.0 in stage 386.0 (TID 392) in 994 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:57:42 INFO TaskSchedulerImpl: Removed TaskSet 386.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:57:42 INFO BlockManagerInfo: Added broadcast_325_piece0 in memory on 10.0.2.15:38963 (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:57:42 INFO SparkContext: Created broadcast 325 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 400 (MapPartitionsRDD[1088] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/28 15:57:42 INFO TaskSchedulerImpl: Adding task set 400.0 with 1 tasks resource profile 0\n",
      "24/09/28 15:57:42 INFO Executor: Finished task 0.0 in stage 384.0 (TID 390). 1546 bytes result sent to driver\n",
      "24/09/28 15:57:42 INFO DAGScheduler: ResultStage 385 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1,270 s\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Job 238 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:57:42 INFO TaskSetManager: Finished task 0.0 in stage 387.0 (TID 393) in 913 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:57:42 INFO TaskSchedulerImpl: Removed TaskSet 387.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:57:42 INFO TaskSetManager: Starting task 0.0 in stage 391.0 (TID 397) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:57:42 INFO Executor: Running task 0.0 in stage 391.0 (TID 397)\n",
      "24/09/28 15:57:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 385: Stage finished\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Job 238 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 1,770492 s\n",
      "24/09/28 15:57:42 INFO DAGScheduler: ResultStage 386 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1,239 s\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Job 239 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:57:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 386: Stage finished\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Job 239 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 1,773797 s\n",
      "24/09/28 15:57:42 INFO DAGScheduler: ResultStage 387 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1,044 s\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Job 240 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:57:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 387: Stage finished\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Job 240 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 1,768553 s\n",
      "24/09/28 15:57:42 INFO TaskSetManager: Finished task 0.0 in stage 384.0 (TID 390) in 1557 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:57:42 INFO TaskSchedulerImpl: Removed TaskSet 384.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:57:42 INFO MemoryStore: Block broadcast_326 stored as values in memory (estimated size 4.0 MiB, free 361.8 MiB)\n",
      "24/09/28 15:57:42 INFO DAGScheduler: ResultStage 384 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1,833 s\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Job 237 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:57:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 384: Stage finished\n",
      "24/09/28 15:57:42 INFO DAGScheduler: Job 237 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 1,906388 s\n",
      "24/09/28 15:57:42 INFO MemoryStore: Block broadcast_328 stored as values in memory (estimated size 4.0 MiB, free 353.8 MiB)\n",
      "24/09/28 15:57:42 INFO MemoryStore: Block broadcast_326_piece0 stored as bytes in memory (estimated size 295.0 B, free 357.8 MiB)\n",
      "24/09/28 15:57:42 INFO MemoryStore: Block broadcast_327 stored as values in memory (estimated size 4.0 MiB, free 357.8 MiB)\n",
      "24/09/28 15:57:42 INFO MemoryStore: Block broadcast_327_piece0 stored as bytes in memory (estimated size 295.0 B, free 349.8 MiB)\n",
      "24/09/28 15:57:42 INFO BlockManagerInfo: Added broadcast_326_piece0 in memory on 10.0.2.15:38963 (size: 295.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:57:42 INFO BlockManagerInfo: Added broadcast_327_piece0 in memory on 10.0.2.15:38963 (size: 295.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:57:42 INFO SparkContext: Created broadcast 326 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:42 INFO MemoryStore: Block broadcast_329 stored as values in memory (estimated size 4.0 MiB, free 349.8 MiB)\n",
      "24/09/28 15:57:42 INFO MemoryStore: Block broadcast_328_piece0 stored as bytes in memory (estimated size 492.0 B, free 353.8 MiB)\n",
      "24/09/28 15:57:42 INFO MemoryStore: Block broadcast_329_piece0 stored as bytes in memory (estimated size 215.0 B, free 349.8 MiB)\n",
      "24/09/28 15:57:42 INFO SparkContext: Created broadcast 327 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:42 INFO BlockManagerInfo: Added broadcast_328_piece0 in memory on 10.0.2.15:38963 (size: 492.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:57:42 INFO BlockManagerInfo: Added broadcast_329_piece0 in memory on 10.0.2.15:38963 (size: 215.0 B, free: 366.1 MiB)\n",
      "24/09/28 15:57:42 INFO SparkContext: Created broadcast 328 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:42 INFO SparkContext: Created broadcast 329 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:42 INFO BlockManagerInfo: Removed broadcast_311_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:57:42 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDJ6Z0VERkxpT3N3ShoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:57:42.352Z\",\"Ended\":\"2024-09-28T18:57:42.958Z\",\"Parse Timings\":\"Average: PT0.00057378S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.564063974S Samples: 1\",\"Bytes/s\":496,\"Rows/s\":0,\"Bytes\":280,\"Rows\":3,\"I/O time\":564}\n",
      "24/09/28 15:57:42 INFO BlockManagerInfo: Removed broadcast_312_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:57:42 INFO Executor: Finished task 0.0 in stage 389.0 (TID 395). 1564 bytes result sent to driver\n",
      "24/09/28 15:57:42 INFO TaskSetManager: Starting task 0.0 in stage 392.0 (TID 398) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:57:42 INFO TaskSetManager: Finished task 0.0 in stage 389.0 (TID 395) in 713 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:57:42 INFO TaskSchedulerImpl: Removed TaskSet 389.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:57:42 INFO DAGScheduler: ResultStage 389 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1,411 s\n",
      "24/09/28 15:57:43 INFO Executor: Running task 0.0 in stage 392.0 (TID 398)\n",
      "24/09/28 15:57:43 INFO DAGScheduler: Job 242 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:57:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 389: Stage finished\n",
      "24/09/28 15:57:43 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx001_data, parameters sent from Spark:|requiredColumns=[UF,Ano,V1013,V1012,A001B3,A003,A004,V1023,A005,B0011,B0014,B0015,B0019,B00111,B002,B0031,B005,B009B,B007,C01011,F001],|filters=[]\n",
      "24/09/28 15:57:43 INFO DAGScheduler: Job 242 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2,300573 s\n",
      "24/09/28 15:57:43 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGQtNEtUdFl4WTZ2TxoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:57:42.500Z\",\"Ended\":\"2024-09-28T18:57:43.313Z\",\"Parse Timings\":\"Average: PT0.000626042S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.528871232S Samples: 1\",\"Bytes/s\":636,\"Rows/s\":0,\"Bytes\":336,\"Rows\":6,\"I/O time\":528}\n",
      "24/09/28 15:57:43 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDE9MSEs1WWF2ZEp1bxoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:57:42.404Z\",\"Ended\":\"2024-09-28T18:57:43.311Z\",\"Parse Timings\":\"Average: PT0.000272619S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.84468258S Samples: 1\",\"Bytes/s\":331,\"Rows/s\":0,\"Bytes\":280,\"Rows\":3,\"I/O time\":844}\n",
      "24/09/28 15:57:43 INFO BlockManagerInfo: Removed broadcast_310_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:57:43 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGROaDBiNFlEX1RTRxoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:57:42.401Z\",\"Ended\":\"2024-09-28T18:57:43.272Z\",\"Parse Timings\":\"Average: PT0.000285677S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.628497911S Samples: 1\",\"Bytes/s\":815,\"Rows/s\":0,\"Bytes\":512,\"Rows\":4,\"I/O time\":628}\n",
      "24/09/28 15:57:43 INFO Executor: Finished task 0.0 in stage 390.0 (TID 396). 1607 bytes result sent to driver\n",
      "24/09/28 15:57:43 INFO Executor: Finished task 0.0 in stage 391.0 (TID 397). 1688 bytes result sent to driver\n",
      "24/09/28 15:57:43 INFO MemoryStore: Block broadcast_330 stored as values in memory (estimated size 4.0 MiB, free 345.9 MiB)\n",
      "24/09/28 15:57:43 INFO Executor: Finished task 0.0 in stage 388.0 (TID 394). 1788 bytes result sent to driver\n",
      "24/09/28 15:57:43 INFO TaskSetManager: Starting task 0.0 in stage 394.0 (TID 399) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:57:43 INFO MemoryStore: Block broadcast_330_piece0 stored as bytes in memory (estimated size 244.0 B, free 345.9 MiB)\n",
      "24/09/28 15:57:43 INFO Executor: Running task 0.0 in stage 394.0 (TID 399)\n",
      "24/09/28 15:57:43 INFO BlockManagerInfo: Added broadcast_330_piece0 in memory on 10.0.2.15:38963 (size: 244.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:57:43 INFO TaskSetManager: Starting task 0.0 in stage 393.0 (TID 400) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:57:43 INFO SparkContext: Created broadcast 330 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:43 INFO Executor: Running task 0.0 in stage 393.0 (TID 400)\n",
      "24/09/28 15:57:43 INFO TaskSetManager: Finished task 0.0 in stage 390.0 (TID 396) in 1097 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:57:43 INFO TaskSchedulerImpl: Removed TaskSet 390.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:57:43 INFO TaskSetManager: Finished task 0.0 in stage 391.0 (TID 397) in 999 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:57:43 INFO TaskSchedulerImpl: Removed TaskSet 391.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:57:43 INFO DAGScheduler: ResultStage 390 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1,826 s\n",
      "24/09/28 15:57:43 INFO DAGScheduler: Job 243 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:57:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 390: Stage finished\n",
      "24/09/28 15:57:43 INFO TaskSetManager: Starting task 0.0 in stage 395.0 (TID 401) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:57:43 INFO Executor: Running task 0.0 in stage 395.0 (TID 401)\n",
      "24/09/28 15:57:43 INFO TaskSetManager: Finished task 0.0 in stage 388.0 (TID 394) in 1222 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:57:43 INFO TaskSchedulerImpl: Removed TaskSet 388.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:57:43 INFO DAGScheduler: Job 243 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2,709230 s\n",
      "24/09/28 15:57:43 INFO DAGScheduler: ResultStage 391 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1,690 s\n",
      "24/09/28 15:57:43 INFO DAGScheduler: Job 244 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:57:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 391: Stage finished\n",
      "24/09/28 15:57:43 INFO DAGScheduler: Job 244 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2,452563 s\n",
      "24/09/28 15:57:43 INFO DAGScheduler: ResultStage 388 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1,984 s\n",
      "24/09/28 15:57:43 INFO DAGScheduler: Job 241 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:57:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 388: Stage finished\n",
      "24/09/28 15:57:43 INFO DAGScheduler: Job 241 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2,763345 s\n",
      "24/09/28 15:57:43 INFO MemoryStore: Block broadcast_331 stored as values in memory (estimated size 4.0 MiB, free 341.9 MiB)\n",
      "24/09/28 15:57:43 INFO MemoryStore: Block broadcast_333 stored as values in memory (estimated size 4.0 MiB, free 337.9 MiB)\n",
      "24/09/28 15:57:43 INFO MemoryStore: Block broadcast_332 stored as values in memory (estimated size 4.0 MiB, free 333.9 MiB)\n",
      "24/09/28 15:57:43 INFO MemoryStore: Block broadcast_333_piece0 stored as bytes in memory (estimated size 446.0 B, free 333.9 MiB)\n",
      "24/09/28 15:57:43 INFO BlockManagerInfo: Added broadcast_333_piece0 in memory on 10.0.2.15:38963 (size: 446.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:57:43 INFO MemoryStore: Block broadcast_331_piece0 stored as bytes in memory (estimated size 359.0 B, free 333.9 MiB)\n",
      "24/09/28 15:57:43 INFO BlockManagerInfo: Added broadcast_331_piece0 in memory on 10.0.2.15:38963 (size: 359.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:57:43 INFO SparkContext: Created broadcast 333 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:43 INFO SparkContext: Created broadcast 331 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:43 INFO MemoryStore: Block broadcast_332_piece0 stored as bytes in memory (estimated size 244.0 B, free 333.9 MiB)\n",
      "24/09/28 15:57:43 INFO BlockManagerInfo: Added broadcast_332_piece0 in memory on 10.0.2.15:38963 (size: 244.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:57:43 INFO SparkContext: Created broadcast 332 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:43 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDG1JTDQ3WTNIZkFoMRoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:57:43.036Z\",\"Ended\":\"2024-09-28T18:57:43.911Z\",\"Parse Timings\":\"Average: PT0.000236001S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.792488661S Samples: 1\",\"Bytes/s\":393,\"Rows/s\":0,\"Bytes\":312,\"Rows\":4,\"I/O time\":792}\n",
      "24/09/28 15:57:43 INFO Executor: Finished task 0.0 in stage 392.0 (TID 398). 1557 bytes result sent to driver\n",
      "24/09/28 15:57:43 INFO TaskSetManager: Starting task 0.0 in stage 396.0 (TID 402) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:57:43 INFO Executor: Running task 0.0 in stage 396.0 (TID 402)\n",
      "24/09/28 15:57:43 INFO TaskSetManager: Finished task 0.0 in stage 392.0 (TID 398) in 961 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:57:43 INFO TaskSchedulerImpl: Removed TaskSet 392.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:57:43 INFO DAGScheduler: ResultStage 392 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,082 s\n",
      "24/09/28 15:57:43 INFO DAGScheduler: Job 245 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:57:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 392: Stage finished\n",
      "24/09/28 15:57:43 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDFJ1M0hScnFVZzVvehoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:57:43.423Z\",\"Ended\":\"2024-09-28T18:57:43.943Z\",\"Parse Timings\":\"Average: PT0.000164641S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.475963782S Samples: 1\",\"Bytes/s\":656,\"Rows/s\":0,\"Bytes\":312,\"Rows\":4,\"I/O time\":475}\n",
      "24/09/28 15:57:43 INFO DAGScheduler: Job 245 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2,946717 s\n",
      "24/09/28 15:57:43 INFO Executor: Finished task 0.0 in stage 394.0 (TID 399). 1557 bytes result sent to driver\n",
      "24/09/28 15:57:43 INFO TaskSetManager: Starting task 0.0 in stage 397.0 (TID 403) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:57:43 INFO Executor: Running task 0.0 in stage 397.0 (TID 403)\n",
      "24/09/28 15:57:43 INFO TaskSetManager: Finished task 0.0 in stage 394.0 (TID 399) in 616 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:57:43 INFO TaskSchedulerImpl: Removed TaskSet 394.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:57:43 INFO MemoryStore: Block broadcast_334 stored as values in memory (estimated size 4.0 MiB, free 329.9 MiB)\n",
      "24/09/28 15:57:44 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEl6RnBmc2RUN1lzWRoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:57:43.046Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:57:44.003Z\",\"readSessionPrepDuration\":590,\"readSessionCreationDuration\":367,\"readSessionDuration\":957}\n",
      "24/09/28 15:57:44 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 4 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEl6RnBmc2RUN1lzWRoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:57:44 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx001_data': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEl6RnBmc2RUN1lzWRoCdngaAnVo\n",
      "24/09/28 15:57:43 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGZSMXVCYWcwSWk4RhoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:57:43.460Z\",\"Ended\":\"2024-09-28T18:57:43.990Z\",\"Parse Timings\":\"Average: PT0.000123298S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.366106857S Samples: 1\",\"Bytes/s\":852,\"Rows/s\":0,\"Bytes\":312,\"Rows\":4,\"I/O time\":366}\n",
      "24/09/28 15:57:44 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDC1oMC1nOXo2MzhRRRoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:57:43.432Z\",\"Ended\":\"2024-09-28T18:57:44.011Z\",\"Parse Timings\":\"Average: PT0.000240463S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.465616429S Samples: 1\",\"Bytes/s\":1668,\"Rows/s\":0,\"Bytes\":776,\"Rows\":27,\"I/O time\":465}\n",
      "24/09/28 15:57:43 INFO DAGScheduler: ResultStage 394 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1,993 s\n",
      "24/09/28 15:57:44 INFO Executor: Finished task 0.0 in stage 393.0 (TID 400). 2060 bytes result sent to driver\n",
      "24/09/28 15:57:44 INFO Executor: Finished task 0.0 in stage 395.0 (TID 401). 1557 bytes result sent to driver\n",
      "24/09/28 15:57:44 INFO DAGScheduler: Job 246 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:57:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 394: Stage finished\n",
      "24/09/28 15:57:44 INFO DAGScheduler: Job 246 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2,999598 s\n",
      "24/09/28 15:57:44 INFO TaskSetManager: Starting task 0.0 in stage 398.0 (TID 404) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:57:44 INFO TaskSetManager: Starting task 0.0 in stage 399.0 (TID 405) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:57:44 INFO Executor: Running task 0.0 in stage 398.0 (TID 404)\n",
      "24/09/28 15:57:44 INFO TaskSetManager: Finished task 0.0 in stage 393.0 (TID 400) in 712 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:57:44 INFO TaskSchedulerImpl: Removed TaskSet 393.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:57:44 INFO DAGScheduler: ResultStage 393 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,170 s\n",
      "24/09/28 15:57:44 INFO TaskSetManager: Finished task 0.0 in stage 395.0 (TID 401) in 659 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:57:44 INFO TaskSchedulerImpl: Removed TaskSet 395.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:57:44 INFO DAGScheduler: Job 247 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:57:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 393: Stage finished\n",
      "24/09/28 15:57:44 INFO MemoryStore: Block broadcast_334_piece0 stored as bytes in memory (estimated size 295.0 B, free 329.9 MiB)\n",
      "24/09/28 15:57:44 INFO Executor: Running task 0.0 in stage 399.0 (TID 405)\n",
      "24/09/28 15:57:44 INFO BlockManagerInfo: Added broadcast_334_piece0 in memory on 10.0.2.15:38963 (size: 295.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:57:44 INFO DAGScheduler: Job 247 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 3,100251 s\n",
      "24/09/28 15:57:44 INFO SparkContext: Created broadcast 334 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:44 INFO DAGScheduler: ResultStage 395 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,111 s\n",
      "24/09/28 15:57:44 INFO DAGScheduler: Job 248 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:57:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 395: Stage finished\n",
      "24/09/28 15:57:44 INFO DAGScheduler: Job 248 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 3,083507 s\n",
      "24/09/28 15:57:44 INFO MemoryStore: Block broadcast_335 stored as values in memory (estimated size 4.0 MiB, free 325.9 MiB)\n",
      "24/09/28 15:57:44 INFO MemoryStore: Block broadcast_335_piece0 stored as bytes in memory (estimated size 1038.0 B, free 325.9 MiB)\n",
      "24/09/28 15:57:44 INFO MemoryStore: Block broadcast_336 stored as values in memory (estimated size 4.0 MiB, free 321.9 MiB)\n",
      "24/09/28 15:57:44 INFO BlockManagerInfo: Added broadcast_335_piece0 in memory on 10.0.2.15:38963 (size: 1038.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:57:44 INFO MemoryStore: Block broadcast_336_piece0 stored as bytes in memory (estimated size 295.0 B, free 321.9 MiB)\n",
      "24/09/28 15:57:44 INFO BlockManagerInfo: Added broadcast_336_piece0 in memory on 10.0.2.15:38963 (size: 295.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:57:44 INFO SparkContext: Created broadcast 336 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:44 INFO MemoryStore: Block broadcast_337 stored as values in memory (estimated size 4.0 MiB, free 317.9 MiB)\n",
      "24/09/28 15:57:44 INFO SparkContext: Created broadcast 335 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:44 INFO MemoryStore: Block broadcast_337_piece0 stored as bytes in memory (estimated size 295.0 B, free 317.9 MiB)\n",
      "24/09/28 15:57:44 INFO BlockManagerInfo: Added broadcast_337_piece0 in memory on 10.0.2.15:38963 (size: 295.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:57:44 INFO SparkContext: Created broadcast 337 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:44 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx001_data, parameters sent from Spark:|requiredColumns=[UF,Ano,V1013,V1012,A001B3,A003,A004,V1023,A005,B0011,B0014,B0015,B0019,B00111,B002,B0031,B005,B009B,B007,C01011,F001],|filters=[]\n",
      "24/09/28 15:57:44 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEMySlBBdGFGNWxsWhoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:57:43.946Z\",\"Ended\":\"2024-09-28T18:57:44.559Z\",\"Parse Timings\":\"Average: PT0.00014767S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.573482047S Samples: 1\",\"Bytes/s\":558,\"Rows/s\":0,\"Bytes\":320,\"Rows\":4,\"I/O time\":573}\n",
      "24/09/28 15:57:44 INFO Executor: Finished task 0.0 in stage 396.0 (TID 402). 1648 bytes result sent to driver\n",
      "24/09/28 15:57:44 INFO TaskSetManager: Starting task 0.0 in stage 400.0 (TID 406) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:57:44 INFO Executor: Running task 0.0 in stage 400.0 (TID 406)\n",
      "24/09/28 15:57:44 INFO TaskSetManager: Finished task 0.0 in stage 396.0 (TID 402) in 657 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:57:44 INFO TaskSchedulerImpl: Removed TaskSet 396.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:57:44 INFO DAGScheduler: ResultStage 396 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,530 s\n",
      "24/09/28 15:57:44 INFO DAGScheduler: Job 249 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:57:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 396: Stage finished\n",
      "24/09/28 15:57:44 INFO DAGScheduler: Job 249 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 3,564745 s\n",
      "24/09/28 15:57:44 INFO MemoryStore: Block broadcast_338 stored as values in memory (estimated size 4.0 MiB, free 313.9 MiB)\n",
      "24/09/28 15:57:44 INFO MemoryStore: Block broadcast_338_piece0 stored as bytes in memory (estimated size 295.0 B, free 313.9 MiB)\n",
      "24/09/28 15:57:44 INFO BlockManagerInfo: Added broadcast_338_piece0 in memory on 10.0.2.15:38963 (size: 295.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:57:44 INFO SparkContext: Created broadcast 338 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:44 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDERwVERJWWhMb0RJRxoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:57:44.122Z\",\"Ended\":\"2024-09-28T18:57:44.897Z\",\"Parse Timings\":\"Average: PT0.000133346S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.598824823S Samples: 1\",\"Bytes/s\":468,\"Rows/s\":0,\"Bytes\":280,\"Rows\":3,\"I/O time\":598}\n",
      "24/09/28 15:57:44 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEFNTDBoQ2tQVnlNMhoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:57:43.960Z\",\"Ended\":\"2024-09-28T18:57:44.899Z\",\"Parse Timings\":\"Average: PT0.000224057S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.770864816S Samples: 1\",\"Bytes/s\":561,\"Rows/s\":0,\"Bytes\":432,\"Rows\":7,\"I/O time\":770}\n",
      "24/09/28 15:57:44 INFO Executor: Finished task 0.0 in stage 398.0 (TID 404). 1564 bytes result sent to driver\n",
      "24/09/28 15:57:44 INFO Executor: Finished task 0.0 in stage 397.0 (TID 403). 1773 bytes result sent to driver\n",
      "24/09/28 15:57:44 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDE5uYzVkSG4xZHZrRxoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:57:44.334Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:57:44.950Z\",\"readSessionPrepDuration\":241,\"readSessionCreationDuration\":375,\"readSessionDuration\":616}\n",
      "24/09/28 15:57:44 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 4 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDE5uYzVkSG4xZHZrRxoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:57:44 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx001_data': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDE5uYzVkSG4xZHZrRxoCdngaAnVo\n",
      "24/09/28 15:57:44 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDDJFQnNNV1pTTDhaQhoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:57:44.149Z\",\"Ended\":\"2024-09-28T18:57:44.939Z\",\"Parse Timings\":\"Average: PT0.000612423S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.560152104S Samples: 1\",\"Bytes/s\":842,\"Rows/s\":0,\"Bytes\":472,\"Rows\":10,\"I/O time\":560}\n",
      "24/09/28 15:57:44 INFO Executor: Finished task 0.0 in stage 399.0 (TID 405). 1727 bytes result sent to driver\n",
      "24/09/28 15:57:44 INFO TaskSetManager: Finished task 0.0 in stage 397.0 (TID 403) in 1010 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:57:44 INFO TaskSchedulerImpl: Removed TaskSet 397.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:57:44 INFO TaskSetManager: Finished task 0.0 in stage 399.0 (TID 405) in 915 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:57:44 INFO TaskSchedulerImpl: Removed TaskSet 399.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:57:44 INFO TaskSetManager: Finished task 0.0 in stage 398.0 (TID 404) in 963 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:57:44 INFO TaskSchedulerImpl: Removed TaskSet 398.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:57:44 INFO DAGScheduler: ResultStage 397 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,864 s\n",
      "24/09/28 15:57:44 INFO DAGScheduler: Job 250 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:57:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 397: Stage finished\n",
      "24/09/28 15:57:44 INFO DAGScheduler: Job 250 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 3,863899 s\n",
      "24/09/28 15:57:44 INFO DAGScheduler: ResultStage 399 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,815 s\n",
      "24/09/28 15:57:44 INFO DAGScheduler: Job 252 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:57:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 399: Stage finished\n",
      "24/09/28 15:57:44 INFO DAGScheduler: ResultStage 398 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,848 s\n",
      "24/09/28 15:57:44 INFO DAGScheduler: Job 251 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:57:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 398: Stage finished\n",
      "24/09/28 15:57:44 INFO DAGScheduler: Job 251 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 3,841405 s\n",
      "24/09/28 15:57:44 INFO DAGScheduler: Job 252 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 3,839102 s\n",
      "24/09/28 15:57:44 INFO MemoryStore: Block broadcast_339 stored as values in memory (estimated size 4.0 MiB, free 309.9 MiB)\n",
      "24/09/28 15:57:44 INFO MemoryStore: Block broadcast_339_piece0 stored as bytes in memory (estimated size 467.0 B, free 309.9 MiB)\n",
      "24/09/28 15:57:44 INFO MemoryStore: Block broadcast_340 stored as values in memory (estimated size 4.0 MiB, free 305.9 MiB)\n",
      "24/09/28 15:57:44 INFO MemoryStore: Block broadcast_341 stored as values in memory (estimated size 4.0 MiB, free 301.9 MiB)\n",
      "24/09/28 15:57:44 INFO MemoryStore: Block broadcast_341_piece0 stored as bytes in memory (estimated size 244.0 B, free 301.9 MiB)\n",
      "24/09/28 15:57:44 INFO BlockManagerInfo: Added broadcast_339_piece0 in memory on 10.0.2.15:38963 (size: 467.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:57:44 INFO BlockManagerInfo: Added broadcast_341_piece0 in memory on 10.0.2.15:38963 (size: 244.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:57:44 INFO SparkContext: Created broadcast 339 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:44 INFO SparkContext: Created broadcast 341 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:44 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx001_data, parameters sent from Spark:|requiredColumns=[UF,Ano,V1013,V1012,A001B3,A003,A004,V1023,A005,B0011,B0014,B0015,B0019,B00111,B002,B0031,B005,B009B,B007,C01011,F001],|filters=[]\n",
      "24/09/28 15:57:45 INFO MemoryStore: Block broadcast_340_piece0 stored as bytes in memory (estimated size 430.0 B, free 301.9 MiB)\n",
      "24/09/28 15:57:45 INFO BlockManagerInfo: Added broadcast_340_piece0 in memory on 10.0.2.15:38963 (size: 430.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:57:45 INFO SparkContext: Created broadcast 340 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:45 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDEw1ZUxRMkVzRmIycBoCdngaAnVo/streams/GgJ2eBoCdWgoAg\",\"Started\":\"2024-09-28T18:57:44.598Z\",\"Ended\":\"2024-09-28T18:57:45.022Z\",\"Parse Timings\":\"Average: PT0.000173165S Samples: 1\",\"Time in Spark\":\"Not enough samples.\",\"Time waiting for service\":\"Average: PT0.376543087S Samples: 1\",\"Bytes/s\":936,\"Rows/s\":0,\"Bytes\":352,\"Rows\":5,\"I/O time\":376}\n",
      "24/09/28 15:57:45 INFO Executor: Finished task 0.0 in stage 400.0 (TID 406). 1596 bytes result sent to driver\n",
      "24/09/28 15:57:45 INFO TaskSetManager: Finished task 0.0 in stage 400.0 (TID 406) in 450 ms on 10.0.2.15 (executor driver) (1/1)\n",
      "24/09/28 15:57:45 INFO TaskSchedulerImpl: Removed TaskSet 400.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:57:45 INFO DAGScheduler: ResultStage 400 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2,833 s\n",
      "24/09/28 15:57:45 INFO DAGScheduler: Job 253 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:57:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 400: Stage finished\n",
      "24/09/28 15:57:45 INFO DAGScheduler: Job 253 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 3,816810 s\n",
      "24/09/28 15:57:45 INFO MemoryStore: Block broadcast_342 stored as values in memory (estimated size 4.0 MiB, free 297.9 MiB)\n",
      "24/09/28 15:57:45 INFO MemoryStore: Block broadcast_342_piece0 stored as bytes in memory (estimated size 348.0 B, free 297.9 MiB)\n",
      "24/09/28 15:57:45 INFO BlockManagerInfo: Added broadcast_342_piece0 in memory on 10.0.2.15:38963 (size: 348.0 B, free: 366.2 MiB)\n",
      "24/09/28 15:57:45 INFO SparkContext: Created broadcast 342 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "24/09/28 15:57:45 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDG1jbXl5RkFmckxTOBoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:57:44.999Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:57:45.543Z\",\"readSessionPrepDuration\":244,\"readSessionCreationDuration\":300,\"readSessionDuration\":544}\n",
      "24/09/28 15:57:45 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 4 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDG1jbXl5RkFmckxTOBoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:57:45 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx001_data': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDG1jbXl5RkFmckxTOBoCdngaAnVo\n",
      "24/09/28 15:57:45 INFO DirectBigQueryRelation: |Querying table my-project-1508437523553.SOR.tbx001_data, parameters sent from Spark:|requiredColumns=[UF,Ano,V1013,V1012,A001B3,A003,A004,V1023,A005,B0011,B0014,B0015,B0019,B00111,B002,B0031,B005,B009B,B007,C01011,F001],|filters=[]\n",
      "24/09/28 15:57:46 INFO ReadSessionCreator: Read session:{\"readSessionName\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlBZThqQ0hyLUl5MBoCdngaAnVo\",\"readSessionCreationStartTime\":\"2024-09-28T18:57:45.591Z\",\"readSessionCreationEndTime\":\"2024-09-28T18:57:46.123Z\",\"readSessionPrepDuration\":231,\"readSessionCreationDuration\":301,\"readSessionDuration\":532}\n",
      "24/09/28 15:57:46 INFO ReadSessionCreator: Requested 20000 max partitions, but only received 4 from the BigQuery Storage API for session projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlBZThqQ0hyLUl5MBoCdngaAnVo. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.\n",
      "24/09/28 15:57:46 INFO BigQueryRDDFactory: Created read session for table 'my-project-1508437523553.SOR.tbx001_data': projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlBZThqQ0hyLUl5MBoCdngaAnVo\n",
      "24/09/28 15:57:47 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/28 15:57:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/28 15:57:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/28 15:57:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/28 15:57:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/28 15:57:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/28 15:57:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/28 15:57:49 INFO GhfsStorageStatistics: Detected potential high latency for operation op_mkdirs. latencyMs=1449; previousMaxLatencyMs=0; operationCount=1; context=gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0\n",
      "24/09/28 15:57:49 INFO CodeGenerator: Code generated in 65.775686 ms\n",
      "24/09/28 15:57:49 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105\n",
      "24/09/28 15:57:49 INFO DAGScheduler: Got job 254 (save at BigQueryWriteHelper.java:105) with 4 output partitions\n",
      "24/09/28 15:57:49 INFO DAGScheduler: Final stage: ResultStage 401 (save at BigQueryWriteHelper.java:105)\n",
      "24/09/28 15:57:49 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/28 15:57:49 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/28 15:57:49 INFO DAGScheduler: Submitting ResultStage 401 (MapPartitionsRDD[1096] at save at BigQueryWriteHelper.java:105), which has no missing parents\n",
      "24/09/28 15:57:50 INFO MemoryStore: Block broadcast_343 stored as values in memory (estimated size 263.5 KiB, free 297.6 MiB)\n",
      "24/09/28 15:57:50 INFO MemoryStore: Block broadcast_343_piece0 stored as bytes in memory (estimated size 90.5 KiB, free 297.5 MiB)\n",
      "24/09/28 15:57:50 INFO BlockManagerInfo: Added broadcast_343_piece0 in memory on 10.0.2.15:38963 (size: 90.5 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:57:50 INFO SparkContext: Created broadcast 343 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/28 15:57:50 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 401 (MapPartitionsRDD[1096] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "24/09/28 15:57:50 INFO TaskSchedulerImpl: Adding task set 401.0 with 4 tasks resource profile 0\n",
      "24/09/28 15:57:50 INFO TaskSetManager: Starting task 0.0 in stage 401.0 (TID 407) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9268 bytes) \n",
      "24/09/28 15:57:50 INFO TaskSetManager: Starting task 1.0 in stage 401.0 (TID 408) (10.0.2.15, executor driver, partition 1, PROCESS_LOCAL, 9270 bytes) \n",
      "24/09/28 15:57:50 INFO TaskSetManager: Starting task 2.0 in stage 401.0 (TID 409) (10.0.2.15, executor driver, partition 2, PROCESS_LOCAL, 9270 bytes) \n",
      "24/09/28 15:57:50 INFO TaskSetManager: Starting task 3.0 in stage 401.0 (TID 410) (10.0.2.15, executor driver, partition 3, PROCESS_LOCAL, 9270 bytes) \n",
      "24/09/28 15:57:50 INFO Executor: Running task 1.0 in stage 401.0 (TID 408)\n",
      "24/09/28 15:57:50 INFO Executor: Running task 3.0 in stage 401.0 (TID 410)\n",
      "24/09/28 15:57:50 INFO Executor: Running task 0.0 in stage 401.0 (TID 407)\n",
      "24/09/28 15:57:50 INFO Executor: Running task 2.0 in stage 401.0 (TID 409)\n",
      "24/09/28 15:57:50 INFO CodeGenerator: Code generated in 240.784405 ms0 + 4) / 4]\n",
      "24/09/28 15:57:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/28 15:57:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/28 15:57:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/28 15:57:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/28 15:57:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/28 15:57:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/28 15:57:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/28 15:57:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/28 15:57:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/28 15:57:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/28 15:57:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/28 15:57:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/28 15:57:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/28 15:57:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/28 15:57:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/28 15:57:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/28 15:57:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/28 15:57:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/28 15:57:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/28 15:57:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/28 15:57:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/28 15:57:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/28 15:57:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/28 15:57:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/09/28 15:57:51 INFO CodecConfig: Compression: SNAPPY\n",
      "24/09/28 15:57:51 INFO CodecConfig: Compression: SNAPPY\n",
      "24/09/28 15:57:52 INFO BlockManagerInfo: Removed broadcast_313_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:57:52 INFO CodecConfig: Compression: SNAPPY\n",
      "24/09/28 15:57:52 INFO CodecConfig: Compression: SNAPPY\n",
      "24/09/28 15:57:53 INFO BlockManagerInfo: Removed broadcast_323_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:57:53 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/09/28 15:57:53 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/09/28 15:57:53 INFO BlockManagerInfo: Removed broadcast_316_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:57:53 INFO BlockManagerInfo: Removed broadcast_315_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:57:53 INFO CodecConfig: Compression: SNAPPY\n",
      "24/09/28 15:57:53 INFO CodecConfig: Compression: SNAPPY\n",
      "24/09/28 15:57:53 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/09/28 15:57:53 INFO CodecConfig: Compression: SNAPPY\n",
      "24/09/28 15:57:53 INFO CodecConfig: Compression: SNAPPY\n",
      "24/09/28 15:57:53 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/09/28 15:57:53 INFO BlockManagerInfo: Removed broadcast_317_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:57:53 INFO BlockManagerInfo: Removed broadcast_314_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:57:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"uf\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ano\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"mes\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"semana\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ano_nascimento\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"sexo\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"cor_raca\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tipo_area\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"escolaridade\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_febre\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_dificuldade_respirar\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_dor_cabeca\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_fadiga\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_perda_cheiro\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"foi_posto_saude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ficou_em_casa\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ficou_internado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"resultado_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tem_plano_saude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"faixa_rendimento\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"situacao_domicilio\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary uf (STRING);\n",
      "  optional binary ano (STRING);\n",
      "  optional binary mes (STRING);\n",
      "  optional binary semana (STRING);\n",
      "  optional binary ano_nascimento (STRING);\n",
      "  optional binary sexo (STRING);\n",
      "  optional binary cor_raca (STRING);\n",
      "  optional binary tipo_area (STRING);\n",
      "  optional binary escolaridade (STRING);\n",
      "  optional binary teve_febre (STRING);\n",
      "  optional binary teve_dificuldade_respirar (STRING);\n",
      "  optional binary teve_dor_cabeca (STRING);\n",
      "  optional binary teve_fadiga (STRING);\n",
      "  optional binary teve_perda_cheiro (STRING);\n",
      "  optional binary foi_posto_saude (STRING);\n",
      "  optional binary ficou_em_casa (STRING);\n",
      "  optional binary ficou_internado (STRING);\n",
      "  optional binary resultado_covid (STRING);\n",
      "  optional binary tem_plano_saude (STRING);\n",
      "  optional binary faixa_rendimento (STRING);\n",
      "  optional binary situacao_domicilio (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/09/28 15:57:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"uf\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ano\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"mes\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"semana\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ano_nascimento\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"sexo\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"cor_raca\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tipo_area\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"escolaridade\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_febre\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_dificuldade_respirar\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_dor_cabeca\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_fadiga\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_perda_cheiro\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"foi_posto_saude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ficou_em_casa\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ficou_internado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"resultado_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tem_plano_saude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"faixa_rendimento\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"situacao_domicilio\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary uf (STRING);\n",
      "  optional binary ano (STRING);\n",
      "  optional binary mes (STRING);\n",
      "  optional binary semana (STRING);\n",
      "  optional binary ano_nascimento (STRING);\n",
      "  optional binary sexo (STRING);\n",
      "  optional binary cor_raca (STRING);\n",
      "  optional binary tipo_area (STRING);\n",
      "  optional binary escolaridade (STRING);\n",
      "  optional binary teve_febre (STRING);\n",
      "  optional binary teve_dificuldade_respirar (STRING);\n",
      "  optional binary teve_dor_cabeca (STRING);\n",
      "  optional binary teve_fadiga (STRING);\n",
      "  optional binary teve_perda_cheiro (STRING);\n",
      "  optional binary foi_posto_saude (STRING);\n",
      "  optional binary ficou_em_casa (STRING);\n",
      "  optional binary ficou_internado (STRING);\n",
      "  optional binary resultado_covid (STRING);\n",
      "  optional binary tem_plano_saude (STRING);\n",
      "  optional binary faixa_rendimento (STRING);\n",
      "  optional binary situacao_domicilio (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/09/28 15:57:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"uf\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ano\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"mes\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"semana\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ano_nascimento\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"sexo\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"cor_raca\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tipo_area\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"escolaridade\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_febre\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_dificuldade_respirar\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_dor_cabeca\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_fadiga\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_perda_cheiro\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"foi_posto_saude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ficou_em_casa\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ficou_internado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"resultado_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tem_plano_saude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"faixa_rendimento\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"situacao_domicilio\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary uf (STRING);\n",
      "  optional binary ano (STRING);\n",
      "  optional binary mes (STRING);\n",
      "  optional binary semana (STRING);\n",
      "  optional binary ano_nascimento (STRING);\n",
      "  optional binary sexo (STRING);\n",
      "  optional binary cor_raca (STRING);\n",
      "  optional binary tipo_area (STRING);\n",
      "  optional binary escolaridade (STRING);\n",
      "  optional binary teve_febre (STRING);\n",
      "  optional binary teve_dificuldade_respirar (STRING);\n",
      "  optional binary teve_dor_cabeca (STRING);\n",
      "  optional binary teve_fadiga (STRING);\n",
      "  optional binary teve_perda_cheiro (STRING);\n",
      "  optional binary foi_posto_saude (STRING);\n",
      "  optional binary ficou_em_casa (STRING);\n",
      "  optional binary ficou_internado (STRING);\n",
      "  optional binary resultado_covid (STRING);\n",
      "  optional binary tem_plano_saude (STRING);\n",
      "  optional binary faixa_rendimento (STRING);\n",
      "  optional binary situacao_domicilio (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/09/28 15:57:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"uf\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ano\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"mes\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"semana\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ano_nascimento\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"sexo\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"cor_raca\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tipo_area\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"escolaridade\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_febre\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_dificuldade_respirar\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_dor_cabeca\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_fadiga\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"teve_perda_cheiro\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"foi_posto_saude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ficou_em_casa\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ficou_internado\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"resultado_covid\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tem_plano_saude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"faixa_rendimento\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"situacao_domicilio\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary uf (STRING);\n",
      "  optional binary ano (STRING);\n",
      "  optional binary mes (STRING);\n",
      "  optional binary semana (STRING);\n",
      "  optional binary ano_nascimento (STRING);\n",
      "  optional binary sexo (STRING);\n",
      "  optional binary cor_raca (STRING);\n",
      "  optional binary tipo_area (STRING);\n",
      "  optional binary escolaridade (STRING);\n",
      "  optional binary teve_febre (STRING);\n",
      "  optional binary teve_dificuldade_respirar (STRING);\n",
      "  optional binary teve_dor_cabeca (STRING);\n",
      "  optional binary teve_fadiga (STRING);\n",
      "  optional binary teve_perda_cheiro (STRING);\n",
      "  optional binary foi_posto_saude (STRING);\n",
      "  optional binary ficou_em_casa (STRING);\n",
      "  optional binary ficou_internado (STRING);\n",
      "  optional binary resultado_covid (STRING);\n",
      "  optional binary tem_plano_saude (STRING);\n",
      "  optional binary faixa_rendimento (STRING);\n",
      "  optional binary situacao_domicilio (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/09/28 15:57:53 INFO BlockManagerInfo: Removed broadcast_319_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:57:54 INFO BlockManagerInfo: Removed broadcast_318_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.1 MiB)\n",
      "24/09/28 15:57:54 INFO BlockManagerInfo: Removed broadcast_321_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:57:54 INFO BlockManagerInfo: Removed broadcast_322_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:57:54 INFO BlockManagerInfo: Removed broadcast_309_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:57:54 INFO BlockManagerInfo: Removed broadcast_324_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:57:54 INFO BlockManagerInfo: Removed broadcast_325_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:57:54 INFO BlockManagerInfo: Removed broadcast_320_piece0 on 10.0.2.15:38963 in memory (size: 10.2 KiB, free: 366.2 MiB)\n",
      "24/09/28 15:57:56 INFO GhfsStorageStatistics: Detected potential high latency for operation op_create. latencyMs=2209; previousMaxLatencyMs=0; operationCount=4; context=gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0/_temporary/attempt_202409281557496863800685234020311_0401_m_000002_409/part-00002-f0dea679-e879-49d4-a6a2-b81a0bb1604e-c000.snappy.parquet\n",
      "24/09/28 15:57:56 INFO GhfsStorageStatistics: Detected potential high latency for operation op_create. latencyMs=2537; previousMaxLatencyMs=2209; operationCount=4; context=gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0/_temporary/attempt_202409281557496863800685234020311_0401_m_000001_408/part-00001-f0dea679-e879-49d4-a6a2-b81a0bb1604e-c000.snappy.parquet\n",
      "24/09/28 15:57:56 INFO GhfsStorageStatistics: Detected potential high latency for operation op_create. latencyMs=2573; previousMaxLatencyMs=2537; operationCount=4; context=gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0/_temporary/attempt_202409281557496863800685234020311_0401_m_000003_410/part-00003-f0dea679-e879-49d4-a6a2-b81a0bb1604e-c000.snappy.parquet\n",
      "24/09/28 15:57:57 INFO GhfsStorageStatistics: Detected potential high latency for operation op_create. latencyMs=3011; previousMaxLatencyMs=2573; operationCount=4; context=gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0/_temporary/attempt_202409281557496863800685234020311_0401_m_000000_407/part-00000-f0dea679-e879-49d4-a6a2-b81a0bb1604e-c000.snappy.parquet\n",
      "24/09/28 15:57:57 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "24/09/28 15:57:57 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "24/09/28 15:57:57 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "24/09/28 15:57:57 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "24/09/28 15:58:33 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlBZThqQ0hyLUl5MBoCdngaAnVo/streams/GgJ2eBoCdWgoAg,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlBZThqQ0hyLUl5MBoCdngaAnVo/streams/CAEaAnZ4GgJ1aCgC,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlBZThqQ0hyLUl5MBoCdngaAnVo/streams/CAIaAnZ4GgJ1aCgC,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlBZThqQ0hyLUl5MBoCdngaAnVo/streams/CAMaAnZ4GgJ1aCgC\",\"Started\":\"2024-09-28T18:57:50.737Z\",\"Ended\":\"2024-09-28T18:58:33.378Z\",\"Parse Timings\":\"Average: PT0.00116191S Samples: 471\",\"Time in Spark\":\"Average: PT0.083859771S\",\"Time waiting for service\":\"Average: PT0.00971333S Samples: 471\",\"Bytes/s\":16055921,\"Rows/s\":1211361,\"Bytes\":73439784,\"Rows\":662615,\"I/O time\":4574}\n",
      "24/09/28 15:58:35 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlBZThqQ0hyLUl5MBoCdngaAnVo/streams/GgJ2eBoCdWgoAg,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlBZThqQ0hyLUl5MBoCdngaAnVo/streams/CAEaAnZ4GgJ1aCgC,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlBZThqQ0hyLUl5MBoCdngaAnVo/streams/CAIaAnZ4GgJ1aCgC,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlBZThqQ0hyLUl5MBoCdngaAnVo/streams/CAMaAnZ4GgJ1aCgC\",\"Started\":\"2024-09-28T18:57:50.704Z\",\"Ended\":\"2024-09-28T18:58:35.455Z\",\"Parse Timings\":\"Average: PT0.001429513S Samples: 472\",\"Time in Spark\":\"Average: PT0.089027079S\",\"Time waiting for service\":\"Average: PT0.012276732S Samples: 472\",\"Bytes/s\":12573532,\"Rows/s\":983108,\"Bytes\":72851048,\"Rows\":662615,\"I/O time\":5794}\n",
      "24/09/28 15:58:35 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlBZThqQ0hyLUl5MBoCdngaAnVo/streams/GgJ2eBoCdWgoAg,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlBZThqQ0hyLUl5MBoCdngaAnVo/streams/CAEaAnZ4GgJ1aCgC,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlBZThqQ0hyLUl5MBoCdngaAnVo/streams/CAIaAnZ4GgJ1aCgC,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlBZThqQ0hyLUl5MBoCdngaAnVo/streams/CAMaAnZ4GgJ1aCgC\",\"Started\":\"2024-09-28T18:57:50.736Z\",\"Ended\":\"2024-09-28T18:58:35.777Z\",\"Parse Timings\":\"Average: PT0.001163836S Samples: 471\",\"Time in Spark\":\"Average: PT0.069582115S\",\"Time waiting for service\":\"Average: PT0.013323822S Samples: 471\",\"Bytes/s\":11610811,\"Rows/s\":1209149,\"Bytes\":72857840,\"Rows\":662614,\"I/O time\":6275}\n",
      "24/09/28 15:58:35 INFO LoggingBigQueryStorageReadRowsTracer: Tracer Logs:{\"Stream Name\":\"projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlBZThqQ0hyLUl5MBoCdngaAnVo/streams/GgJ2eBoCdWgoAg,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlBZThqQ0hyLUl5MBoCdngaAnVo/streams/CAEaAnZ4GgJ1aCgC,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlBZThqQ0hyLUl5MBoCdngaAnVo/streams/CAIaAnZ4GgJ1aCgC,projects/my-project-1508437523553/locations/us-east1/sessions/CAISDGlBZThqQ0hyLUl5MBoCdngaAnVo/streams/CAMaAnZ4GgJ1aCgC\",\"Started\":\"2024-09-28T18:57:50.737Z\",\"Ended\":\"2024-09-28T18:58:35.868Z\",\"Parse Timings\":\"Average: PT0.001143776S Samples: 472\",\"Time in Spark\":\"Average: PT0.089049556S\",\"Time waiting for service\":\"Average: PT0.013340515S Samples: 472\",\"Bytes/s\":11594055,\"Rows/s\":1229341,\"Bytes\":72996176,\"Rows\":662615,\"I/O time\":6296}\n",
      "24/09/28 15:58:38 INFO GhfsStorageStatistics: Detected potential high latency for operation stream_write_close_operations. latencyMs=1650; previousMaxLatencyMs=0; operationCount=4; context=gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0/_temporary/attempt_202409281557496863800685234020311_0401_m_000000_407/part-00000-f0dea679-e879-49d4-a6a2-b81a0bb1604e-c000.snappy.parquet\n",
      "24/09/28 15:58:38 INFO GhfsStorageStatistics: Detected potential high latency for operation stream_write_close_operations. latencyMs=1671; previousMaxLatencyMs=1650; operationCount=4; context=gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0/_temporary/attempt_202409281557496863800685234020311_0401_m_000003_410/part-00003-f0dea679-e879-49d4-a6a2-b81a0bb1604e-c000.snappy.parquet\n",
      "24/09/28 15:58:38 INFO GhfsStorageStatistics: Detected potential high latency for operation stream_write_close_operations. latencyMs=1731; previousMaxLatencyMs=1671; operationCount=4; context=gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0/_temporary/attempt_202409281557496863800685234020311_0401_m_000002_409/part-00002-f0dea679-e879-49d4-a6a2-b81a0bb1604e-c000.snappy.parquet\n",
      "24/09/28 15:58:38 INFO GhfsStorageStatistics: Detected potential high latency for operation stream_write_close_operations. latencyMs=2116; previousMaxLatencyMs=1731; operationCount=4; context=gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0/_temporary/attempt_202409281557496863800685234020311_0401_m_000001_408/part-00001-f0dea679-e879-49d4-a6a2-b81a0bb1604e-c000.snappy.parquet\n",
      "24/09/28 15:58:43 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0/_temporary/' directory.\n",
      "24/09/28 15:58:43 INFO GhfsStorageStatistics: Detected potential high latency for operation op_rename. latencyMs=2683; previousMaxLatencyMs=0; operationCount=4; context=rename(gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0/_temporary/attempt_202409281557496863800685234020311_0401_m_000000_407 -> gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0/task_202409281557496863800685234020311_0401_m_000000)\n",
      "24/09/28 15:58:43 INFO FileOutputCommitter: Saved output of task 'attempt_202409281557496863800685234020311_0401_m_000000_407' to gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0/task_202409281557496863800685234020311_0401_m_000000\n",
      "24/09/28 15:58:43 INFO SparkHadoopMapRedUtil: attempt_202409281557496863800685234020311_0401_m_000000_407: Committed. Elapsed time: 3602 ms.\n",
      "24/09/28 15:58:43 INFO Executor: Finished task 0.0 in stage 401.0 (TID 407). 3522 bytes result sent to driver\n",
      "24/09/28 15:58:43 INFO TaskSetManager: Finished task 0.0 in stage 401.0 (TID 407) in 53415 ms on 10.0.2.15 (executor driver) (1/4)\n",
      "24/09/28 15:58:44 INFO GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
      "24/09/28 15:58:44 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0/_temporary/' directory.\n",
      "24/09/28 15:58:44 INFO GhfsStorageStatistics: Detected potential high latency for operation op_rename. latencyMs=3471; previousMaxLatencyMs=2683; operationCount=4; context=rename(gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0/_temporary/attempt_202409281557496863800685234020311_0401_m_000003_410 -> gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0/task_202409281557496863800685234020311_0401_m_000003)\n",
      "24/09/28 15:58:44 INFO FileOutputCommitter: Saved output of task 'attempt_202409281557496863800685234020311_0401_m_000003_410' to gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0/task_202409281557496863800685234020311_0401_m_000003\n",
      "24/09/28 15:58:44 INFO SparkHadoopMapRedUtil: attempt_202409281557496863800685234020311_0401_m_000003_410: Committed. Elapsed time: 4431 ms.\n",
      "24/09/28 15:58:44 INFO Executor: Finished task 3.0 in stage 401.0 (TID 410). 3479 bytes result sent to driver\n",
      "24/09/28 15:58:44 INFO TaskSetManager: Finished task 3.0 in stage 401.0 (TID 410) in 54211 ms on 10.0.2.15 (executor driver) (2/4)\n",
      "24/09/28 15:58:44 INFO GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
      "24/09/28 15:58:44 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0/_temporary/' directory.\n",
      "24/09/28 15:58:44 INFO FileOutputCommitter: Saved output of task 'attempt_202409281557496863800685234020311_0401_m_000002_409' to gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0/task_202409281557496863800685234020311_0401_m_000002\n",
      "24/09/28 15:58:44 INFO SparkHadoopMapRedUtil: attempt_202409281557496863800685234020311_0401_m_000002_409: Committed. Elapsed time: 4495 ms.\n",
      "24/09/28 15:58:44 INFO Executor: Finished task 2.0 in stage 401.0 (TID 409). 3479 bytes result sent to driver\n",
      "24/09/28 15:58:44 INFO TaskSetManager: Finished task 2.0 in stage 401.0 (TID 409) in 54374 ms on 10.0.2.15 (executor driver) (3/4)\n",
      "24/09/28 15:58:44 INFO GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
      "24/09/28 15:58:44 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0/_temporary/' directory.\n",
      "24/09/28 15:58:44 INFO FileOutputCommitter: Saved output of task 'attempt_202409281557496863800685234020311_0401_m_000001_408' to gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0/task_202409281557496863800685234020311_0401_m_000001\n",
      "24/09/28 15:58:44 INFO SparkHadoopMapRedUtil: attempt_202409281557496863800685234020311_0401_m_000001_408: Committed. Elapsed time: 4541 ms.\n",
      "24/09/28 15:58:44 INFO Executor: Finished task 1.0 in stage 401.0 (TID 408). 3479 bytes result sent to driver\n",
      "24/09/28 15:58:44 INFO TaskSetManager: Finished task 1.0 in stage 401.0 (TID 408) in 54588 ms on 10.0.2.15 (executor driver) (4/4)\n",
      "24/09/28 15:58:44 INFO TaskSchedulerImpl: Removed TaskSet 401.0, whose tasks have all completed, from pool \n",
      "24/09/28 15:58:44 INFO DAGScheduler: ResultStage 401 (save at BigQueryWriteHelper.java:105) finished in 55,170 s\n",
      "24/09/28 15:58:44 INFO DAGScheduler: Job 254 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/28 15:58:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 401: Stage finished\n",
      "24/09/28 15:58:44 INFO DAGScheduler: Job 254 finished: save at BigQueryWriteHelper.java:105, took 55,182812 s\n",
      "24/09/28 15:58:44 INFO FileFormatWriter: Start to commit write Job b9a8ac78-023a-461b-8395-dbdd4d352373.\n",
      "24/09/28 15:58:48 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0/task_202409281557496863800685234020311_0401_m_000000/' directory.\n",
      "24/09/28 15:58:51 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0/task_202409281557496863800685234020311_0401_m_000001/' directory.\n",
      "24/09/28 15:58:53 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0/task_202409281557496863800685234020311_0401_m_000002/' directory.\n",
      "24/09/28 15:58:54 INFO GhfsStorageStatistics: Detected potential high latency for operation op_list_status. latencyMs=577; previousMaxLatencyMs=483; operationCount=5; context=gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0/task_202409281557496863800685234020311_0401_m_000003\n",
      "24/09/28 15:58:56 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary/0/task_202409281557496863800685234020311_0401_m_000003/' directory.\n",
      "24/09/28 15:58:57 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/' directory.\n",
      "24/09/28 15:58:57 INFO GhfsStorageStatistics: Detected potential high latency for operation op_delete. latencyMs=1634; previousMaxLatencyMs=0; operationCount=1; context=gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/_temporary\n",
      "24/09/28 15:59:00 INFO FileFormatWriter: Write Job b9a8ac78-023a-461b-8395-dbdd4d352373 committed. Elapsed time: 15149 ms.\n",
      "24/09/28 15:59:00 INFO FileFormatWriter: Finished processing stats for write job b9a8ac78-023a-461b-8395-dbdd4d352373.\n",
      "24/09/28 15:59:01 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=SOT, tableId=tbx001_data}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=uf, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ano, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=mes, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=semana, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ano_nascimento, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=sexo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=cor_raca, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=tipo_area, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=escolaridade, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teve_febre, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teve_dificuldade_respirar, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teve_dor_cabeca, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teve_fadiga, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teve_perda_cheiro, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=foi_posto_saude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ficou_em_casa, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ficou_internado, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=resultado_covid, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=tem_plano_saude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=faixa_rendimento, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=situacao_domicilio, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/part-00000-f0dea679-e879-49d4-a6a2-b81a0bb1604e-c000.snappy.parquet, gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/part-00001-f0dea679-e879-49d4-a6a2-b81a0bb1604e-c000.snappy.parquet, gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/part-00002-f0dea679-e879-49d4-a6a2-b81a0bb1604e-c000.snappy.parquet, gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef/part-00003-f0dea679-e879-49d4-a6a2-b81a0bb1604e-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=my-project-1508437523553, job=a94b6996-dda7-4b2d-ae79-5da247b39a70, location=us-east1}\n",
      "24/09/28 15:59:11 INFO BigQueryClient: Done loading to SOT.tbx001_data. jobId: JobId{project=my-project-1508437523553, job=a94b6996-dda7-4b2d-ae79-5da247b39a70, location=us-east1}\n",
      "24/09/28 15:59:14 INFO GhfsStorageStatistics: Detected potential high latency for operation op_delete. latencyMs=1972; previousMaxLatencyMs=1634; operationCount=3; context=gs://meu-bucket-temporario-spark/.spark-bigquery-local-1727544150129-0a11234e-5570-4d0e-b2cd-951e11d2ceef\n"
     ]
    }
   ],
   "source": [
    "save_to_bigquery(df_joined, \"SOT\", \"tbx001_data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
