{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as T\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie uma sessão Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"tech-challenge-fase3\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para obter o próximo número de tabela\n",
    "def get_next_table_number(parquet_dir):\n",
    "    # Lista todos os arquivos no diretório\n",
    "    files = os.listdir(parquet_dir)\n",
    "    \n",
    "    # Filtra os arquivos que correspondem ao padrão \"tbx{number}_{table_name}.parquet\"\n",
    "    table_numbers = [\n",
    "        int(f.split('_')[0][3:]) for f in files if f.startswith('tbx') and f.endswith('.parquet')\n",
    "    ]\n",
    "    \n",
    "    # Determina o próximo número de tabela\n",
    "    if table_numbers:\n",
    "        next_table_number = max(table_numbers) + 1\n",
    "    else:\n",
    "        next_table_number = 1\n",
    "    \n",
    "    # Retorna o número da tabela como string completando com zeros até 3 casas\n",
    "    return str(next_table_number).zfill(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para salvar DataFrame em formato Parquet\n",
    "def save_to_parquet(df, table_name):\n",
    "    # Diretório onde os arquivos Parquet são salvos\n",
    "    parquet_dir = \"./data/parquet/\"\n",
    "    \n",
    "    # Verifica se já existe um arquivo com o mesmo table_name\n",
    "    existing_files = [f for f in os.listdir(parquet_dir) if f.endswith(f\"_{table_name}.parquet\")]\n",
    "    \n",
    "    if existing_files:\n",
    "        # Extrai o número da tabela do arquivo existente\n",
    "        existing_file = existing_files[0]\n",
    "        table_number = existing_file.split('_')[0][3:]\n",
    "    else:         \n",
    "        # Obtém o próximo número de tabela\n",
    "        table_number = get_next_table_number(parquet_dir)\n",
    "    \n",
    "    # Salva o DataFrame em formato Parquet\n",
    "    df.write \\\n",
    "        .format(\"parquet\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(f\"{parquet_dir}tbx{table_number}_{table_name}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/18 18:01:18 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: UF, V1012, V1013, V1022, A002, A003, A004, A005, A007A, B0011, B0012, B0016, B0018, B00111, B00112, B0044, B0046, B007, C006, C009A, E0022\n",
      " Schema: UF, V1012, V1013, V1022, A002, A003, A004, A005, B0011, B0014, B0015, B0019, B00111, B002, B0031, B005, B007, B009B, C007B, C01011, F001\n",
      "Expected: B0011 but found: A007A\n",
      "CSV file: file:///home/spark/source/repos/fiap-dataanalysis-techchallenge-fase3/data/raw/PNAD_COVID_112020.csv\n",
      "24/08/18 18:01:22 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: UF, V1012, V1013, V1022, A002, A003, A004, A005, B0015, B0018, B0019, B002, B0032, B0035, B0036, C003, C005, C0053, C011A11, D0013, \n",
      " Schema: UF, V1012, V1013, V1022, A002, A003, A004, A005, B0011, B0014, B0015, B0019, B00111, B002, B0031, B005, B007, B009B, C007B, C01011, F001\n",
      "Expected: B0011 but found: B0015\n",
      "CSV file: file:///home/spark/source/repos/fiap-dataanalysis-techchallenge-fase3/data/raw/PNAD_COVID_062020.csv\n",
      "24/08/18 18:01:23 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: UF, V1012, V1013, V1022, A002, A003, A004, A005, B0015, B0018, B0019, B002, B0032, B0035, B0036, C003, C005, C0053, C011A11, D0013, \n",
      " Schema: UF, V1012, V1013, V1022, A002, A003, A004, A005, B0011, B0014, B0015, B0019, B00111, B002, B0031, B005, B007, B009B, C007B, C01011, F001\n",
      "Expected: B0011 but found: B0015\n",
      "CSV file: file:///home/spark/source/repos/fiap-dataanalysis-techchallenge-fase3/data/raw/PNAD_COVID_052020.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark\\\n",
    "    .read\\\n",
    "    .option('delimiter',',')\\\n",
    "    .option('header',True)\\\n",
    "    .option('inferSchema',True)\\\n",
    "    .csv('./data/raw')\n",
    "\n",
    "columns = [\n",
    "    \"UF\", \"V1012\", \"V1013\", \"V1022\", \"A002\", \"A003\",\n",
    "    \"A004\", \"A005\", \"B0011\", \"B0014\", \"B0015\", \"B0019\",\n",
    "    \"B00111\", \"B002\", \"B0031\", \"B005\", \"B007\", \"C007B\",\n",
    "    \"C01011\", \"F001\",'B009B'\n",
    "]\n",
    "\n",
    "df = df.select(columns)\n",
    "\n",
    "# Renomeie as colunas\n",
    "df = df\\\n",
    "        .withColumnRenamed(\"UF\", \"uf\")\\\n",
    "        .withColumnRenamed(\"V1012\", \"semana_mes\")\\\n",
    "        .withColumnRenamed(\"V1013\", \"mes\")\\\n",
    "        .withColumnRenamed(\"V1022\", \"area_domicilio\")\\\n",
    "        .withColumnRenamed(\"A002\", \"idade\")\\\n",
    "        .withColumnRenamed(\"A003\", \"sexo\")\\\n",
    "        .withColumnRenamed(\"A004\", \"cor_raca\")\\\n",
    "        .withColumnRenamed(\"A005\", \"escolaridade\")\\\n",
    "        .withColumnRenamed(\"B0011\", \"teve_febre\")\\\n",
    "        .withColumnRenamed(\"B0014\", \"teve_dificuldade_respirar\")\\\n",
    "        .withColumnRenamed(\"B0015\", \"teve_dor_cabeca\")\\\n",
    "        .withColumnRenamed(\"B0019\", \"teve_fadiga\")\\\n",
    "        .withColumnRenamed(\"B00111\", \"teve_perda_cheiro\")\\\n",
    "        .withColumnRenamed(\"B002\", \"foi_posto_saude\")\\\n",
    "        .withColumnRenamed(\"B0031\", \"ficou_em_casa\")\\\n",
    "        .withColumnRenamed(\"B005\", \"ficou_internado\")\\\n",
    "        .withColumnRenamed(\"B009B\", \"resultado_covid\")\\\n",
    "        .withColumnRenamed(\"B007\", \"tem_plano_saude\")\\\n",
    "        .withColumnRenamed(\"C007B\", \"assalariado\")\\\n",
    "        .withColumnRenamed(\"C01011\", \"faixa_rendimento\")\\\n",
    "        .withColumnRenamed(\"F001\", \"situacao_domicilio\")\n",
    "\n",
    "\n",
    "# Salve o DataFrame em formato Parquet\n",
    "save_to_parquet(df, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o esquema para o DataFrame\n",
    "schema = T.StructType(\n",
    "    [\n",
    "        T.StructField('id', T.StringType()),  # Código\n",
    "        T.StructField('name', T.StringType())   # Nome\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados dos estados brasileiros com seus respectivos códigos\n",
    "data = [\n",
    "    {\n",
    "        \"11\": \"Rondônia\",\n",
    "        \"12\": \"Acre\",\n",
    "        \"13\": \"Amazonas\",\n",
    "        \"14\": \"Roraima\",\n",
    "        \"15\": \"Pará\",\n",
    "        \"16\": \"Amapá\",\n",
    "        \"17\": \"Tocantins\",\n",
    "        \"21\": \"Maranhão\",\n",
    "        \"22\": \"Piauí\",\n",
    "        \"23\": \"Ceará\",\n",
    "        \"24\": \"Rio Grande do Norte\",\n",
    "        \"25\": \"Paraíba\",\n",
    "        \"26\": \"Pernambuco\",\n",
    "        \"27\": \"Alagoas\",\n",
    "        \"28\": \"Sergipe\",\n",
    "        \"29\": \"Bahia\",\n",
    "        \"31\": \"Minas Gerais\",\n",
    "        \"32\": \"Espírito Santo\",\n",
    "        \"33\": \"Rio de Janeiro\",\n",
    "        \"35\": \"São Paulo\",\n",
    "        \"41\": \"Paraná\",\n",
    "        \"42\": \"Santa Catarina\",\n",
    "        \"43\": \"Rio Grande do Sul\",\n",
    "        \"50\": \"Mato Grosso do Sul\",\n",
    "        \"51\": \"Mato Grosso\",\n",
    "        \"52\": \"Goiás\",\n",
    "        \"53\": \"Distrito Federal\"\n",
    "    }\n",
    "]\n",
    "# Transformar o array de dicionários em uma lista de tuplas\n",
    "transformed_data = [(k, v) for d in data for k, v in d.items()]\n",
    "\n",
    "# Cria um DataFrame com os dados de sexo\n",
    "df_data = spark.createDataFrame(data=transformed_data, schema=schema)\n",
    "\n",
    "# Salva o DataFrame em formato Parquet\n",
    "save_to_parquet(df_data, \"states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|     name|\n",
      "+---+---------+\n",
      "|  1|Masculino|\n",
      "|  2| Feminino|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dados de sexo com seus respectivos códigos\n",
    "data = [\n",
    "    {\n",
    "        '1': 'Masculino',\n",
    "        '2': 'Feminino'\n",
    "    }\n",
    "]\n",
    "# Transformar o array de dicionários em uma lista de tuplas\n",
    "transformed_data = [(k, v) for d in data for k, v in d.items()]\n",
    "\n",
    "# Cria um DataFrame com os dados de sexo\n",
    "df_data = spark.createDataFrame(data=transformed_data, schema=schema)\n",
    "\n",
    "df_data.show()\n",
    "\n",
    "# Salva o DataFrame em formato Parquet\n",
    "save_to_parquet(df_data,\"gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados das áreas de domicílio com seus respectivos códigos\n",
    "data = [\n",
    "    {\n",
    "        '1': 'Urbana',\n",
    "        '2': 'Rural'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Transformar o array de dicionários em uma lista de tuplas\n",
    "transformed_data = [(k, v) for d in data for k, v in d.items()]\n",
    "\n",
    "# Cria um DataFrame com os dados de sexo\n",
    "df_data = spark.createDataFrame(data=transformed_data, schema=schema)\n",
    "\n",
    "# Salva o DataFrame em formato Parquet\n",
    "save_to_parquet(df_data, \"residential_area\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados das raças com seus respectivos códigos\n",
    "data = [\n",
    "    {\n",
    "        '1': 'Branca',\n",
    "        '2': 'Preta',\n",
    "        '3': 'Amarela',\n",
    "        '4': 'Parda',\n",
    "        '5': 'Indígena',\n",
    "        '9': 'Ignorado'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Transformar o array de dicionários em uma lista de tuplas\n",
    "transformed_data = [(k, v) for d in data for k, v in d.items()]\n",
    "\n",
    "# Cria um DataFrame com os dados de sexo\n",
    "df_data = spark.createDataFrame(data=transformed_data, schema=schema)\n",
    "\n",
    "# Salva o DataFrame em formato Parquet\n",
    "save_to_parquet(df_data, \"race\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados de escolaridade com seus respectivos códigos\n",
    "data = [\n",
    "    {\n",
    "        '1': 'Sem instrução',\n",
    "        '2': 'Fundamental incompleto',\n",
    "        '3': 'Fundamental completa',\n",
    "        '4': 'Médio incompleto',\n",
    "        '5': 'Médio completo',\n",
    "        '6': 'Superior incompleto',\n",
    "        '7': 'Superior completo',\n",
    "        '8': 'Pós-graduação, mestrado ou doutorado'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Transformar o array de dicionários em uma lista de tuplas\n",
    "transformed_data = [(k, v) for d in data for k, v in d.items()]\n",
    "\n",
    "# Cria um DataFrame com os dados de sexo\n",
    "df_data = spark.createDataFrame(data=transformed_data, schema=schema)\n",
    "\n",
    "# Salva o DataFrame em formato Parquet\n",
    "save_to_parquet(df_data, \"education_levels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados das respostas de COVID com seus respectivos códigos\n",
    "data = [\n",
    "    {\n",
    "        '1': 'Sim',\n",
    "        '2': 'Não ',\n",
    "        '3': 'Não sabe',\n",
    "        '9': 'Ignorado'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Transformar o array de dicionários em uma lista de tuplas\n",
    "transformed_data = [(k, v) for d in data for k, v in d.items()]\n",
    "\n",
    "# Cria um DataFrame com os dados de sexo\n",
    "df_data = spark.createDataFrame(data=transformed_data, schema=schema)\n",
    "\n",
    "# Salva o DataFrame em formato Parquet\n",
    "save_to_parquet(df_data, \"covid_responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados das respostas de internação com seus respectivos códigos\n",
    "data = [\n",
    "    {\n",
    "        '1': 'Sim',\n",
    "        '2': 'Não ',\n",
    "        '3': 'Não foi atendido',\n",
    "        '9': 'Ignorado'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Transformar o array de dicionários em uma lista de tuplas\n",
    "transformed_data = [(k, v) for d in data for k, v in d.items()]\n",
    "\n",
    "# Cria um DataFrame com os dados de sexo\n",
    "df_data = spark.createDataFrame(data=transformed_data, schema=schema)\n",
    "\n",
    "# Salva o DataFrame em formato Parquet\n",
    "save_to_parquet(df_data, \"hospitalization_status\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados das faixas de rendimento com seus respectivos códigos\n",
    "data = [\n",
    "    {\n",
    "        '00':   '0 - 100',\n",
    "        '01':\t'101 - 300',\n",
    "        '02':\t'301 - 600',\n",
    "        '03':\t'601 - 800',\n",
    "        '04':\t'801 - 1.600',\n",
    "        '05':\t'1.601 - 3.000',\n",
    "        '06':\t'3.001 - 10.000',\n",
    "        '07':\t'10.001 - 50.000',\n",
    "        '08':\t'50.001 - 100.000',\n",
    "        '09':\t'Mais de 100.000'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Transformar o array de dicionários em uma lista de tuplas\n",
    "transformed_data = [(k, v) for d in data for k, v in d.items()]\n",
    "\n",
    "# Cria um DataFrame com os dados de sexo\n",
    "df_data = spark.createDataFrame(data=transformed_data, schema=schema)\n",
    "# Salva o DataFrame em formato Parquet\n",
    "save_to_parquet(df_data, \"income_brackets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados das situações de domicílio com seus respectivos códigos\n",
    "data = [\n",
    "    {\n",
    "        '1': 'Próprio - já pago ',\n",
    "        '2': 'Próprio - ainda pagando',\n",
    "        '3': 'Alugado',\n",
    "        '4': 'Cedido por empregador',\n",
    "        '5': 'Cedido por familiar ',\n",
    "        '6': 'Cedido de outra forma ',\n",
    "        '7': 'Outra condição',\n",
    "    }\n",
    "]\n",
    "\n",
    "# Transformar o array de dicionários em uma lista de tuplas\n",
    "transformed_data = [(k, v) for d in data for k, v in d.items()]\n",
    "\n",
    "# Cria um DataFrame com os dados de sexo\n",
    "df_data = spark.createDataFrame(data=transformed_data, schema=schema)\n",
    "\n",
    "# Salva o DataFrame em formato Parquet\n",
    "save_to_parquet(df_data, \"residential_situations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados das situações de domicílio com seus respectivos códigos\n",
    "data = [\n",
    "    {\n",
    "        \"UF\": \"uf\",\n",
    "        \"V1012\": \"semana_mes\",\n",
    "        \"V1013\": \"mes\",\n",
    "        \"V1022\": \"area_domicilio\",\n",
    "        \"A002\": \"idade\",\n",
    "        \"A003\": \"sexo\",\n",
    "        \"A004\": \"cor_raca\",\n",
    "        \"A005\": \"escolaridade\",\n",
    "        \"B0011\": \"teve_febre\",\n",
    "        \"B0014\": \"teve_dificuldade_respirar\",\n",
    "        \"B0015\": \"teve_dor_cabeca\",\n",
    "        \"B0019\": \"teve_fadiga\",\n",
    "        \"B00111\": \"teve_perda_cheiro\",\n",
    "        \"B002\": \"foi_posto_saude\",\n",
    "        \"B0031\": \"ficou_em_casa\",\n",
    "        \"B005\": \"ficou_internado\",\n",
    "        \"B007\": \"tem_plano_saude\",\n",
    "        \"C007B\": \"assalariado\",\n",
    "        \"C01011\": \"faixa_rendimento\",\n",
    "        \"F001\": \"situacao_domicilio\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Transformar o array de dicionários em uma lista de tuplas\n",
    "transformed_data = [(k, v) for d in data for k, v in d.items()]\n",
    "\n",
    "# Cria um DataFrame com os dados de sexo\n",
    "df_data = spark.createDataFrame(data=transformed_data, schema=schema)\n",
    "\n",
    "# Salva o DataFrame em formato Parquet\n",
    "save_to_parquet(df_data, \"questions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
